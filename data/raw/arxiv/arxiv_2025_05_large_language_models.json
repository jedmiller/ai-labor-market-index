{
  "query": "all:large language models AND (labor market OR employment OR jobs OR workforce OR automation)",
  "date_collected": "2025-06-08T03:20:52.732376",
  "target_period": "2025-05",
  "papers": [
    {
      "id": "http://arxiv.org/abs/2506.00742v1",
      "title": "ArtiScene: Language-Driven Artistic 3D Scene Generation Through Image\n  Intermediary",
      "published": "2025-05-31T23:03:54Z",
      "updated": "2025-05-31T23:03:54Z",
      "summary": "Designing 3D scenes is traditionally a challenging task that demands both\nartistic expertise and proficiency with complex software. Recent advances in\ntext-to-3D generation have greatly simplified this process by letting users\ncreate scenes based on simple text descriptions. However, as these methods\ngenerally require extra training or in-context learning, their performance is\noften hindered by the limited availability of high-quality 3D data. In\ncontrast, modern text-to-image models learned from web-scale images can\ngenerate scenes with diverse, reliable spatial layouts and consistent, visually\nappealing styles. Our key insight is that instead of learning directly from 3D\nscenes, we can leverage generated 2D images as an intermediary to guide 3D\nsynthesis. In light of this, we introduce ArtiScene, a training-free automated\npipeline for scene design that integrates the flexibility of free-form\ntext-to-image generation with the diversity and reliability of 2D intermediary\nlayouts.\n  First, we generate 2D images from a scene description, then extract the shape\nand appearance of objects to create 3D models. These models are assembled into\nthe final scene using geometry, position, and pose information derived from the\nsame intermediary image. Being generalizable to a wide range of scenes and\nstyles, ArtiScene outperforms state-of-the-art benchmarks by a large margin in\nlayout and aesthetic quality by quantitative metrics. It also averages a 74.89%\nwinning rate in extensive user studies and 95.07% in GPT-4o evaluation. Project\npage: https://artiscene-cvpr.github.io/",
      "authors": [
        "Zeqi Gu",
        "Yin Cui",
        "Zhaoshuo Li",
        "Fangyin Wei",
        "Yunhao Ge",
        "Jinwei Gu",
        "Ming-Yu Liu",
        "Abe Davis",
        "Yifan Ding"
      ],
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "links": [
        "http://arxiv.org/abs/2506.00742v1",
        "http://arxiv.org/pdf/2506.00742v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2506.00694v2",
      "title": "Measuring Faithfulness and Abstention: An Automated Pipeline for\n  Evaluating LLM-Generated 3-ply Case-Based Legal Arguments",
      "published": "2025-05-31T19:56:40Z",
      "updated": "2025-06-03T03:22:48Z",
      "summary": "Large Language Models (LLMs) demonstrate potential in complex legal tasks\nlike argument generation, yet their reliability remains a concern. Building\nupon pilot work assessing LLM generation of 3-ply legal arguments using human\nevaluation, this paper introduces an automated pipeline to evaluate LLM\nperformance on this task, specifically focusing on faithfulness (absence of\nhallucination), factor utilization, and appropriate abstention. We define\nhallucination as the generation of factors not present in the input case\nmaterials and abstention as the model's ability to refrain from generating\narguments when instructed and no factual basis exists. Our automated method\nemploys an external LLM to extract factors from generated arguments and\ncompares them against the ground-truth factors provided in the input case\ntriples (current case and two precedent cases). We evaluated eight distinct\nLLMs on three tests of increasing difficulty: 1) generating a standard 3-ply\nargument, 2) generating an argument with swapped precedent roles, and 3)\nrecognizing the impossibility of argument generation due to lack of shared\nfactors and abstaining. Our findings indicate that while current LLMs achieve\nhigh accuracy (over 90%) in avoiding hallucination on viable argument\ngeneration tests (Tests 1 & 2), they often fail to utilize the full set of\nrelevant factors present in the cases. Critically, on the abstention test (Test\n3), most models failed to follow instructions to stop, instead generating\nspurious arguments despite the lack of common factors. This automated pipeline\nprovides a scalable method for assessing these crucial LLM behaviors,\nhighlighting the need for improvements in factor utilization and robust\nabstention capabilities before reliable deployment in legal settings. Link:\nhttps://lizhang-aiandlaw.github.io/An-Automated-Pipeline-for-Evaluating-LLM-Generated-3-ply-Case-Based-Legal-Arguments/",
      "authors": [
        "Li Zhang",
        "Morgan Gray",
        "Jaromir Savelka",
        "Kevin D. Ashley"
      ],
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG",
        "68T50"
      ],
      "links": [
        "http://arxiv.org/abs/2506.00694v2",
        "http://arxiv.org/pdf/2506.00694v2"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2506.00680v1",
      "title": "Understanding the European energy crisis through structural causal\n  models",
      "published": "2025-05-31T19:14:46Z",
      "updated": "2025-05-31T19:14:46Z",
      "summary": "Natural gas supplies in Europe were disrupted and energy prices soared in the\ncontext of Russia's invasion of Ukraine. Electricity prices in France\nexperienced the largest relative increase among European countries, even though\nnatural gas plays a negligible role in the French electricity system. In this\narticle, we demonstrate the importance of causal statistical methods and\npropose causal graphs to investigate the French electricity market and pinpoint\nkey influencing factors on electricity prices and net exports. We demonstrate\nthat a causal approach resolves paradoxical results of simple correlation\nstudies and enables a quantitative analysis of indirect causal effects. We\nintroduce a linear structural causal model as well as non-linear tree-based\nmachine learning combined with Shapley flows. The models elucidate the\ninterplay of gas prices and the unavailability of nuclear power plants during\nthe energy crisis: The high unavailability made France dependent on imports and\nlinked prices to neighbouring countries.",
      "authors": [
        "Anton Tausendfreund",
        "Sarah Schreyer",
        "Florian Immig",
        "Ulrich Oberhofer",
        "Julius Trebbien",
        "Aaron Praktiknjo",
        "Benjamin Sch\u00e4fer",
        "Dirk Witthaut"
      ],
      "categories": [
        "stat.AP"
      ],
      "links": [
        "http://arxiv.org/abs/2506.00680v1",
        "http://arxiv.org/pdf/2506.00680v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2506.00679v1",
      "title": "CineMA: A Foundation Model for Cine Cardiac MRI",
      "published": "2025-05-31T19:12:34Z",
      "updated": "2025-05-31T19:12:34Z",
      "summary": "Cardiac magnetic resonance (CMR) is a key investigation in clinical\ncardiovascular medicine and has been used extensively in population research.\nHowever, extracting clinically important measurements such as ejection fraction\nfor diagnosing cardiovascular diseases remains time-consuming and subjective.\nWe developed CineMA, a foundation AI model automating these tasks with limited\nlabels. CineMA is a self-supervised autoencoder model trained on 74,916 cine\nCMR studies to reconstruct images from masked inputs. After fine-tuning, it was\nevaluated across eight datasets on 23 tasks from four categories: ventricle and\nmyocardium segmentation, left and right ventricle ejection fraction\ncalculation, disease detection and classification, and landmark localisation.\nCineMA is the first foundation model for cine CMR to match or outperform\nconvolutional neural networks (CNNs). CineMA demonstrated greater label\nefficiency than CNNs, achieving comparable or better performance with fewer\nannotations. This reduces the burden of clinician labelling and supports\nreplacing task-specific training with fine-tuning foundation models in future\ncardiac imaging applications. Models and code for pre-training and fine-tuning\nare available at https://github.com/mathpluscode/CineMA, democratising access\nto high-performance models that otherwise require substantial computational\nresources, promoting reproducibility and accelerating clinical translation.",
      "authors": [
        "Yunguan Fu",
        "Weixi Yi",
        "Charlotte Manisty",
        "Anish N Bhuva",
        "Thomas A Treibel",
        "James C Moon",
        "Matthew J Clarkson",
        "Rhodri Huw Davies",
        "Yipeng Hu"
      ],
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "links": [
        "http://arxiv.org/abs/2506.00679v1",
        "http://arxiv.org/pdf/2506.00679v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2506.00664v1",
      "title": "OntoRAG: Enhancing Question-Answering through Automated Ontology\n  Derivation from Unstructured Knowledge Bases",
      "published": "2025-05-31T18:33:39Z",
      "updated": "2025-05-31T18:33:39Z",
      "summary": "Ontologies are pivotal for structuring knowledge bases to enhance question\nanswering (QA) systems powered by Large Language Models (LLMs). However,\ntraditional ontology creation relies on manual efforts by domain experts, a\nprocess that is time intensive, error prone, and impractical for large, dynamic\nknowledge domains. This paper introduces OntoRAG, an automated pipeline\ndesigned to derive ontologies from unstructured knowledge bases, with a focus\non electrical relay documents. OntoRAG integrates advanced techniques,\nincluding web scraping, PDF parsing, hybrid chunking, information extraction,\nknowledge graph construction, and ontology creation, to transform unstructured\ndata into a queryable ontology. By leveraging LLMs and graph based methods,\nOntoRAG enhances global sensemaking capabilities, outperforming conventional\nRetrieval Augmented Generation (RAG) and GraphRAG approaches in\ncomprehensiveness and diversity. Experimental results demonstrate OntoRAGs\neffectiveness, achieving a comprehensiveness win rate of 85% against vector RAG\nand 75% against GraphRAGs best configuration. This work addresses the critical\nchallenge of automating ontology creation, advancing the vision of the semantic\nweb.",
      "authors": [
        "Yash Tiwari",
        "Owais Ahmad Lone",
        "Mayukha Pal"
      ],
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "links": [
        "http://arxiv.org/abs/2506.00664v1",
        "http://arxiv.org/pdf/2506.00664v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2506.03193v1",
      "title": "Human Fall Detection using Transfer Learning-based 3D CNN",
      "published": "2025-05-31T16:58:12Z",
      "updated": "2025-05-31T16:58:12Z",
      "summary": "Unintentional or accidental falls are one of the significant health issues in\nsenior persons. The population of senior persons is increasing steadily. So,\nthere is a need for an automated fall detection monitoring system. This paper\nintroduces a vision-based fall detection system using a pre-trained 3D CNN.\nUnlike 2D CNN, 3D CNN extracts not only spatial but also temporal features. The\nproposed model leverages the original learned weights of a 3D CNN model\npre-trained on the Sports1M dataset to extract the spatio-temporal features.\nOnly the SVM classifier was trained, which saves the time required to train the\n3D CNN. Stratified shuffle five split cross-validation has been used to split\nthe dataset into training and testing data. Extracted features from the\nproposed 3D CNN model were fed to an SVM classifier to classify the activity as\nfall or ADL. Two datasets, GMDCSA and CAUCAFall, were utilized to conduct the\nexperiment. The source code for this work can be accessed via the following\nlink: https://github.com/ekramalam/HFD_3DCNN.",
      "authors": [
        "Ekram Alam",
        "Abu Sufian",
        "Paramartha Dutta",
        "Marco Leo"
      ],
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "links": [
        "http://dx.doi.org/10.1007/978-3-031-81935-3_9",
        "http://arxiv.org/abs/2506.03193v1",
        "http://arxiv.org/pdf/2506.03193v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2506.00633v1",
      "title": "Text-to-CT Generation via 3D Latent Diffusion Model with Contrastive\n  Vision-Language Pretraining",
      "published": "2025-05-31T16:41:55Z",
      "updated": "2025-05-31T16:41:55Z",
      "summary": "Objective: While recent advances in text-conditioned generative models have\nenabled the synthesis of realistic medical images, progress has been largely\nconfined to 2D modalities such as chest X-rays. Extending text-to-image\ngeneration to volumetric Computed Tomography (CT) remains a significant\nchallenge, due to its high dimensionality, anatomical complexity, and the\nabsence of robust frameworks that align vision-language data in 3D medical\nimaging. Methods: We introduce a novel architecture for Text-to-CT generation\nthat combines a latent diffusion model with a 3D contrastive vision-language\npretraining scheme. Our approach leverages a dual-encoder CLIP-style model\ntrained on paired CT volumes and radiology reports to establish a shared\nembedding space, which serves as the conditioning input for generation. CT\nvolumes are compressed into a low-dimensional latent space via a pretrained\nvolumetric VAE, enabling efficient 3D denoising diffusion without requiring\nexternal super-resolution stages. Results: We evaluate our method on the\nCT-RATE dataset and conduct a comprehensive assessment of image fidelity,\nclinical relevance, and semantic alignment. Our model achieves competitive\nperformance across all tasks, significantly outperforming prior baselines for\ntext-to-CT generation. Moreover, we demonstrate that CT scans synthesized by\nour framework can effectively augment real data, improving downstream\ndiagnostic performance. Conclusion: Our results show that modality-specific\nvision-language alignment is a key component for high-quality 3D medical image\ngeneration. By integrating contrastive pretraining and volumetric diffusion,\nour method offers a scalable and controllable solution for synthesizing\nclinically meaningful CT volumes from text, paving the way for new applications\nin data augmentation, medical education, and automated clinical simulation.",
      "authors": [
        "Daniele Molino",
        "Camillo Maria Caruso",
        "Filippo Ruffini",
        "Paolo Soda",
        "Valerio Guarrasi"
      ],
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "links": [
        "http://arxiv.org/abs/2506.00633v1",
        "http://arxiv.org/pdf/2506.00633v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2506.00613v1",
      "title": "Evaluating Robot Policies in a World Model",
      "published": "2025-05-31T15:51:56Z",
      "updated": "2025-05-31T15:51:56Z",
      "summary": "Robotics has broad applications from automating house chores to taking care\nof patients. However, evaluating robot control policies is challenging, as\nreal-world testing is expensive, while handcrafted simulations often fail to\naccurately reflect real-world conditions, resulting in poor correlation between\nsimulated evaluation and real-world outcomes. In this work, we investigate\nWorld-model-based Policy Evaluation (WPE). We first train an action-conditioned\nvideo generation model as a proxy to real-world environments. To enable\nefficient rollouts of hundreds of interactive steps while mitigating error\naccumulation in the world model, we propose an inference scheme which we call\nBlockwise-Autoregressive Diffusion Transformer with adjustable context and\ndecoding horizon lengths. To ensure that the world model indeed follows action\ninput, we propose metrics based on the agreement between the ground truth video\nand generated video conditioned on the same sequence of actions to evaluate the\nworld model. We then use the world model for policy evaluation by performing\nMonte Carlo rollouts in the world model while employing a vision-language model\n(VLM) as a reward function. Interestingly, we found that WPE tends to\nunderestimate the policy values for in-distribution actions and overestimate\npolicy values for out-of-distribution actions. Nevertheless, WPE preserves the\nrelative rankings of different policies. In emulating real robot executions,\nWPE achieves high fidelity in mimicing robot arm movements as in real videos,\nwhile emulating highly realistic object interaction remains challenging.\nDespite this limitation, we show that a world model can serve as a starting\npoint for evaluating robot policies before real-world deployment.",
      "authors": [
        "Julian Quevedo",
        "Percy Liang",
        "Sherry Yang"
      ],
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "links": [
        "http://arxiv.org/abs/2506.00613v1",
        "http://arxiv.org/pdf/2506.00613v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2506.00608v1",
      "title": "PAKTON: A Multi-Agent Framework for Question Answering in Long Legal\n  Agreements",
      "published": "2025-05-31T15:38:21Z",
      "updated": "2025-05-31T15:38:21Z",
      "summary": "Contract review is a complex and time-intensive task that typically demands\nspecialized legal expertise, rendering it largely inaccessible to non-experts.\nMoreover, legal interpretation is rarely straightforward-ambiguity is\npervasive, and judgments often hinge on subjective assessments. Compounding\nthese challenges, contracts are usually confidential, restricting their use\nwith proprietary models and necessitating reliance on open-source alternatives.\nTo address these challenges, we introduce PAKTON: a fully open-source,\nend-to-end, multi-agent framework with plug-and-play capabilities. PAKTON is\ndesigned to handle the complexities of contract analysis through collaborative\nagent workflows and a novel retrieval-augmented generation (RAG) component,\nenabling automated legal document review that is more accessible, adaptable,\nand privacy-preserving. Experiments demonstrate that PAKTON outperforms both\ngeneral-purpose and pretrained models in predictive accuracy, retrieval\nperformance, explainability, completeness, and grounded justifications as\nevaluated through a human study and validated with automated metrics.",
      "authors": [
        "Petros Raptopoulos",
        "Giorgos Filandrianos",
        "Maria Lymperaiou",
        "Giorgos Stamou"
      ],
      "categories": [
        "cs.CL"
      ],
      "links": [
        "http://arxiv.org/abs/2506.00608v1",
        "http://arxiv.org/pdf/2506.00608v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2506.00577v1",
      "title": "Reasoning Like an Economist: Post-Training on Economic Problems Induces\n  Strategic Generalization in LLMs",
      "published": "2025-05-31T14:22:40Z",
      "updated": "2025-05-31T14:22:40Z",
      "summary": "Directly training Large Language Models (LLMs) for Multi-Agent Systems (MAS)\nremains challenging due to intricate reward modeling, dynamic agent\ninteractions, and demanding generalization requirements. This paper explores\nwhether post-training techniques, specifically Supervised Fine-Tuning (SFT) and\nReinforcement Learning with Verifiable Rewards (RLVR), can effectively\n$\\textit{generalize}$ to multi-agent scenarios. We use economic reasoning as a\ntestbed, leveraging its strong foundations in mathematics and game theory, its\ndemand for structured analytical reasoning, and its relevance to real-world\napplications such as market design, resource allocation, and policy analysis.\nWe introduce $\\textbf{Recon}$ ($\\textbf{R}$easoning like an\n$\\textbf{ECON}$omist), a 7B-parameter open-source LLM post-trained on a\nhand-curated dataset of 2,100 high-quality economic reasoning problems.\nComprehensive evaluation on economic reasoning benchmarks and multi-agent games\nreveals clear improvements in structured reasoning and economic rationality.\nThese results underscore the promise of domain-aligned post-training for\nenhancing reasoning and agent alignment, shedding light on the roles of SFT and\nRL in shaping model behavior. Code is available at\nhttps://github.com/MasterZhou1/Recon .",
      "authors": [
        "Yufa Zhou",
        "Shaobo Wang",
        "Xingyu Dong",
        "Xiangqi Jin",
        "Yifang Chen",
        "Yue Min",
        "Kexin Yang",
        "Xingzhang Ren",
        "Dayiheng Liu",
        "Linfeng Zhang"
      ],
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.GT",
        "cs.MA"
      ],
      "links": [
        "http://arxiv.org/abs/2506.00577v1",
        "http://arxiv.org/pdf/2506.00577v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2506.00572v1",
      "title": "Machine-learning Growth at Risk",
      "published": "2025-05-31T14:06:53Z",
      "updated": "2025-05-31T14:06:53Z",
      "summary": "We analyse growth vulnerabilities in the US using quantile partial\ncorrelation regression, a selection-based machine-learning method that achieves\nmodel selection consistency under time series. We find that downside risk is\nprimarily driven by financial, labour-market, and housing variables, with their\nimportance changing over time. Decomposing downside risk into its individual\ncomponents, we construct sector-specific indices that predict it, while\ncontrolling for information from other sectors, thereby isolating the downside\nrisks emanating from each sector.",
      "authors": [
        "Tobias Adrian",
        "Hongqi Chen",
        "Max-Sebastian Dov\u00ec",
        "Ji Hyung Lee"
      ],
      "categories": [
        "econ.GN",
        "q-fin.EC"
      ],
      "links": [
        "http://arxiv.org/abs/2506.00572v1",
        "http://arxiv.org/pdf/2506.00572v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2506.00543v1",
      "title": "Hydrodynamical modelling of Type IIb SNe",
      "published": "2025-05-31T13:06:26Z",
      "updated": "2025-05-31T13:06:26Z",
      "summary": "We present HYDE, a new one-dimensional hydrodynamical code, and use it to\nconstruct a grid of supernova (SN) models based on solar-metallicity bare\nhelium-core models evolved to the verge of core-collapse with MESA STAR. This\ngrid is suited to model Type IIb SNe, which progenitor stars are thought to\nhave lost all but a tiny fraction of their hydrogen envelopes. Using an\nautomated procedure we fit the bolometric lightcurves and photospheric\nvelocities for a large sample of (17) Type IIb SNe to the grid of SN models. We\nfind that the distribution of initial masses for the sample can be reasonably\nwell described by a standard Salpeter IMF, although there is an\nunder-population in the >25 M$_\\odot$ range. The fractions of SNe with initial\nmasses <15 M$_\\odot$ and <20 M$_\\odot$ are 56 and 81 percent, respectively,\nsuggesting either the binary channel to dominate the production of Type IIb SNe\nor a flaw in our understanding of single-star mass-loss. We find correlations\nbetween the explosion energy, initial mass and mass of $^{56}$Ni; the explosion\nenergy increases with initial mass and the mass of $^{56}$Ni increases with\nexplosion energy. The method used allows us to determine the errors in the\nmodel parameters arising from the observed quantities and the degeneracy of the\nsolution. We find that an error in the distance and extinction propagates\nmainly to the derived mass of $^{56}$Ni, whereas an error in the photospheric\nvelocity propagates mainly to the derived helium-core mass and explosion\nenergy. Fits using the bolometric lightcurve alone are completely degenerate\nalong the M$_{\\mathrm{ej}}^{2}$/E$_{\\mathrm{ej}}$=const curve, whereas fits\nusing also the photospheric velocities are quite robust for well-sampled SNe.\nFinally, we provide a description and tests of the HYDE code, and a discussion\nof the limitations of the method used.",
      "authors": [
        "Mattias Ergon",
        "Maximilian Stritzinger",
        "Francesco Taddia",
        "Jesper Sollerman",
        "Claes Fransson"
      ],
      "categories": [
        "astro-ph.SR",
        "astro-ph.HE"
      ],
      "links": [
        "http://arxiv.org/abs/2506.00543v1",
        "http://arxiv.org/pdf/2506.00543v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2506.00532v1",
      "title": "Generative AI and Organizational Structure in the Knowledge Economy",
      "published": "2025-05-31T12:33:08Z",
      "updated": "2025-05-31T12:33:08Z",
      "summary": "The adoption of GenAI is fundamentally reshaping organizations in the\nknowledge economy. GenAI can significantly enhance workers' problem-solving\nabilities and productivity, yet it also presents a major reliability challenge:\nhallucinations, or errors presented as plausible outputs. This study develops a\ntheoretical model to examine GenAI's impact on organizational structure and the\nrole of human-in-the-loop oversight. Our findings indicate that successful\nGenAI adoption hinges primarily on maintaining hallucination rates below a\ncritical level. After adoption, as GenAI advances in capability or reliability,\norganizations optimize their workforce by reducing worker knowledge\nrequirements while preserving operational effectiveness through GenAI\naugmentation-a phenomenon known as deskilling. Unexpectedly, enhanced\ncapability or reliability of GenAI may actually narrow the span of control,\nincreasing the demand for managers rather than flattening organizational\nhierarchies. To effectively mitigate hallucination risks, many firms implement\nhuman-in-the-loop validation, where managers review GenAI-enhanced outputs\nbefore implementation. While the validation increases managerial workload, it\ncan, surprisingly, expand the span of control, reducing the number of managers\nneeded. Furthermore, human-in-the-loop validation influences GenAI adoption\ndifferently based on validation costs and hallucination rates, deterring\nadoption in low-error, high-cost scenarios, while promoting it in high-error,\nlow-cost cases. Finally, productivity improvements from GenAI yield distinctive\norganizational shifts: as productivity increases, firms tend to employ fewer\nbut more knowledgeable workers, gradually expanding managerial spans of\ncontrol.",
      "authors": [
        "Fasheng Xu",
        "Jing Hou",
        "Wei Chen",
        "Karen Xie"
      ],
      "categories": [
        "econ.TH"
      ],
      "links": [
        "http://arxiv.org/abs/2506.00532v1",
        "http://arxiv.org/pdf/2506.00532v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2506.00520v1",
      "title": "Temac: Multi-Agent Collaboration for Automated Web GUI Testing",
      "published": "2025-05-31T11:43:37Z",
      "updated": "2025-05-31T11:43:37Z",
      "summary": "Quality assurance of web applications is critical, as web applications play\nan essential role in people's daily lives. To reduce labor costs, automated web\nGUI testing (AWGT) is widely adopted, exploring web applications via GUI\nactions such as clicks and text inputs. However, these approaches face\nlimitations in generating continuous and meaningful action sequences capable of\ncovering complex functionalities. Recent work incorporates large language\nmodels (LLMs) for GUI testing. However, these approaches face various\nchallenges, including low efficiency of LLMs, high complexity of rich web\napplication contexts, and a low success rate of LLMs in executing GUI tasks.\n  To address these challenges, in this paper, we propose Temac, an approach\nthat enhances AWGT using LLM-based multi-agent collaboration to increase code\ncoverage. Temac is motivated by our insight that LLMs can enhance AWGT in\nexecuting complex functionalities, while the information discovered during AWGT\ncan, in turn, be provided as the domain knowledge to improve the LLM-based task\nexecution. Specifically, given a web application, Temac initially runs an\nexisting approach to broadly explore application states. When the testing\ncoverage stagnates, Temac then employs LLM-based agents to summarize the\ncollected information to form a knowledge base and to infer not-covered\nfunctionalities. Guided by this knowledge base, Temac finally uses specialized\nLLM-based agents to target and execute the not-covered functionalities,\nreaching deeper states beyond those explored by the existing approach.\n  Our evaluation results show that Temac exceeds state-of-the-art approaches\nfrom 12.5% to 60.3% on average code coverage on six complex open-source web\napplications, while revealing 445 unique failures in the top 20 real-world web\napplications. These results strongly demonstrate the effectiveness and the\ngeneral applicability of Temac.",
      "authors": [
        "Chenxu Liu",
        "Zhiyu Gu",
        "Guoquan Wu",
        "Ying Zhang",
        "Jun Wei",
        "Tao Xie"
      ],
      "categories": [
        "cs.SE"
      ],
      "links": [
        "http://arxiv.org/abs/2506.00520v1",
        "http://arxiv.org/pdf/2506.00520v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2506.00505v1",
      "title": "From Rules to Rewards: Reinforcement Learning for Interest Rate\n  Adjustment in DeFi Lending",
      "published": "2025-05-31T10:56:07Z",
      "updated": "2025-05-31T10:56:07Z",
      "summary": "Decentralized Finance (DeFi) lending enables permissionless borrowing via\nsmart contracts. However, it faces challenges in optimizing interest rates,\nmitigating bad debt, and improving capital efficiency. Rule-based interest-rate\nmodels struggle to adapt to dynamic market conditions, leading to\ninefficiencies. This work applies Offline Reinforcement Learning (RL) to\noptimize interest rate adjustments in DeFi lending protocols. Using historical\ndata from Aave protocol, we evaluate three RL approaches: Conservative\nQ-Learning (CQL), Behavior Cloning (BC), and TD3 with Behavior Cloning\n(TD3-BC). TD3-BC demonstrates superior performance in balancing utilization,\ncapital stability, and risk, outperforming existing models. It adapts\neffectively to historical stress events like the May 2021 crash and the March\n2023 USDC depeg, showcasing potential for automated, real-time governance.",
      "authors": [
        "Hanxiao Qu",
        "Krzysztof Gogol",
        "Florian Groetschla",
        "Claudio Tessone"
      ],
      "categories": [
        "cs.LG"
      ],
      "links": [
        "http://arxiv.org/abs/2506.00505v1",
        "http://arxiv.org/pdf/2506.00505v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2506.00498v1",
      "title": "UNSURF: Uncertainty Quantification for Cortical Surface Reconstruction\n  of Clinical Brain MRIs",
      "published": "2025-05-31T10:31:51Z",
      "updated": "2025-05-31T10:31:51Z",
      "summary": "We propose UNSURF, a novel uncertainty measure for cortical surface\nreconstruction of clinical brain MRI scans of any orientation, resolution, and\ncontrast. It relies on the discrepancy between predicted voxel-wise signed\ndistance functions (SDFs) and the actual SDFs of the fitted surfaces. Our\nexperiments on real clinical scans show that traditional uncertainty measures,\nsuch as voxel-wise Monte Carlo variance, are not suitable for modeling the\nuncertainty of surface placement. Our results demonstrate that UNSURF estimates\ncorrelate well with the ground truth errors and: \\textit{(i)}~enable effective\nautomated quality control of surface reconstructions at the subject-, parcel-,\nmesh node-level; and \\textit{(ii)}~improve performance on a downstream\nAlzheimer's disease classification task.",
      "authors": [
        "Raghav Mehta",
        "Karthik Gopinath",
        "Ben Glocker",
        "Juan Eugenio Iglesias"
      ],
      "categories": [
        "eess.IV",
        "cs.CV"
      ],
      "links": [
        "http://arxiv.org/abs/2506.00498v1",
        "http://arxiv.org/pdf/2506.00498v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2506.00490v2",
      "title": "LLM-Driven Instance-Specific Heuristic Generation and Selection",
      "published": "2025-05-31T09:54:36Z",
      "updated": "2025-06-03T03:22:26Z",
      "summary": "Combinatorial optimization problems are widely encountered in real-world\napplications. Designing high-quality heuristic algorithms that efficiently\napproximate optimal solutions within reasonable time is a critical research\nchallenge. In recent years, many works have explored integrating Large Language\nModels (LLMs) with Evolutionary Algorithms to automate heuristic algorithm\ndesign through prompt engineering. However, these approaches generally adopt a\nproblem-specific paradigm, applying a single algorithm across all problem\ninstances, failing to account for the heterogeneity across instances. In this\npaper, we propose InstSpecHH, a novel framework that introduces the concept of\ninstance-specific heuristic generation. InstSpecHH partitions the overall\nproblem class into sub-classes based on instance features and performs\ndifferentiated, automated heuristic design for each problem subclass. By\ntailoring heuristics to the unique features of different sub-classes,\nInstSpecHH achieves better performance at the problem class level while\navoiding redundant heuristic generation for similar instances, thus reducing\ncomputational overhead. This approach effectively balances the trade-off\nbetween the cost of automatic heuristic design and the quality of the obtained\nsolutions. To evaluate the performance of InstSpecHH, we conduct experiments on\n4,500 subclasses of the Online Bin Packing Problem (OBPP) and 365 subclasses of\nthe Capacitated Vehicle Routing Problem (CVRP). Experimental results show that\nInstSpecHH demonstrates strong intra-subclass and inter-subclass generalization\ncapabilities. Compared to previous problem-specific methods, InstSpecHH reduces\nthe average optimality gap by more than 5.6\\% for OBPP and 0.9\\% for CVRP.\nThese results highlight the potential of instance-aware automatic heuristic\ndesign to further enhance solution quality.",
      "authors": [
        "Shaofeng Zhang",
        "Shengcai Liu",
        "Ning Lu",
        "Jiahao Wu",
        "Ji Liu",
        "Yew-Soon Ong",
        "Ke Tang"
      ],
      "categories": [
        "cs.NE"
      ],
      "links": [
        "http://arxiv.org/abs/2506.00490v2",
        "http://arxiv.org/pdf/2506.00490v2"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2506.00467v1",
      "title": "SST: Self-training with Self-adaptive Thresholding for Semi-supervised\n  Learning",
      "published": "2025-05-31T08:34:04Z",
      "updated": "2025-05-31T08:34:04Z",
      "summary": "Neural networks have demonstrated exceptional performance in supervised\nlearning, benefiting from abundant high-quality annotated data. However,\nobtaining such data in real-world scenarios is costly and labor-intensive.\nSemi-supervised learning (SSL) offers a solution to this problem. Recent\nstudies, such as Semi-ViT and Noisy Student, which employ consistency\nregularization or pseudo-labeling, have demonstrated significant achievements.\nHowever, they still face challenges, particularly in accurately selecting\nsufficient high-quality pseudo-labels due to their reliance on fixed\nthresholds. Recent methods such as FlexMatch and FreeMatch have introduced\nflexible or self-adaptive thresholding techniques, greatly advancing SSL\nresearch. Nonetheless, their process of updating thresholds at each iteration\nis deemed time-consuming, computationally intensive, and potentially\nunnecessary. To address these issues, we propose Self-training with\nSelf-adaptive Thresholding (SST), a novel, effective, and efficient SSL\nframework. SST introduces an innovative Self-Adaptive Thresholding (SAT)\nmechanism that adaptively adjusts class-specific thresholds based on the\nmodel's learning progress. SAT ensures the selection of high-quality\npseudo-labeled data, mitigating the risks of inaccurate pseudo-labels and\nconfirmation bias. Extensive experiments demonstrate that SST achieves\nstate-of-the-art performance with remarkable efficiency, generalization, and\nscalability across various architectures and datasets. Semi-SST-ViT-Huge\nachieves the best results on competitive ImageNet-1K SSL benchmarks, with 80.7%\n/ 84.9% Top-1 accuracy using only 1% / 10% labeled data. Compared to the\nfully-supervised DeiT-III-ViT-Huge, which achieves 84.8% Top-1 accuracy using\n100% labeled data, our method demonstrates superior performance using only 10%\nlabeled data.",
      "authors": [
        "Shuai Zhao",
        "Heyan Huang",
        "Xinge Li",
        "Xiaokang Chen",
        "Rui Wang"
      ],
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "links": [
        "http://dx.doi.org/10.1016/j.ipm.2025.104158",
        "http://arxiv.org/abs/2506.00467v1",
        "http://arxiv.org/pdf/2506.00467v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2506.00455v1",
      "title": "Diffusion Models for Increasing Accuracy in Olfaction Sensors and\n  Datasets",
      "published": "2025-05-31T08:22:09Z",
      "updated": "2025-05-31T08:22:09Z",
      "summary": "Robotic odour source localization (OSL) is a critical capability for\nautonomous systems operating in complex environments. However, current OSL\nmethods often suffer from ambiguities, particularly when robots misattribute\nodours to incorrect objects due to limitations in olfactory datasets and sensor\nresolutions. To address this challenge, we introduce a novel machine learning\nmethod using diffusion-based molecular generation to enhance odour localization\naccuracy that can be used by itself or with automated olfactory dataset\nconstruction pipelines with vision-language models (VLMs) This generative\nprocess of our diffusion model expands the chemical space beyond the\nlimitations of both current olfactory datasets and the training data of VLMs,\nenabling the identification of potential odourant molecules not previously\ndocumented. The generated molecules can then be more accurately validated using\nadvanced olfactory sensors which emulate human olfactory recognition through\nelectronic sensor arrays. By integrating visual analysis, language processing,\nand molecular generation, our framework enhances the ability of\nolfaction-vision models on robots to accurately associate odours with their\ncorrect sources, thereby improving navigation and decision-making in\nenvironments where olfactory cues are essential. Our methodology represents a\nfoundational advancement in the field of robotic olfaction, offering a scalable\nsolution to the challenges posed by limited olfactory data and sensor\nambiguities.",
      "authors": [
        "Kordel K. France",
        "Ovidiu Daescu"
      ],
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "links": [
        "http://arxiv.org/abs/2506.00455v1",
        "http://arxiv.org/pdf/2506.00455v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2506.00454v1",
      "title": "Towards Temporally Explainable Dysarthric Speech Clarity Assessment",
      "published": "2025-05-31T08:16:54Z",
      "updated": "2025-05-31T08:16:54Z",
      "summary": "Dysarthria, a motor speech disorder, affects intelligibility and requires\ntargeted interventions for effective communication. In this work, we\ninvestigate automated mispronunciation feedback by collecting a dysarthric\nspeech dataset from six speakers reading two passages, annotated by a speech\ntherapist with temporal markers and mispronunciation descriptions. We design a\nthree-stage framework for explainable mispronunciation evaluation: (1) overall\nclarity scoring, (2) mispronunciation localization, and (3) mispronunciation\ntype classification. We systematically analyze pretrained Automatic Speech\nRecognition (ASR) models in each stage, assessing their effectiveness in\ndysarthric speech evaluation (Code available at:\nhttps://github.com/augmented-human-lab/interspeech25_speechtherapy,\nSupplementary webpage: https://apps.ahlab.org/interspeech25_speechtherapy/).\nOur findings offer clinically relevant insights for automating actionable\nfeedback for pronunciation assessment, which could enable independent practice\nfor patients and help therapists deliver more effective interventions.",
      "authors": [
        "Seohyun Park",
        "Chitralekha Gupta",
        "Michelle Kah Yian Kwan",
        "Xinhui Fung",
        "Alexander Wenjun Yip",
        "Suranga Nanayakkara"
      ],
      "categories": [
        "eess.AS",
        "cs.HC",
        "cs.SD"
      ],
      "links": [
        "http://arxiv.org/abs/2506.00454v1",
        "http://arxiv.org/pdf/2506.00454v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2506.00426v1",
      "title": "Hybrid Cloud Security: Balancing Performance, Cost, and Compliance in\n  Multi-Cloud Deployments",
      "published": "2025-05-31T07:04:08Z",
      "updated": "2025-05-31T07:04:08Z",
      "summary": "The pervasive use of hybrid cloud computing models has changed enterprise as\nwell as Information Technology services infrastructure by giving businesses\nsimple and cost-effective options of combining on-premise IT equipment with\npublic cloud services. hybrid cloud solutions deploy multifaceted models of\nsecurity, performance optimization, and cost efficiency, conventionally\nfragmented in the cloud computing milieu. This paper examines how organizations\nmanage these parameters in hybrid cloud ecosystems while providing solutions to\nthe challenges they face in operationalizing hybrid cloud adoptions. The study\ncaptures the challenges of achieving a balance in resource distribution between\non-premise and cloud resources (herein referred to as the \"resource allocation\nchallenge\"), the complexity of pricing models from cloud providers like AWS,\nMicrosoft Azure, Google Cloud (herein called the 'pricing complexity problem'),\nand the urgency for strong security infrastructure to safeguard sensitive\ninformation (known as 'the information security problem'). This study\ndemonstrates the security and performance management solutions proposed were\nvalidated in a detailed case study of adoption of AWS and Azure based hybrid\ncloud and provides useful guidance. Also, a hybrid cloud security and cost\noptimization framework based on zero trust architecture, encryption, hybrid\ncloud policies, and others, is proposed.\n  The conclusion includes recommendations for research on automation of hybrid\ncloud service management, integration of multi-clouds, and the ever-present\nquestion of data privacy, stressing how those matters affect contemporary\nenterprises.",
      "authors": [
        "Anjani kumar Polinati"
      ],
      "categories": [
        "cs.CR"
      ],
      "links": [
        "http://arxiv.org/abs/2506.00426v1",
        "http://arxiv.org/pdf/2506.00426v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2506.00386v1",
      "title": "Adaptive-VP: A Framework for LLM-Based Virtual Patients that Adapts to\n  Trainees' Dialogue to Facilitate Nurse Communication Training",
      "published": "2025-05-31T04:34:55Z",
      "updated": "2025-05-31T04:34:55Z",
      "summary": "Effective communication training is essential to preparing nurses for\nhigh-quality patient care. While standardized patient (SP) simulations provide\nvaluable experiential learning, they are often costly and inflexible. Virtual\npatient (VP) systems offer a scalable alternative, but most fail to adapt to\nthe varying communication skills of trainees. In particular, when trainees\nrespond ineffectively, VPs should escalate in hostility or become\nuncooperative--yet this level of adaptive interaction remains largely\nunsupported. To address this gap, we introduce Adaptive-VP, a VP dialogue\ngeneration framework that leverages large language models (LLMs) to dynamically\nadapt VP behavior based on trainee input. The framework features a pipeline for\nconstructing clinically grounded yet flexible VP scenarios and a modular system\nfor assessing trainee communication and adjusting VP responses in real time,\nwhile ensuring learner safety. We validated Adaptive-VP by simulating\nchallenging patient conversations. Automated evaluation using a corpus from\npracticing nurses showed that our communication skill evaluation mechanism\nreflected real-world proficiency levels. Expert nurses further confirmed that\nAdaptive-VP produced more natural and realistic interactions than existing\napproaches, demonstrating its potential as a scalable and effective tool for\nnursing communication training.",
      "authors": [
        "Keyeun Lee",
        "Seolhee Lee",
        "Esther Hehsun Kim",
        "Yena Ko",
        "Jinsu Eun",
        "Dahee Kim",
        "Hyewon Cho",
        "Haiyi Zhu",
        "Robert E. Kraut",
        "Eunyoung Suh",
        "Eun-mee Kim",
        "Hajin Lim"
      ],
      "categories": [
        "cs.CL",
        "cs.HC"
      ],
      "links": [
        "http://arxiv.org/abs/2506.00386v1",
        "http://arxiv.org/pdf/2506.00386v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2506.02037v1",
      "title": "FinS-Pilot: A Benchmark for Online Financial System",
      "published": "2025-05-31T03:50:19Z",
      "updated": "2025-05-31T03:50:19Z",
      "summary": "Large language models (LLMs) have demonstrated remarkable capabilities across\nvarious professional domains, with their performance typically evaluated\nthrough standardized benchmarks. However, the development of financial RAG\nbenchmarks has been constrained by data confidentiality issues and the lack of\ndynamic data integration. To address this issue, we introduces FinS-Pilot, a\nnovel benchmark for evaluating RAG systems in online financial applications.\nConstructed from real-world financial assistant interactions, our benchmark\nincorporates both real-time API data and structured text sources, organized\nthrough an intent classification framework covering critical financial domains\nsuch as equity analysis and macroeconomic forecasting. The benchmark enables\ncomprehensive evaluation of financial assistants' capabilities in handling both\nstatic knowledge and time-sensitive market information. Through systematic\nexperiments with multiple Chinese leading LLMs, we demonstrate FinS-Pilot's\neffectiveness in identifying models suitable for financial applications while\naddressing the current gap in specialized evaluation tools for the financial\ndomain. Our work contributes both a practical evaluation framework and a\ncurated dataset to advance research in financial NLP systems. The code and\ndataset are accessible on\nGitHub\\footnote{https://github.com/PhealenWang/financial\\_rag\\_benchmark}.",
      "authors": [
        "Feng Wang",
        "Yiding Sun",
        "Jiaxin Mao",
        "Wei Xue",
        "Danqing Xu"
      ],
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "links": [
        "http://arxiv.org/abs/2506.02037v1",
        "http://arxiv.org/pdf/2506.02037v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2506.00305v1",
      "title": "Learning Aerodynamics for the Control of Flying Humanoid Robots",
      "published": "2025-05-30T23:27:44Z",
      "updated": "2025-05-30T23:27:44Z",
      "summary": "Robots with multi-modal locomotion are an active research field due to their\nversatility in diverse environments. In this context, additional actuation can\nprovide humanoid robots with aerial capabilities. Flying humanoid robots face\nchallenges in modeling and control, particularly with aerodynamic forces. This\npaper addresses these challenges from a technological and scientific\nstandpoint. The technological contribution includes the mechanical design of\niRonCub-Mk1, a jet-powered humanoid robot, optimized for jet engine\nintegration, and hardware modifications for wind tunnel experiments on humanoid\nrobots for precise aerodynamic forces and surface pressure measurements. The\nscientific contribution offers a comprehensive approach to model and control\naerodynamic forces using classical and learning techniques. Computational Fluid\nDynamics (CFD) simulations calculate aerodynamic forces, validated through wind\ntunnel experiments on iRonCub-Mk1. An automated CFD framework expands the\naerodynamic dataset, enabling the training of a Deep Neural Network and a\nlinear regression model. These models are integrated into a simulator for\ndesigning aerodynamic-aware controllers, validated through flight simulations\nand balancing experiments on the iRonCub-Mk1 physical prototype.",
      "authors": [
        "Antonello Paolino",
        "Gabriele Nava",
        "Fabio Di Natale",
        "Fabio Bergonti",
        "Punith Reddy Vanteddu",
        "Donato Grassi",
        "Luca Riccobene",
        "Alex Zanotti",
        "Renato Tognaccini",
        "Gianluca Iaccarino",
        "Daniele Pucci"
      ],
      "categories": [
        "cs.RO",
        "cs.LG"
      ],
      "links": [
        "http://arxiv.org/abs/2506.00305v1",
        "http://arxiv.org/pdf/2506.00305v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2506.00274v1",
      "title": "Chances and Challenges of the Model Context Protocol in Digital\n  Forensics and Incident Response",
      "published": "2025-05-30T22:15:48Z",
      "updated": "2025-05-30T22:15:48Z",
      "summary": "Large language models hold considerable promise for supporting forensic\ninvestigations, but their widespread adoption is hindered by a lack of\ntransparency, explainability, and reproducibility. This paper explores how the\nemerging Model Context Protocol can address these challenges and support the\nmeaningful use of LLMs in digital forensics. Through a theoretical analysis, we\nexamine how MCP can be integrated across various forensic scenarios - ranging\nfrom artifact analysis to the generation of interpretable reports. We also\noutline both technical and conceptual considerations for deploying an MCP\nserver in forensic environments. Our analysis reveals a wide range of use cases\nin which MCP not only strengthens existing forensic workflows but also\nfacilitates the application of LLMs to areas of forensics where their use was\npreviously limited. Furthermore, we introduce the concept of the inference\nconstraint level - a way of characterizing how specific MCP design choices can\ndeliberately constrain model behavior, thereby enhancing both auditability and\ntraceability. Our insights demonstrate that MCP has significant potential as a\nfoundational component for developing LLM-assisted forensic workflows that are\nnot only more transparent, reproducible, and legally defensible, but also\nrepresent a step toward increased automation in digital forensic analysis.\nHowever, we also highlight potential challenges that the adoption of MCP may\npose for digital forensics in the future.",
      "authors": [
        "Jan-Niclas Hilgert",
        "Carlo Jakobs",
        "Michael K\u00fclper",
        "Martin Lambertz",
        "Axel Mahr",
        "Elmar Padilla"
      ],
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "links": [
        "http://arxiv.org/abs/2506.00274v1",
        "http://arxiv.org/pdf/2506.00274v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2506.00256v1",
      "title": "The Impact of Disability Disclosure on Fairness and Bias in LLM-Driven\n  Candidate Selection",
      "published": "2025-05-30T21:44:17Z",
      "updated": "2025-05-30T21:44:17Z",
      "summary": "As large language models (LLMs) become increasingly integrated into hiring\nprocesses, concerns about fairness have gained prominence. When applying for\njobs, companies often request/require demographic information, including\ngender, race, and disability or veteran status. This data is collected to\nsupport diversity and inclusion initiatives, but when provided to LLMs,\nespecially disability-related information, it raises concerns about potential\nbiases in candidate selection outcomes. Many studies have highlighted how\ndisability can impact CV screening, yet little research has explored the\nspecific effect of voluntarily disclosed information on LLM-driven candidate\nselection. This study seeks to bridge that gap. When candidates shared\nidentical gender, race, qualifications, experience, and backgrounds, and sought\njobs with minimal employment rate gaps between individuals with and without\ndisabilities (e.g., Cashier, Software Developer), LLMs consistently favored\ncandidates who disclosed that they had no disability. Even in cases where\ncandidates chose not to disclose their disability status, the LLMs were less\nlikely to select them compared to those who explicitly stated they did not have\na disability.",
      "authors": [
        "Mahammed Kamruzzaman",
        "Gene Louis Kim"
      ],
      "categories": [
        "cs.CL"
      ],
      "links": [
        "http://arxiv.org/abs/2506.00256v1",
        "http://arxiv.org/pdf/2506.00256v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2506.00249v1",
      "title": "MIR: Methodology Inspiration Retrieval for Scientific Research Problems",
      "published": "2025-05-30T21:33:03Z",
      "updated": "2025-05-30T21:33:03Z",
      "summary": "There has been a surge of interest in harnessing the reasoning capabilities\nof Large Language Models (LLMs) to accelerate scientific discovery. While\nexisting approaches rely on grounding the discovery process within the relevant\nliterature, effectiveness varies significantly with the quality and nature of\nthe retrieved literature. We address the challenge of retrieving prior work\nwhose concepts can inspire solutions for a given research problem, a task we\ndefine as Methodology Inspiration Retrieval (MIR). We construct a novel dataset\ntailored for training and evaluating retrievers on MIR, and establish\nbaselines. To address MIR, we build the Methodology Adjacency Graph (MAG);\ncapturing methodological lineage through citation relationships. We leverage\nMAG to embed an \"intuitive prior\" into dense retrievers for identifying\npatterns of methodological inspiration beyond superficial semantic similarity.\nThis achieves significant gains of +5.4 in Recall@3 and +7.8 in Mean Average\nPrecision (mAP) over strong baselines. Further, we adapt LLM-based re-ranking\nstrategies to MIR, yielding additional improvements of +4.5 in Recall@3 and\n+4.8 in mAP. Through extensive ablation studies and qualitative analyses, we\nexhibit the promise of MIR in enhancing automated scientific discovery and\noutline avenues for advancing inspiration-driven retrieval.",
      "authors": [
        "Aniketh Garikaparthi",
        "Manasi Patwardhan",
        "Aditya Sanjiv Kanade",
        "Aman Hassan",
        "Lovekesh Vig",
        "Arman Cohan"
      ],
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "links": [
        "http://arxiv.org/abs/2506.00249v1",
        "http://arxiv.org/pdf/2506.00249v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2506.00246v1",
      "title": "Automatic detection of overshooting tops and their properties from\n  visible satellite channels",
      "published": "2025-05-30T21:24:10Z",
      "updated": "2025-05-30T21:24:10Z",
      "summary": "Overshooting tops (OTs) are critical indicators of convective storm intensity\nand are widely utilized in meteorological analyses. This study presents an\nautomated algorithm for OT detection and OT height estimation using\nconvolutional neural networks applied to visible satellite imagery. The models\nare trained and validated on an extensive OT dataset comprising approximately\n10,000 manually detected cases over Europe. The OTs were identified from\nhigh-resolution visible (HRV) channel of the SEVIRI instrument on board the MSG\ngeostationary satellite, with the heights determined from the length of their\nshadows in the imagery. While conventional OT detection methods primarily rely\non the identification of cold features in thermal infrared channels, our\napproach extracts information from visible channels, leveraging the ground\ntruth data on OT shadow length provided by the training dataset. In the morning\nand afternoon hours, when the shadows are visible, the proposed models detect\nOTs with a probability of detection reaching 95% and estimate their height with\nan average error of 0.25 km. The performance is expected to further improve\nonce the model is applied to polar and new generation geostationary satellite\nwith increased spatial resolution.",
      "authors": [
        "Ane\u017eka Dole\u017ealov\u00e1",
        "Jakub Seidl",
        "Jind\u0159ich \u0160\u0165\u00e1stka",
        "J\u00e1n Ka\u0148\u00e1k"
      ],
      "categories": [
        "physics.ao-ph"
      ],
      "links": [
        "http://arxiv.org/abs/2506.00246v1",
        "http://arxiv.org/pdf/2506.00246v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2506.00233v1",
      "title": "Ethical AI: Towards Defining a Collective Evaluation Framework",
      "published": "2025-05-30T21:10:47Z",
      "updated": "2025-05-30T21:10:47Z",
      "summary": "Artificial Intelligence (AI) is transforming sectors such as healthcare,\nfinance, and autonomous systems, offering powerful tools for innovation. Yet\nits rapid integration raises urgent ethical concerns related to data ownership,\nprivacy, and systemic bias. Issues like opaque decision-making, misleading\noutputs, and unfair treatment in high-stakes domains underscore the need for\ntransparent and accountable AI systems. This article addresses these challenges\nby proposing a modular ethical assessment framework built on ontological blocks\nof meaning-discrete, interpretable units that encode ethical principles such as\nfairness, accountability, and ownership. By integrating these blocks with FAIR\n(Findable, Accessible, Interoperable, Reusable) principles, the framework\nsupports scalable, transparent, and legally aligned ethical evaluations,\nincluding compliance with the EU AI Act. Using a real-world use case in\nAI-powered investor profiling, the paper demonstrates how the framework enables\ndynamic, behavior-informed risk classification. The findings suggest that\nontological blocks offer a promising path toward explainable and auditable AI\nethics, though challenges remain in automation and probabilistic reasoning.",
      "authors": [
        "Aasish Kumar Sharma",
        "Dimitar Kyosev",
        "Julian Kunkel"
      ],
      "categories": [
        "cs.AI"
      ],
      "links": [
        "http://arxiv.org/abs/2506.00233v1",
        "http://arxiv.org/pdf/2506.00233v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2506.02034v1",
      "title": "High-throughput viscometry via machine-learning from videos of inverted\n  vials",
      "published": "2025-05-30T20:45:05Z",
      "updated": "2025-05-30T20:45:05Z",
      "summary": "Although the inverted vial test has been widely used as a qualitative method\nfor estimating fluid viscosity, quantitative rheological characterization has\nremained limited due to its complex, uncontrolled flow - driven by gravity,\nsurface tension, inertia, and initial conditions. Here, we present a computer\nvision (CV) viscometer that automates the inverted vial test and enables\nquantitative viscosity inference across nearly five orders of magnitude\n(0.01-1000 Pas), without requiring direct velocity field measurements. The\nsystem simultaneously inverts multiple vials and records videos of the evolving\nfluid, which are fed into a neural network that approximates the inverse\nfunction from visual features and known fluid density. Despite the complex,\nmulti-regime flow within the vial, our approach achieves relative errors below\n25%, improving to 15% for viscosities above 0.1 Pas. When tested on\nnon-Newtonian polymer solutions, the method reliably estimates zero-shear\nviscosity as long as viscoelastic or shear-thinning behaviors remain negligible\nwithin the flow regime. Moreover, high standard deviations in the inferred\nvalues may serve as a proxy for identifying fluids with strong non-Newtonian\nbehavior. The CV viscometer requires only one camera and one motor, is\ncontactless and low-cost, and can be easily integrated into high-throughput\nexperimental automated and manual workflows. Transcending traditional\ncharacterization paradigms, our method leverages uncontrolled flows and visual\nfeatures to achieve simplicity and scalability, enabling high-throughput\nviscosity inference that can meet the growing demand of data-driven material\nmodels while remaining accessible to lower resource environments.",
      "authors": [
        "Ignacio Arretche",
        "Mohammad Tanver Hossain",
        "Ramdas Tiwari",
        "Abbie Kim",
        "Mya G. Mills",
        "Connor D. Armstrong",
        "Jacob J. Lessard",
        "Sameh H. Tawfick",
        "Randy H. Ewoldt"
      ],
      "categories": [
        "cs.GR"
      ],
      "links": [
        "http://arxiv.org/abs/2506.02034v1",
        "http://arxiv.org/pdf/2506.02034v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2506.00215v1",
      "title": "Symbolic Hamiltonian Compiler for Hybrid Qubit-Boson Processors",
      "published": "2025-05-30T20:41:50Z",
      "updated": "2025-05-30T20:41:50Z",
      "summary": "Quantum simulation of the interactions of fermions and bosons -- the\nfundamental particles of nature -- is essential for modeling complex quantum\nsystems in material science, chemistry and high-energy physics and has been\nproposed as a promising application of fermion-boson quantum computers, which\novercome the overhead encountered in mapping fermions and bosons to qubits.\nHowever, compiling the simulation of specific fermion-boson Hamiltonians into\nthe natively available fermion-boson gate set is challenging. In particular,\nthe large local dimension of bosons renders matrix-based compilation methods,\nas used for qubits and in existing tools such as Bosonic Qiskit or OpenFermion,\nchallenging. We overcome this issue by introducing a novel symbolic compiler\nbased on matrix-free symbolic manipulation of second quantised Hamiltonians,\nwhich automates the decomposition of fermion-boson second quantized problems\ninto qubit-boson instruction set architectures. This integration establishes a\ncomprehensive pipeline for simulating quantum systems on emerging qubit-boson\nand fermion-boson hardware, paving the way for their large-scale usage.",
      "authors": [
        "Ethan Decker",
        "Erik Gustafson",
        "Evan McKinney",
        "Alex K. Jones",
        "Lucas Goetz",
        "Ang Li",
        "Alexander Schuckert",
        "Samuel Stein",
        "Gushu Li",
        "Eleanor Crane"
      ],
      "categories": [
        "quant-ph",
        "cond-mat.mtrl-sci",
        "cond-mat.str-el"
      ],
      "links": [
        "http://arxiv.org/abs/2506.00215v1",
        "http://arxiv.org/pdf/2506.00215v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2506.00206v1",
      "title": "Residual Income Valuation and Stock Returns: Evidence from a\n  Value-to-Price Investment Strategy",
      "published": "2025-05-30T20:26:37Z",
      "updated": "2025-05-30T20:26:37Z",
      "summary": "We hypothesize that portfolio sorts based on the V/P ratio generate excess\nreturns and consist of companies that are undervalued for prolonged periods.\nResults, for the US market show that high V/P portfolios outperform low V/P\nportfolios across horizons extending from one to three years. The V/P ratio is\npositively correlated to future stock returns after controlling for firm\ncharacteristics, which are well known risk proxies. Findings also indicate that\nprofitability and investment add explanatory power to the Fama and French three\nfactor model and for stocks with V/P ratio close to 1. However, these factors\ncannot explain all variation in excess returns especially for years two and\nthree and for stocks with high V/P ratio. Finally, portfolios with the highest\nV/P stocks select companies that are significantly mispriced relative to their\nequity (investment) and profitability growth persistence in the future.",
      "authors": [
        "Ahmad Haboub",
        "Aris Kartsaklas",
        "Vasilis Sarafidis"
      ],
      "categories": [
        "econ.EM"
      ],
      "links": [
        "http://arxiv.org/abs/2506.00206v1",
        "http://arxiv.org/pdf/2506.00206v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2506.00178v1",
      "title": "Tournament of Prompts: Evolving LLM Instructions Through Structured\n  Debates and Elo Ratings",
      "published": "2025-05-30T19:33:41Z",
      "updated": "2025-05-30T19:33:41Z",
      "summary": "Prompt engineering represents a critical bottleneck to harness the full\npotential of Large Language Models (LLMs) for solving complex tasks, as it\nrequires specialized expertise, significant trial-and-error, and manual\nintervention. This challenge is particularly pronounced for tasks involving\nsubjective quality assessment, where defining explicit optimization objectives\nbecomes fundamentally problematic. Existing automated prompt optimization\nmethods falter in these scenarios, as they typically require well-defined\ntask-specific numerical fitness functions or rely on generic templates that\ncannot capture the nuanced requirements of complex use cases. We introduce\nDEEVO (DEbate-driven EVOlutionary prompt optimization), a novel framework that\nguides prompt evolution through a debate-driven evaluation with an Elo-based\nselection. Contrary to prior work, DEEVOs approach enables exploration of the\ndiscrete prompt space while preserving semantic coherence through intelligent\ncrossover and strategic mutation operations that incorporate debate-based\nfeedback, combining elements from both successful and unsuccessful prompts\nbased on identified strengths rather than arbitrary splicing. Using Elo ratings\nas a fitness proxy, DEEVO simultaneously drives improvement and preserves\nvaluable diversity in the prompt population. Experimental results demonstrate\nthat DEEVO significantly outperforms both manual prompt engineering and\nalternative state-of-the-art optimization approaches on open-ended tasks and\nclose-ended tasks despite using no ground truth feedback. By connecting LLMs\nreasoning capabilities with adaptive optimization, DEEVO represents a\nsignificant advancement in prompt optimization research by eliminating the need\nof predetermined metrics to continuously improve AI systems.",
      "authors": [
        "Anirudh Nair",
        "Adi Banerjee",
        "Laurent Mombaerts",
        "Matthew Hagen",
        "Tarik Borogovac"
      ],
      "categories": [
        "cs.AI",
        "cs.NE"
      ],
      "links": [
        "http://arxiv.org/abs/2506.00178v1",
        "http://arxiv.org/pdf/2506.00178v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2506.00169v1",
      "title": "Utilizing AI for Aviation Post-Accident Analysis Classification",
      "published": "2025-05-30T19:15:04Z",
      "updated": "2025-05-30T19:15:04Z",
      "summary": "The volume of textual data available in aviation safety reports presents a\nchallenge for timely and accurate analysis. This paper examines how Artificial\nIntelligence (AI) and, specifically, Natural Language Processing (NLP) can\nautomate the process of extracting valuable insights from this data, ultimately\nenhancing aviation safety. The paper reviews ongoing efforts focused on the\napplication of NLP and deep learning to aviation safety reports, with the goal\nof classifying the level of damage to an aircraft and identifying the phase of\nflight during which safety occurrences happen. Additionally, the paper explores\nthe use of Topic Modeling (TM) to uncover latent thematic structures within\naviation incident reports, aiming to identify recurring patterns and potential\nareas for safety improvement. The paper compares and contrasts the performance\nof various deep learning models and TM techniques applied to datasets from the\nNational Transportation Safety Board (NTSB) and the Australian Transport Safety\nBureau (ATSB), as well as the Aviation Safety Network (ASN), discussing the\nimpact of dataset size and source on the accuracy of the analysis. The findings\ndemonstrate that both NLP and deep learning, as well as TM, can significantly\nimprove the efficiency and accuracy of aviation safety analysis, paving the way\nfor more proactive safety management and risk mitigation strategies.",
      "authors": [
        "Aziida Nanyonga",
        "Graham Wild"
      ],
      "categories": [
        "cs.AI"
      ],
      "links": [
        "http://arxiv.org/abs/2506.00169v1",
        "http://arxiv.org/pdf/2506.00169v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2506.00128v1",
      "title": "Applying Large Language Models to Issue Classification: Revisiting with\n  Extended Data and New Models",
      "published": "2025-05-30T18:02:55Z",
      "updated": "2025-05-30T18:02:55Z",
      "summary": "Effective prioritization of issue reports in software engineering helps to\noptimize resource allocation and information recovery. However, manual issue\nclassification is laborious and lacks scalability. As an alternative, many open\nsource software (OSS) projects employ automated processes for this task, yet\nthis method often relies on large datasets for adequate training.\nTraditionally, machine learning techniques have been used for issue\nclassification. More recently, large language models (LLMs) have emerged as\npowerful tools for addressing a range of software engineering challenges,\nincluding code and test generation, mapping new requirements to legacy software\nendpoints, and conducting code reviews. The following research investigates an\nautomated approach to issue classification based on LLMs. By leveraging the\ncapabilities of such models, we aim to develop a robust system for prioritizing\nissue reports, mitigating the necessity for extensive training data while also\nmaintaining reliability in classification. In our research, we developed an\nLLM-based approach for accurately labeling issues by selecting two of the most\nprominent large language models. We then compared their performance across\nmultiple datasets. Our findings show that GPT-4o achieved the best results in\nclassifying issues from the NLBSE 2024 competition. Moreover, GPT-4o\noutperformed DeepSeek R1, achieving an F1 score 20% higher when both models\nwere trained on the same dataset from the NLBSE 2023 competition, which was ten\ntimes larger than the NLBSE 2024 dataset. The fine-tuned GPT-4o model attained\nan average F1 score of 80.7%, while the fine-tuned DeepSeek R1 model achieved\n59.33%. Increasing the dataset size did not improve the F1 score, reducing the\ndependence on massive datasets for building an efficient solution to issue\nclassification.",
      "authors": [
        "Gabriel Aracena",
        "Kyle Luster",
        "Fabio Santos",
        "Igor Steinmacher",
        "Marco A. Gerosa"
      ],
      "categories": [
        "cs.SE",
        "cs.LG"
      ],
      "links": [
        "http://arxiv.org/abs/2506.00128v1",
        "http://arxiv.org/pdf/2506.00128v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2506.00117v1",
      "title": "Binarity at LOw Metallicity (BLOeM): Pipeline-Determined Physical\n  Properties of OB Stars",
      "published": "2025-05-30T18:00:02Z",
      "updated": "2025-05-30T18:00:02Z",
      "summary": "We aim to determine the physical properties of OB stars from the multi-epoch\nVLT/FLAMES BLOeM spectroscopic survey of the Small Magellanic Cloud. We apply a\npipeline designed to analyse large spectroscopic samples of OB stars to the\nco-added, initial 9 epochs of the BLOeM survey, utilising grids of synthetic\nmodel spectra computed with the stellar atmosphere code FASTWIND. 69 OB stars\nare excluded from the analysis owing to disk emission or significant\ncontamination by secondaries in SB2 binaries. We determine physical properties\nof 778 OB stars, including Teff, log g, log L/Lsun and v_e sin i. There appears\nto be a bimodality in v_e sin i of single O stars, while v_e sin i\ndistributions of OB stars are strikingly different for single (median 78 km/s)\nand binary (median 200 km/s) systems. Inferred temperatures are broadly in\nagreement with literature results for stars in common, plus results from a\ngrid-based automization tool for a subset of O and early B stars, although\nuncertainties are larger for surface gravities. Rotational velocities are\nbroadly in line with an independent tool applied to the same subset. We recover\nthe anticipated lower mass cutoff at 8 Msun from the survey design using a\nBayesian inference method coupled with SMC metallicity evolutionary models,\nwith median masses of 12.6 Msun (19.8 Msun) for B-type (O-type) stars.\nSpectroscopic masses exceed evolutionary masses, albeit with large\nuncertainties in surface gravities. We also provide an updated catalogue of O\nstars in the SMC since half of the 159 BLOeM O stars are newly classified as\nO-type stars.",
      "authors": [
        "J. M. Bestenlehner",
        "Paul A. Crowther",
        "V. A. Bronner",
        "S. Simon-Diaz",
        "D. J. Lennon",
        "J. Bodensteiner",
        "N. Langer",
        "P. Marchant",
        "H. Sana",
        "F. R. N. Schneider",
        "T. Shenar"
      ],
      "categories": [
        "astro-ph.SR",
        "astro-ph.GA"
      ],
      "links": [
        "http://arxiv.org/abs/2506.00117v1",
        "http://arxiv.org/pdf/2506.00117v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2506.02032v1",
      "title": "Towards Secure MLOps: Surveying Attacks, Mitigation Strategies, and\n  Research Challenges",
      "published": "2025-05-30T17:45:31Z",
      "updated": "2025-05-30T17:45:31Z",
      "summary": "The rapid adoption of machine learning (ML) technologies has driven\norganizations across diverse sectors to seek efficient and reliable methods to\naccelerate model development-to-deployment. Machine Learning Operations (MLOps)\nhas emerged as an integrative approach addressing these requirements by\nunifying relevant roles and streamlining ML workflows. As the MLOps market\ncontinues to grow, securing these pipelines has become increasingly critical.\nHowever, the unified nature of MLOps ecosystem introduces vulnerabilities,\nmaking them susceptible to adversarial attacks where a single misconfiguration\ncan lead to compromised credentials, severe financial losses, damaged public\ntrust, and the poisoning of training data. Our paper presents a systematic\napplication of the MITRE ATLAS (Adversarial Threat Landscape for\nArtificial-Intelligence Systems) framework, a comprehensive and continuously\nupdated catalog of AI-focused attacks, to systematically assess attacks across\ndifferent phases of the MLOps ecosystem. We begin by examining the preparatory\nphases during which adversaries acquire the essential intelligence required to\ninitiate their attacks. We then present a structured taxonomy of attack\ntechniques explicitly mapped to corresponding phases of the MLOps ecosystem,\nsupported by examples drawn from red-teaming exercises and real-world\nincidents. This is followed by a taxonomy of mitigation strategies aligned with\nthese attack categories, offering actionable early-stage defenses to strengthen\nthe security of MLOps ecosystem. Given the rapid evolution and adoption of\nMLOps, we further highlight key research gaps that require immediate attention.\nOur work emphasizes the importance of implementing robust security protocols\nfrom the outset, empowering practitioners to safeguard MLOps ecosystem against\nevolving cyber attacks.",
      "authors": [
        "Raj Patel",
        "Himanshu Tripathi",
        "Jasper Stone",
        "Noorbakhsh Amiri Golilarz",
        "Sudip Mittal",
        "Shahram Rahimi",
        "Vini Chaudhary"
      ],
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "links": [
        "http://arxiv.org/abs/2506.02032v1",
        "http://arxiv.org/pdf/2506.02032v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2505.24838v1",
      "title": "VideoCAD: A Large-Scale Video Dataset for Learning UI Interactions and\n  3D Reasoning from CAD Software",
      "published": "2025-05-30T17:39:52Z",
      "updated": "2025-05-30T17:39:52Z",
      "summary": "Computer-Aided Design (CAD) is a time-consuming and complex process,\nrequiring precise, long-horizon user interactions with intricate 3D interfaces.\nWhile recent advances in AI-driven user interface (UI) agents show promise,\nmost existing datasets and methods focus on short, low-complexity tasks in\nmobile or web applications, failing to capture the demands of professional\nengineering tools. In this work, we introduce VideoCAD, the first attempt at\nengineering UI interaction learning for precision tasks. Specifically, VideoCAD\nis a large-scale synthetic dataset consisting of over 41K annotated video\nrecordings of CAD operations, generated using an automated framework for\ncollecting high-fidelity UI action data from human-made CAD designs. Compared\nto existing datasets, VideoCAD offers an order of magnitude higher complexity\nin UI interaction learning for real-world engineering tasks, having up to a 20x\nlonger time horizon than other datasets. We show two important downstream\napplications of VideoCAD: learning UI interactions from professional precision\n3D CAD tools and a visual question-answering (VQA) benchmark designed to\nevaluate multimodal large language models' (LLM) spatial reasoning and video\nunderstanding abilities. To learn the UI interactions, we propose\nVideoCADFormer - a state-of-the-art model in learning CAD interactions directly\nfrom video, which outperforms multiple behavior cloning baselines. Both\nVideoCADFormer and the VQA benchmark derived from VideoCAD reveal key\nchallenges in the current state of video-based UI understanding, including the\nneed for precise action grounding, multi-modal and spatial reasoning, and\nlong-horizon dependencies.",
      "authors": [
        "Brandon Man",
        "Ghadi Nehme",
        "Md Ferdous Alam",
        "Faez Ahmed"
      ],
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "links": [
        "http://arxiv.org/abs/2505.24838v1",
        "http://arxiv.org/pdf/2505.24838v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2505.24831v1",
      "title": "Optimising cryptocurrency portfolios through stable clustering of price\n  correlation networks",
      "published": "2025-05-30T17:33:12Z",
      "updated": "2025-05-30T17:33:12Z",
      "summary": "The emerging cryptocurrency market presents unique challenges for investment\ndue to its unregulated nature and inherent volatility. However, collective\nprice movements can be explored to maximise profits with minimal risk using\ninvestment portfolios. In this paper, we develop a technical framework that\nutilises historical data on daily closing prices and integrates network\nanalysis, price forecasting, and portfolio theory to identify cryptocurrencies\nfor building profitable portfolios under uncertainty. Our method utilises the\nLouvain network community algorithm and consensus clustering to detect robust\nand temporally stable clusters of highly correlated cryptocurrencies, from\nwhich the chosen cryptocurrencies are selected. A price prediction step using\nthe ARIMA model guarantees that the portfolio performs well for up to 14 days\nin the investment horizon. Empirical analysis over a 5-year period shows that\ndespite the high volatility in the crypto market, hidden price patterns can be\neffectively utilised to generate consistently profitable, time-agnostic\ncryptocurrency portfolios.",
      "authors": [
        "Ruixue Jing",
        "Ryota Kobayashi",
        "Luis Enrique Correa Rocha"
      ],
      "categories": [
        "physics.pop-ph",
        "physics.soc-ph",
        "q-fin.PM"
      ],
      "links": [
        "http://arxiv.org/abs/2505.24831v1",
        "http://arxiv.org/pdf/2505.24831v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2505.24830v1",
      "title": "Improving Reliability and Explainability of Medical Question Answering\n  through Atomic Fact Checking in Retrieval-Augmented LLMs",
      "published": "2025-05-30T17:33:07Z",
      "updated": "2025-05-30T17:33:07Z",
      "summary": "Large language models (LLMs) exhibit extensive medical knowledge but are\nprone to hallucinations and inaccurate citations, which pose a challenge to\ntheir clinical adoption and regulatory compliance. Current methods, such as\nRetrieval Augmented Generation, partially address these issues by grounding\nanswers in source documents, but hallucinations and low fact-level\nexplainability persist. In this work, we introduce a novel atomic fact-checking\nframework designed to enhance the reliability and explainability of LLMs used\nin medical long-form question answering. This method decomposes LLM-generated\nresponses into discrete, verifiable units called atomic facts, each of which is\nindependently verified against an authoritative knowledge base of medical\nguidelines. This approach enables targeted correction of errors and direct\ntracing to source literature, thereby improving the factual accuracy and\nexplainability of medical Q&A. Extensive evaluation using multi-reader\nassessments by medical experts and an automated open Q&A benchmark demonstrated\nsignificant improvements in factual accuracy and explainability. Our framework\nachieved up to a 40% overall answer improvement and a 50% hallucination\ndetection rate. The ability to trace each atomic fact back to the most relevant\nchunks from the database provides a granular, transparent explanation of the\ngenerated responses, addressing a major gap in current medical AI applications.\nThis work represents a crucial step towards more trustworthy and reliable\nclinical applications of LLMs, addressing key prerequisites for clinical\napplication and fostering greater confidence in AI-assisted healthcare.",
      "authors": [
        "Juraj Vladika",
        "Annika Domres",
        "Mai Nguyen",
        "Rebecca Moser",
        "Jana Nano",
        "Felix Busch",
        "Lisa C. Adams",
        "Keno K. Bressem",
        "Denise Bernhardt",
        "Stephanie E. Combs",
        "Kai J. Borm",
        "Florian Matthes",
        "Jan C. Peeken"
      ],
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "links": [
        "http://arxiv.org/abs/2505.24830v1",
        "http://arxiv.org/pdf/2505.24830v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2505.24803v2",
      "title": "Guiding Generative Storytelling with Knowledge Graphs",
      "published": "2025-05-30T17:08:21Z",
      "updated": "2025-06-02T17:37:17Z",
      "summary": "Large Language Models (LLMs) have shown great potential in automated story\ngeneration, but challenges remain in maintaining long-form coherence and\nproviding users with intuitive and effective control. Retrieval-Augmented\nGeneration (RAG) has proven effective in reducing hallucinations in text\ngeneration; however, the use of structured data to support generative\nstorytelling remains underexplored. This paper investigates how knowledge\ngraphs (KGs) can enhance LLM-based storytelling by improving narrative quality\nand enabling user-driven modifications. We propose a KG-assisted storytelling\npipeline and evaluate its effectiveness through a user study with 15\nparticipants. Participants created their own story prompts, generated stories,\nand edited knowledge graphs to shape their narratives. Through quantitative and\nqualitative analysis, our findings demonstrate that knowledge graphs\nsignificantly enhance story quality in action-oriented and structured\nnarratives within our system settings. Additionally, editing the knowledge\ngraph increases users' sense of control, making storytelling more engaging,\ninteractive, and playful.",
      "authors": [
        "Zhijun Pan",
        "Antonios Andronis",
        "Eva Hayek",
        "Oscar AP Wilkinson",
        "Ilya Lasy",
        "Annette Parry",
        "Guy Gadney",
        "Tim J. Smith",
        "Mick Grierson"
      ],
      "categories": [
        "cs.CL",
        "cs.HC"
      ],
      "links": [
        "http://arxiv.org/abs/2505.24803v2",
        "http://arxiv.org/pdf/2505.24803v2"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2505.24787v1",
      "title": "Draw ALL Your Imagine: A Holistic Benchmark and Agent Framework for\n  Complex Instruction-based Image Generation",
      "published": "2025-05-30T16:48:14Z",
      "updated": "2025-05-30T16:48:14Z",
      "summary": "Recent advancements in text-to-image (T2I) generation have enabled models to\nproduce high-quality images from textual descriptions. However, these models\noften struggle with complex instructions involving multiple objects,\nattributes, and spatial relationships. Existing benchmarks for evaluating T2I\nmodels primarily focus on general text-image alignment and fail to capture the\nnuanced requirements of complex, multi-faceted prompts. Given this gap, we\nintroduce LongBench-T2I, a comprehensive benchmark specifically designed to\nevaluate T2I models under complex instructions. LongBench-T2I consists of 500\nintricately designed prompts spanning nine diverse visual evaluation\ndimensions, enabling a thorough assessment of a model's ability to follow\ncomplex instructions. Beyond benchmarking, we propose an agent framework\n(Plan2Gen) that facilitates complex instruction-driven image generation without\nrequiring additional model training. This framework integrates seamlessly with\nexisting T2I models, using large language models to interpret and decompose\ncomplex prompts, thereby guiding the generation process more effectively. As\nexisting evaluation metrics, such as CLIPScore, fail to adequately capture the\nnuances of complex instructions, we introduce an evaluation toolkit that\nautomates the quality assessment of generated images using a set of\nmulti-dimensional metrics. The data and code are released at\nhttps://github.com/yczhou001/LongBench-T2I.",
      "authors": [
        "Yucheng Zhou",
        "Jiahao Yuan",
        "Qianning Wang"
      ],
      "categories": [
        "cs.CV",
        "cs.CL"
      ],
      "links": [
        "http://arxiv.org/abs/2505.24787v1",
        "http://arxiv.org/pdf/2505.24787v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2505.24731v1",
      "title": "Circuit Stability Characterizes Language Model Generalization",
      "published": "2025-05-30T15:53:56Z",
      "updated": "2025-05-30T15:53:56Z",
      "summary": "Extensively evaluating the capabilities of (large) language models is\ndifficult. Rapid development of state-of-the-art models induce benchmark\nsaturation, while creating more challenging datasets is labor-intensive.\nInspired by the recent developments in mechanistic interpretability, we\nintroduce circuit stability as a new way to assess model performance. Circuit\nstability refers to a model's ability to apply a consistent reasoning\nprocess-its circuit-across various inputs. We mathematically formalize circuit\nstability and circuit equivalence. Then, through three case studies, we\nempirically show that circuit stability and the lack thereof can characterize\nand predict different aspects of generalization. Our proposed methods offer a\nstep towards rigorously relating the generality of models to their\ninterpretability.",
      "authors": [
        "Alan Sun"
      ],
      "categories": [
        "cs.CL"
      ],
      "links": [
        "http://arxiv.org/abs/2505.24731v1",
        "http://arxiv.org/pdf/2505.24731v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2505.24716v1",
      "title": "Towards Scalable Schema Mapping using Large Language Models",
      "published": "2025-05-30T15:36:56Z",
      "updated": "2025-05-30T15:36:56Z",
      "summary": "The growing need to integrate information from a large number of diverse\nsources poses significant scalability challenges for data integration systems.\nThese systems often rely on manually written schema mappings, which are\ncomplex, source-specific, and costly to maintain as sources evolve. While\nrecent advances suggest that large language models (LLMs) can assist in\nautomating schema matching by leveraging both structural and natural language\ncues, key challenges remain. In this paper, we identify three core issues with\nusing LLMs for schema mapping: (1) inconsistent outputs due to sensitivity to\ninput phrasing and structure, which we propose methods to address through\nsampling and aggregation techniques; (2) the need for more expressive mappings\n(e.g., GLaV), which strain the limited context windows of LLMs; and (3) the\ncomputational cost of repeated LLM calls, which we propose to mitigate through\nstrategies like data type prefiltering.",
      "authors": [
        "Christopher Buss",
        "Mahdis Safari",
        "Arash Termehchy",
        "Stefan Lee",
        "David Maier"
      ],
      "categories": [
        "cs.DB",
        "cs.AI"
      ],
      "links": [
        "http://arxiv.org/abs/2505.24716v1",
        "http://arxiv.org/pdf/2505.24716v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2505.24708v1",
      "title": "Efficient Bayesian multi-fidelity inverse analysis for expensive and\n  non-differentiable physics-based simulations in high stochastic dimensions",
      "published": "2025-05-30T15:29:36Z",
      "updated": "2025-05-30T15:29:36Z",
      "summary": "High-dimensional Bayesian inverse analysis (dim >> 100) is mostly unfeasible\nfor computationally demanding, nonlinear physics-based high-fidelity (HF)\nmodels. Usually, the use of more efficient gradient-based inference schemes is\nimpeded if the multi-physics models are provided by complex legacy codes.\nAdjoint-based derivatives are either exceedingly cumbersome to derive or\nnon-existent for practically relevant large-scale nonlinear and coupled\nmulti-physics problems. Similarly, holistic automated differentiation w.r.t.\nprimary variables of multi-physics codes is usually not yet an option and\nrequires extensive code restructuring if not considered from the outset in the\nsoftware design. This absence of differentiability further exacerbates the\nalready present computational challenges. To overcome the existing limitations,\nwe propose a novel inference approach called Bayesian multi-fidelity inverse\nanalysis (BMFIA), which leverages simpler and computationally cheaper\nlower-fidelity (LF) models that are designed to provide model derivatives.\nBMFIA learns a simple, probabilistic dependence of the LF and HF models, which\nis then employed in an altered likelihood formulation to statistically correct\nthe inaccurate LF response. From a Bayesian viewpoint, this dependence\nrepresents a multi-fidelity conditional density (discriminative model). We\ndemonstrate how this multi-fidelity conditional density can be learned robustly\nin the small data regime from only a few HF and LF simulations (50 to 300),\nwhich would not be sufficient for naive surrogate approaches. The formulation\nis fully differentiable and allows the flexible design of a wide range of LF\nmodels. We demonstrate that BMFIA solves Bayesian inverse problems for\nscenarios that used to be prohibitive, such as finely-resolved spatial\nreconstruction problems for nonlinear and transient coupled poro-elastic media\nphysics.",
      "authors": [
        "Jonas Nitzler",
        "Bugrahan Z. Tem\u00fcr",
        "Phaedon-Stelios Koutsourelakis",
        "Wolfgang A. Wall"
      ],
      "categories": [
        "cs.CE"
      ],
      "links": [
        "http://arxiv.org/abs/2505.24708v1",
        "http://arxiv.org/pdf/2505.24708v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2505.24698v1",
      "title": "Next Generation Authentication for Data Spaces: An Authentication Flow\n  Based On Grant Negotiation And Authorization Protocol For Verifiable\n  Presentations (GNAP4VP)",
      "published": "2025-05-30T15:20:39Z",
      "updated": "2025-05-30T15:20:39Z",
      "summary": "Identity verification in Data Spaces is a fundamental aspect of ensuring\nsecurity and privacy in digital environments. This paper presents an identity\nverification protocol tailored for shared data environments within Data Spaces.\nThis protocol extends the Grant Negotiation and Authorization Protocol (GNAP)\nand integrates OpenID Connect for Verifiable Presentations (OIDC4VP) along with\nsupport for Linked Verifiable Presentations (LVP), providing a robust\nfoundation for secure and privacy-preserving interactions. The proposed\nsolution adheres to the principles of Self-Sovereign Identity (SSI) to\nfacilitate decentralized, user-centric identity management while maintaining\nflexibility through protocol negotiation. Two alternative interaction flows are\nintroduced: a \"Wallet-Driven Interaction\" utilizing OIDC4VP, and a \"LVP\nAuthorization\" model for fully automated machine-to-machine communication.\nThese flows address critical challenges encountered in Data Spaces, including\nprivacy, interoperability, and regulatory compliance while simultaneously\nensuring scalability and minimizing trust assumptions. The paper provides a\ndetailed technical design, outlining the implementation considerations, and\ndemonstrating how the proposed flows guarantee verifiable, secure, and\nefficient interactions between participants. This work contributes towards the\nestablishment of a more trustworthy and sovereign digital infrastructure, in\nalignment with emerging European data governance initiatives.",
      "authors": [
        "Rodrigo Men\u00e9ndez",
        "Andres Munoz-Arcentales",
        "Joaqu\u00edn Salvach\u00faa",
        "Carlos Aparicio",
        "Irene Plaza",
        "Gabriel Huecas"
      ],
      "categories": [
        "cs.CR",
        "cs.ET"
      ],
      "links": [
        "http://arxiv.org/abs/2505.24698v1",
        "http://arxiv.org/pdf/2505.24698v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2505.24672v1",
      "title": "TRIDENT: Enhancing Large Language Model Safety with Tri-Dimensional\n  Diversified Red-Teaming Data Synthesis",
      "published": "2025-05-30T15:02:21Z",
      "updated": "2025-05-30T15:02:21Z",
      "summary": "Large Language Models (LLMs) excel in various natural language processing\ntasks but remain vulnerable to generating harmful content or being exploited\nfor malicious purposes. Although safety alignment datasets have been introduced\nto mitigate such risks through supervised fine-tuning (SFT), these datasets\noften lack comprehensive risk coverage. Most existing datasets focus primarily\non lexical diversity while neglecting other critical dimensions. To address\nthis limitation, we propose a novel analysis framework to systematically\nmeasure the risk coverage of alignment datasets across three essential\ndimensions: Lexical Diversity, Malicious Intent, and Jailbreak Tactics. We\nfurther introduce TRIDENT, an automated pipeline that leverages persona-based,\nzero-shot LLM generation to produce diverse and comprehensive instructions\nspanning these dimensions. Each harmful instruction is paired with an ethically\naligned response, resulting in two datasets: TRIDENT-Core, comprising 26,311\nexamples, and TRIDENT-Edge, with 18,773 examples. Fine-tuning Llama 3.1-8B on\nTRIDENT-Edge demonstrates substantial improvements, achieving an average 14.29%\nreduction in Harm Score, and a 20% decrease in Attack Success Rate compared to\nthe best-performing baseline model fine-tuned on the WildBreak dataset.",
      "authors": [
        "Xiaorui Wu",
        "Xiaofeng Mao",
        "Fei Li",
        "Xin Zhang",
        "Xuanhong Li",
        "Chong Teng",
        "Donghong Ji",
        "Zhuang Li"
      ],
      "categories": [
        "cs.CL"
      ],
      "links": [
        "http://arxiv.org/abs/2505.24672v1",
        "http://arxiv.org/pdf/2505.24672v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2505.24667v1",
      "title": "Decoupled Competitive Framework for Semi-supervised Medical Image\n  Segmentation",
      "published": "2025-05-30T14:56:00Z",
      "updated": "2025-05-30T14:56:00Z",
      "summary": "Confronting the critical challenge of insufficiently annotated samples in\nmedical domain, semi-supervised medical image segmentation (SSMIS) emerges as a\npromising solution. Specifically, most methodologies following the Mean Teacher\n(MT) or Dual Students (DS) architecture have achieved commendable results.\nHowever, to date, these approaches face a performance bottleneck due to two\ninherent limitations, \\textit{e.g.}, the over-coupling problem within MT\nstructure owing to the employment of exponential moving average (EMA)\nmechanism, as well as the severe cognitive bias between two students of DS\nstructure, both of which potentially lead to reduced efficacy, or even model\ncollapse eventually. To mitigate these issues, a Decoupled Competitive\nFramework (DCF) is elaborated in this work, which utilizes a straightforward\ncompetition mechanism for the update of EMA, effectively decoupling students\nand teachers in a dynamical manner. In addition, the seamless exchange of\ninvaluable and precise insights is facilitated among students, guaranteeing a\nbetter learning paradigm. The DCF introduced undergoes rigorous validation on\nthree publicly accessible datasets, which encompass both 2D and 3D datasets.\nThe results demonstrate the superiority of our method over previous\ncutting-edge competitors. Code will be available at\nhttps://github.com/JiaheChen2002/DCF.",
      "authors": [
        "Jiahe Chen",
        "Jiahe Ying",
        "Shen Wang",
        "Jianwei Zheng"
      ],
      "categories": [
        "cs.CV"
      ],
      "links": [
        "http://arxiv.org/abs/2505.24667v1",
        "http://arxiv.org/pdf/2505.24667v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2505.24640v1",
      "title": "Efficient Text Encoders for Labor Market Analysis",
      "published": "2025-05-30T14:27:25Z",
      "updated": "2025-05-30T14:27:25Z",
      "summary": "Labor market analysis relies on extracting insights from job advertisements,\nwhich provide valuable yet unstructured information on job titles and\ncorresponding skill requirements. While state-of-the-art methods for skill\nextraction achieve strong performance, they depend on large language models\n(LLMs), which are computationally expensive and slow. In this paper, we propose\n\\textbf{ConTeXT-match}, a novel contrastive learning approach with token-level\nattention that is well-suited for the extreme multi-label classification task\nof skill classification. \\textbf{ConTeXT-match} significantly improves skill\nextraction efficiency and performance, achieving state-of-the-art results with\na lightweight bi-encoder model. To support robust evaluation, we introduce\n\\textbf{Skill-XL}, a new benchmark with exhaustive, sentence-level skill\nannotations that explicitly address the redundancy in the large label space.\nFinally, we present \\textbf{JobBERT V2}, an improved job title normalization\nmodel that leverages extracted skills to produce high-quality job title\nrepresentations. Experiments demonstrate that our models are efficient,\naccurate, and scalable, making them ideal for large-scale, real-time labor\nmarket analysis.",
      "authors": [
        "Jens-Joris Decorte",
        "Jeroen Van Hautte",
        "Chris Develder",
        "Thomas Demeester"
      ],
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "links": [
        "http://arxiv.org/abs/2505.24640v1",
        "http://arxiv.org/pdf/2505.24640v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2505.24630v1",
      "title": "The Hallucination Dilemma: Factuality-Aware Reinforcement Learning for\n  Large Reasoning Models",
      "published": "2025-05-30T14:23:32Z",
      "updated": "2025-05-30T14:23:32Z",
      "summary": "Large language models (LLMs) have significantly advanced in reasoning tasks\nthrough reinforcement learning (RL) optimization, achieving impressive\ncapabilities across various challenging benchmarks. However, our empirical\nanalysis reveals a critical drawback: reasoning-oriented RL fine-tuning\nsignificantly increases the prevalence of hallucinations. We theoretically\nanalyze the RL training dynamics, identifying high-variance gradient,\nentropy-induced randomness, and susceptibility to spurious local optima as key\nfactors leading to hallucinations. To address this drawback, we propose\nFactuality-aware Step-wise Policy Optimization (FSPO), an innovative RL\nfine-tuning algorithm incorporating explicit factuality verification at each\nreasoning step. FSPO leverages automated verification against given evidence to\ndynamically adjust token-level advantage values, incentivizing factual\ncorrectness throughout the reasoning process. Experiments across mathematical\nreasoning and hallucination benchmarks using Qwen2.5 and Llama models\ndemonstrate that FSPO effectively reduces hallucinations while enhancing\nreasoning accuracy, substantially improving both reliability and performance.",
      "authors": [
        "Junyi Li",
        "Hwee Tou Ng"
      ],
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "links": [
        "http://arxiv.org/abs/2505.24630v1",
        "http://arxiv.org/pdf/2505.24630v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    }
  ]
}