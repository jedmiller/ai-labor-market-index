{
  "query": "all:machine learning AND (labor market OR employment OR jobs OR workforce OR automation)",
  "date_collected": "2025-05-04T03:13:31.479168",
  "target_period": "2025-04",
  "papers": [
    {
      "id": "http://arxiv.org/abs/2505.00222v1",
      "title": "AI-Enhanced Automatic Design of Efficient Underwater Gliders",
      "published": "2025-04-30T23:55:44Z",
      "updated": "2025-04-30T23:55:44Z",
      "summary": "The development of novel autonomous underwater gliders has been hindered by\nlimited shape diversity, primarily due to the reliance on traditional design\ntools that depend heavily on manual trial and error. Building an automated\ndesign framework is challenging due to the complexities of representing glider\nshapes and the high computational costs associated with modeling complex\nsolid-fluid interactions. In this work, we introduce an AI-enhanced automated\ncomputational framework designed to overcome these limitations by enabling the\ncreation of underwater robots with non-trivial hull shapes. Our approach\ninvolves an algorithm that co-optimizes both shape and control signals,\nutilizing a reduced-order geometry representation and a differentiable\nneural-network-based fluid surrogate model. This end-to-end design workflow\nfacilitates rapid iteration and evaluation of hydrodynamic performance, leading\nto the discovery of optimal and complex hull shapes across various control\nsettings. We validate our method through wind tunnel experiments and swimming\npool gliding tests, demonstrating that our computationally designed gliders\nsurpass manually designed counterparts in terms of energy efficiency. By\naddressing challenges in efficient shape representation and neural fluid\nsurrogate models, our work paves the way for the development of highly\nefficient underwater gliders, with implications for long-range ocean\nexploration and environmental monitoring.",
      "authors": [
        "Peter Yichen Chen",
        "Pingchuan Ma",
        "Niklas Hagemann",
        "John Romanishin",
        "Wei Wang",
        "Daniela Rus",
        "Wojciech Matusik"
      ],
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.GR",
        "cs.LG",
        "physics.comp-ph"
      ],
      "links": [
        "http://arxiv.org/abs/2505.00222v1",
        "http://arxiv.org/pdf/2505.00222v1"
      ],
      "primary_search_term": "machine learning",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2504.21789v1",
      "title": "Anomaly-Driven Approach for Enhanced Prostate Cancer Segmentation",
      "published": "2025-04-30T16:48:00Z",
      "updated": "2025-04-30T16:48:00Z",
      "summary": "Magnetic Resonance Imaging (MRI) plays an important role in identifying\nclinically significant prostate cancer (csPCa), yet automated methods face\nchallenges such as data imbalance, variable tumor sizes, and a lack of\nannotated data. This study introduces Anomaly-Driven U-Net (adU-Net), which\nincorporates anomaly maps derived from biparametric MRI sequences into a deep\nlearning-based segmentation framework to improve csPCa identification. We\nconduct a comparative analysis of anomaly detection methods and evaluate the\nintegration of anomaly maps into the segmentation pipeline. Anomaly maps,\ngenerated using Fixed-Point GAN reconstruction, highlight deviations from\nnormal prostate tissue, guiding the segmentation model to potential cancerous\nregions. We compare the performance by using the average score, computed as the\nmean of the AUROC and Average Precision (AP). On the external test set, adU-Net\nachieves the best average score of 0.618, outperforming the baseline nnU-Net\nmodel (0.605). The results demonstrate that incorporating anomaly detection\ninto segmentation improves generalization and performance, particularly with\nADC-based anomaly maps, offering a promising direction for automated csPCa\nidentification.",
      "authors": [
        "Alessia Hu",
        "Regina Beets-Tan",
        "Lishan Cai",
        "Eduardo Pooch"
      ],
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "links": [
        "http://arxiv.org/abs/2504.21789v1",
        "http://arxiv.org/pdf/2504.21789v1"
      ],
      "primary_search_term": "machine learning",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2504.21464v1",
      "title": "VR-FuseNet: A Fusion of Heterogeneous Fundus Data and Explainable Deep\n  Network for Diabetic Retinopathy Classification",
      "published": "2025-04-30T09:38:47Z",
      "updated": "2025-04-30T09:38:47Z",
      "summary": "Diabetic retinopathy is a severe eye condition caused by diabetes where the\nretinal blood vessels get damaged and can lead to vision loss and blindness if\nnot treated. Early and accurate detection is key to intervention and stopping\nthe disease progressing. For addressing this disease properly, this paper\npresents a comprehensive approach for automated diabetic retinopathy detection\nby proposing a new hybrid deep learning model called VR-FuseNet. Diabetic\nretinopathy is a major eye disease and leading cause of blindness especially\namong diabetic patients so accurate and efficient automated detection methods\nare required. To address the limitations of existing methods including dataset\nimbalance, diversity and generalization issues this paper presents a hybrid\ndataset created from five publicly available diabetic retinopathy datasets.\nEssential preprocessing techniques such as SMOTE for class balancing and CLAHE\nfor image enhancement are applied systematically to the dataset to improve the\nrobustness and generalizability of the dataset. The proposed VR-FuseNet model\ncombines the strengths of two state-of-the-art convolutional neural networks,\nVGG19 which captures fine-grained spatial features and ResNet50V2 which is\nknown for its deep hierarchical feature extraction. This fusion improves the\ndiagnostic performance and achieves an accuracy of 91.824%. The model\noutperforms individual architectures on all performance metrics demonstrating\nthe effectiveness of hybrid feature extraction in Diabetic Retinopathy\nclassification tasks. To make the proposed model more clinically useful and\ninterpretable this paper incorporates multiple XAI techniques. These techniques\ngenerate visual explanations that clearly indicate the retinal features\naffecting the model's prediction such as microaneurysms, hemorrhages and\nexudates so that clinicians can interpret and validate.",
      "authors": [
        "Shamim Rahim Refat",
        "Ziyan Shirin Raha",
        "Shuvashis Sarker",
        "Faika Fairuj Preotee",
        "MD. Musfikur Rahman",
        "Tashreef Muhammad",
        "Mohammad Shafiul Islam"
      ],
      "categories": [
        "cs.CV"
      ],
      "links": [
        "http://arxiv.org/abs/2504.21464v1",
        "http://arxiv.org/pdf/2504.21464v1"
      ],
      "primary_search_term": "machine learning",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2504.21254v1",
      "title": "ABG-NAS: Adaptive Bayesian Genetic Neural Architecture Search for Graph\n  Representation Learning",
      "published": "2025-04-30T01:44:27Z",
      "updated": "2025-04-30T01:44:27Z",
      "summary": "Effective and efficient graph representation learning is essential for\nenabling critical downstream tasks, such as node classification, link\nprediction, and subgraph search. However, existing graph neural network (GNN)\narchitectures often struggle to adapt to diverse and complex graph structures,\nlimiting their ability to provide robust and generalizable representations. To\naddress this challenge, we propose ABG-NAS, a novel framework for automated\ngraph neural network architecture search tailored for efficient graph\nrepresentation learning. ABG-NAS encompasses three key components: a\nComprehensive Architecture Search Space (CASS), an Adaptive Genetic\nOptimization Strategy (AGOS), and a Bayesian-Guided Tuning Module (BGTM). CASS\nsystematically explores diverse propagation (P) and transformation (T)\noperations, enabling the discovery of GNN architectures capable of capturing\nintricate graph characteristics. AGOS dynamically balances exploration and\nexploitation, ensuring search efficiency and preserving solution diversity.\nBGTM further optimizes hyperparameters periodically, enhancing the scalability\nand robustness of the resulting architectures. Empirical evaluations on\nbenchmark datasets (Cora, PubMed, Citeseer, and CoraFull) demonstrate that\nABG-NAS consistently outperforms both manually designed GNNs and\nstate-of-the-art neural architecture search (NAS) methods. These results\nhighlight the potential of ABG-NAS to advance graph representation learning by\nproviding scalable and adaptive solutions for diverse graph structures. Our\ncode is publicly available at https://github.com/sserranw/ABG-NAS.",
      "authors": [
        "Sixuan Wang",
        "Jiao Yin",
        "Jinli Cao",
        "MingJian Tang",
        "Hua Wang",
        "Yanchun Zhang"
      ],
      "categories": [
        "cs.LG",
        "cs.NE"
      ],
      "links": [
        "http://arxiv.org/abs/2504.21254v1",
        "http://arxiv.org/pdf/2504.21254v1"
      ],
      "primary_search_term": "machine learning",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2504.21245v1",
      "title": "Database and deep-learning scalability of anharmonic phonon properties\n  by automated brute-force first-principles calculations",
      "published": "2025-04-30T00:54:32Z",
      "updated": "2025-04-30T00:54:32Z",
      "summary": "Understanding the anharmonic phonon properties of crystal compounds -- such\nas phonon lifetimes and thermal conductivities -- is essential for\ninvestigating and optimizing their thermal transport behaviors. These\nproperties also impact optical, electronic, and magnetic characteristics\nthrough interactions between phonons and other quasiparticles and fields. In\nthis study, we develop an automated first-principles workflow to calculate\nanharmonic phonon properties and build a comprehensive database encompassing\nmore than 6,000 inorganic compounds. Utilizing this dataset, we train a graph\nneural network model to predict thermal conductivity values and spectra from\nstructural parameters, demonstrating a scaling law in which prediction accuracy\nimproves with increasing training data size. High-throughput screening with the\nmodel enables the identification of materials exhibiting extreme thermal\nconductivities -- both high and low. The resulting database offers valuable\ninsights into the anharmonic behavior of phonons, thereby accelerating the\ndesign and development of advanced functional materials.",
      "authors": [
        "Masato Ohnishi",
        "Tianqi Deng",
        "Pol Torres",
        "Zhihao Xu",
        "Terumasa Tadano",
        "Haoming Zhang",
        "Wei Nong",
        "Masatoshi Hanai",
        "Zhiting Tian",
        "Ming Hu",
        "Xiulin Ruan",
        "Ryo Yoshida",
        "Toyotaro Suzumura",
        "Lucas Lindsay",
        "Alan J. H. McGaughey",
        "Tengfei Luo",
        "Kedar Hippalgaonkar",
        "Junichiro Shiomi"
      ],
      "categories": [
        "cond-mat.mtrl-sci"
      ],
      "links": [
        "http://arxiv.org/abs/2504.21245v1",
        "http://arxiv.org/pdf/2504.21245v1"
      ],
      "primary_search_term": "machine learning",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2504.21211v1",
      "title": "A Cost-Effective LLM-based Approach to Identify Wildlife Trafficking in\n  Online Marketplaces",
      "published": "2025-04-29T22:34:42Z",
      "updated": "2025-04-29T22:34:42Z",
      "summary": "Wildlife trafficking remains a critical global issue, significantly impacting\nbiodiversity, ecological stability, and public health. Despite efforts to\ncombat this illicit trade, the rise of e-commerce platforms has made it easier\nto sell wildlife products, putting new pressure on wild populations of\nendangered and threatened species. The use of these platforms also opens a new\nopportunity: as criminals sell wildlife products online, they leave digital\ntraces of their activity that can provide insights into trafficking activities\nas well as how they can be disrupted. The challenge lies in finding these\ntraces. Online marketplaces publish ads for a plethora of products, and\nidentifying ads for wildlife-related products is like finding a needle in a\nhaystack. Learning classifiers can automate ad identification, but creating\nthem requires costly, time-consuming data labeling that hinders support for\ndiverse ads and research questions. This paper addresses a critical challenge\nin the data science pipeline for wildlife trafficking analytics: generating\nquality labeled data for classifiers that select relevant data. While large\nlanguage models (LLMs) can directly label advertisements, doing so at scale is\nprohibitively expensive. We propose a cost-effective strategy that leverages\nLLMs to generate pseudo labels for a small sample of the data and uses these\nlabels to create specialized classification models. Our novel method\nautomatically gathers diverse and representative samples to be labeled while\nminimizing the labeling costs. Our experimental evaluation shows that our\nclassifiers achieve up to 95% F1 score, outperforming LLMs at a lower cost. We\npresent real use cases that demonstrate the effectiveness of our approach in\nenabling analyses of different aspects of wildlife trafficking.",
      "authors": [
        "Juliana Barbosa",
        "Ulhas Gondhali",
        "Gohar Petrossian",
        "Kinshuk Sharma",
        "Sunandan Chakraborty",
        "Jennifer Jacquet",
        "Juliana Freire"
      ],
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "links": [
        "http://dx.doi.org/10.1145/3725256",
        "http://arxiv.org/abs/2504.21211v1",
        "http://arxiv.org/pdf/2504.21211v1"
      ],
      "primary_search_term": "machine learning",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2504.21194v1",
      "title": "Geolocating Earth Imagery from ISS: Integrating Machine Learning with\n  Astronaut Photography for Enhanced Geographic Mapping",
      "published": "2025-04-29T22:00:02Z",
      "updated": "2025-04-29T22:00:02Z",
      "summary": "This paper presents a novel approach to geolocating images captured from the\nInternational Space Station (ISS) using advanced machine learning algorithms.\nDespite having precise ISS coordinates, the specific Earth locations depicted\nin astronaut-taken photographs often remain unidentified. Our research\naddresses this gap by employing three distinct image processing pipelines: a\nNeural Network based approach, a SIFT based method, and GPT-4 model. Each\npipeline is tailored to process high-resolution ISS imagery, identifying both\nnatural and man-made geographical features. Through extensive evaluation on a\ndiverse dataset of over 140 ISS images, our methods demonstrate significant\npromise in automated geolocation with varied levels of success. The NN approach\nshowed a high success rate in accurately matching geographical features, while\nthe SIFT pipeline excelled in processing zoomed-in images. GPT-4 model provided\nenriched geographical descriptions alongside location predictions. This\nresearch contributes to the fields of remote sensing and Earth observation by\nenhancing the accuracy and efficiency of geolocating space-based imagery,\nthereby aiding environmental monitoring and global mapping efforts.",
      "authors": [
        "Vedika Srivastava",
        "Hemant Kumar Singh",
        "Jaisal Singh"
      ],
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "links": [
        "http://arxiv.org/abs/2504.21194v1",
        "http://arxiv.org/pdf/2504.21194v1"
      ],
      "primary_search_term": "machine learning",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2504.21190v1",
      "title": "TT-LoRA MoE: Unifying Parameter-Efficient Fine-Tuning and Sparse\n  Mixture-of-Experts",
      "published": "2025-04-29T21:46:43Z",
      "updated": "2025-04-29T21:46:43Z",
      "summary": "We propose Tensor-Trained Low-Rank Adaptation Mixture of Experts (TT-LoRA\nMoE), a novel computational framework integrating Parameter-Efficient\nFine-Tuning (PEFT) with sparse MoE routing to address scalability challenges in\nlarge model deployments. Unlike traditional MoE approaches, which face\nsubstantial computational overhead as expert counts grow, TT-LoRA MoE\ndecomposes training into two distinct, optimized stages. First, we\nindependently train lightweight, tensorized low-rank adapters (TT-LoRA\nexperts), each specialized for specific tasks. Subsequently, these expert\nadapters remain frozen, eliminating inter-task interference and catastrophic\nforgetting in multi-task setting. A sparse MoE router, trained separately,\ndynamically leverages base model representations to select exactly one\nspecialized adapter per input at inference time, automating expert selection\nwithout explicit task specification. Comprehensive experiments confirm our\narchitecture retains the memory efficiency of low-rank adapters, seamlessly\nscales to large expert pools, and achieves robust task-level optimization. This\nstructured decoupling significantly enhances computational efficiency and\nflexibility: uses only 2% of LoRA, 0.3% of Adapters and 0.03% of AdapterFusion\nparameters and outperforms AdapterFusion by 4 value in multi-tasking, enabling\npractical and scalable multi-task inference deployments.",
      "authors": [
        "Pradip Kunwar",
        "Minh N. Vu",
        "Maanak Gupta",
        "Mahmoud Abdelsalam",
        "Manish Bhattarai"
      ],
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "links": [
        "http://arxiv.org/abs/2504.21190v1",
        "http://arxiv.org/pdf/2504.21190v1"
      ],
      "primary_search_term": "machine learning",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2504.21174v1",
      "title": "Efficient LLMs with AMP: Attention Heads and MLP Pruning",
      "published": "2025-04-29T20:50:08Z",
      "updated": "2025-04-29T20:50:08Z",
      "summary": "Deep learning drives a new wave in computing systems and triggers the\nautomation of increasingly complex problems. In particular, Large Language\nModels (LLMs) have significantly advanced cognitive tasks, often matching or\neven surpassing human-level performance. However, their extensive parameters\nresult in high computational costs and slow inference, posing challenges for\ndeployment in resource-limited settings. Among the strategies to overcome the\naforementioned challenges, pruning emerges as a successful mechanism since it\nreduces model size while maintaining predictive ability. In this paper, we\nintroduce AMP: Attention Heads and MLP Pruning, a novel structured pruning\nmethod that efficiently compresses LLMs by removing less critical structures\nwithin Multi-Head Attention (MHA) and Multilayer Perceptron (MLP). By\nprojecting the input data onto weights, AMP assesses structural importance and\novercomes the limitations of existing techniques, which often fall short in\nflexibility or efficiency. In particular, AMP surpasses the current\nstate-of-the-art on commonsense reasoning tasks by up to 1.49 percentage\npoints, achieving a 30% pruning ratio with minimal impact on zero-shot task\nperformance. Moreover, AMP also improves inference speeds, making it\nwell-suited for deployment in resource-constrained environments. We confirm the\nflexibility of AMP on different families of LLMs, including LLaMA and Phi.",
      "authors": [
        "Leandro Giusti Mugnaini",
        "Bruno Lopes Yamamoto",
        "Lucas Lauton de Alcantara",
        "Victor Zacarias",
        "Edson Bollis",
        "Lucas Pellicer",
        "Anna Helena Reali Costa",
        "Artur Jordao"
      ],
      "categories": [
        "cs.LG"
      ],
      "links": [
        "http://arxiv.org/abs/2504.21174v1",
        "http://arxiv.org/pdf/2504.21174v1"
      ],
      "primary_search_term": "machine learning",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2504.21166v1",
      "title": "Dance Style Recognition Using Laban Movement Analysis",
      "published": "2025-04-29T20:35:01Z",
      "updated": "2025-04-29T20:35:01Z",
      "summary": "The growing interest in automated movement analysis has presented new\nchallenges in recognition of complex human activities including dance. This\nstudy focuses on dance style recognition using features extracted using Laban\nMovement Analysis. Previous studies for dance style recognition often focus on\ncross-frame movement analysis, which limits the ability to capture temporal\ncontext and dynamic transitions between movements. This gap highlights the need\nfor a method that can add temporal context to LMA features. For this, we\nintroduce a novel pipeline which combines 3D pose estimation, 3D human mesh\nreconstruction, and floor aware body modeling to effectively extract LMA\nfeatures. To address the temporal limitation, we propose a sliding window\napproach that captures movement evolution across time in features. These\nfeatures are then used to train various machine learning methods for\nclassification, and their explainability explainable AI methods to evaluate the\ncontribution of each feature to classification performance. Our proposed method\nachieves a highest classification accuracy of 99.18\\% which shows that the\naddition of temporal context significantly improves dance style recognition\nperformance.",
      "authors": [
        "Muhammad Turab",
        "Philippe Colantoni",
        "Damien Muselet",
        "Alain Tremeau"
      ],
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "links": [
        "http://arxiv.org/abs/2504.21166v1",
        "http://arxiv.org/pdf/2504.21166v1"
      ],
      "primary_search_term": "machine learning",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2504.20993v1",
      "title": "GDP-GFCF Dynamics Across Global Economies: A Comparative Study of Panel\n  Regressions and Random Forest",
      "published": "2025-04-29T17:58:47Z",
      "updated": "2025-04-29T17:58:47Z",
      "summary": "This study examines the relationship between GDP growth and Gross Fixed\nCapital Formation (GFCF) across developed economies (G7, EU-15, OECD) and\nemerging markets (BRICS). We integrate Random Forest machine learning\n(non-linear regression) with traditional econometric models (linear regression)\nto better capture non-linear interactions in investment analysis. Our findings\nreveal that while GDP growth positively influences corporate investment, its\nimpact varies significantly by region. Developed economies show stronger\nGDP-GFCF linkages due to stable financial systems, while emerging markets\ndemonstrate weaker connections due to economic heterogeneity and structural\nconstraints. Random Forest models indicate that GDP growth's importance is\nlower than suggested by traditional econometrics, with lagged GFCF emerging as\nthe dominant predictor-confirming investment follows path-dependent patterns\nrather than short-term GDP fluctuations. Regional variations in investment\ndrivers are substantial: taxation significantly influences developed economies\nbut minimally affects BRICS, while unemployment strongly drives investment in\nBRICS but less so elsewhere. We introduce a parallelized p-value importance\nalgorithm for Random Forest that enhances computational efficiency while\nmaintaining statistical rigor through sequential testing methods (SPRT and\nSAPT). The research demonstrates that hybrid methodologies combining machine\nlearning with econometric techniques provide more nuanced understanding of\ninvestment dynamics, supporting region-specific policy design and improving\nforecasting accuracy.",
      "authors": [
        "Alina Landowska",
        "Robert A. K\u0142opotek",
        "Dariusz Filip",
        "Konrad Raczkowski"
      ],
      "categories": [
        "econ.GN",
        "q-fin.EC"
      ],
      "links": [
        "http://arxiv.org/abs/2504.20993v1",
        "http://arxiv.org/pdf/2504.20993v1"
      ],
      "primary_search_term": "machine learning",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2504.20970v1",
      "title": "SVD Based Least Squares for X-Ray Pneumonia Classification Using Deep\n  Features",
      "published": "2025-04-29T17:39:16Z",
      "updated": "2025-04-29T17:39:16Z",
      "summary": "Accurate and early diagnosis of pneumonia through X-ray imaging is essential\nfor effective treatment and improved patient outcomes. Recent advancements in\nmachine learning have enabled automated diagnostic tools that assist\nradiologists in making more reliable and efficient decisions. In this work, we\npropose a Singular Value Decomposition-based Least Squares (SVD-LS) framework\nfor multi-class pneumonia classification, leveraging powerful feature\nrepresentations from state-of-the-art self-supervised and transfer learning\nmodels. Rather than relying on computationally expensive gradient based\nfine-tuning, we employ a closed-form, non-iterative classification approach\nthat ensures efficiency without compromising accuracy. Experimental results\ndemonstrate that SVD-LS achieves competitive performance while offering\nsignificantly reduced computational costs, making it a viable alternative for\nreal-time medical imaging applications.",
      "authors": [
        "Mete Erdogan",
        "Sebnem Demirtas"
      ],
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "links": [
        "http://arxiv.org/abs/2504.20970v1",
        "http://arxiv.org/pdf/2504.20970v1"
      ],
      "primary_search_term": "machine learning",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2504.20965v1",
      "title": "AegisLLM: Scaling Agentic Systems for Self-Reflective Defense in LLM\n  Security",
      "published": "2025-04-29T17:36:05Z",
      "updated": "2025-04-29T17:36:05Z",
      "summary": "We introduce AegisLLM, a cooperative multi-agent defense against adversarial\nattacks and information leakage. In AegisLLM, a structured workflow of\nautonomous agents - orchestrator, deflector, responder, and evaluator -\ncollaborate to ensure safe and compliant LLM outputs, while self-improving over\ntime through prompt optimization. We show that scaling agentic reasoning system\nat test-time - both by incorporating additional agent roles and by leveraging\nautomated prompt optimization (such as DSPy)- substantially enhances robustness\nwithout compromising model utility. This test-time defense enables real-time\nadaptability to evolving attacks, without requiring model retraining.\nComprehensive evaluations across key threat scenarios, including unlearning and\njailbreaking, demonstrate the effectiveness of AegisLLM. On the WMDP unlearning\nbenchmark, AegisLLM achieves near-perfect unlearning with only 20 training\nexamples and fewer than 300 LM calls. For jailbreaking benchmarks, we achieve\n51% improvement compared to the base model on StrongReject, with false refusal\nrates of only 7.9% on PHTest compared to 18-55% for comparable methods. Our\nresults highlight the advantages of adaptive, agentic reasoning over static\ndefenses, establishing AegisLLM as a strong runtime alternative to traditional\napproaches based on model modifications. Code is available at\nhttps://github.com/zikuicai/aegisllm",
      "authors": [
        "Zikui Cai",
        "Shayan Shabihi",
        "Bang An",
        "Zora Che",
        "Brian R. Bartoldson",
        "Bhavya Kailkhura",
        "Tom Goldstein",
        "Furong Huang"
      ],
      "categories": [
        "cs.LG"
      ],
      "links": [
        "http://arxiv.org/abs/2504.20965v1",
        "http://arxiv.org/pdf/2504.20965v1"
      ],
      "primary_search_term": "machine learning",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2504.20938v1",
      "title": "Towards Understanding the Nature of Attention with Low-Rank Sparse\n  Decomposition",
      "published": "2025-04-29T17:03:03Z",
      "updated": "2025-04-29T17:03:03Z",
      "summary": "We propose Low-Rank Sparse Attention (Lorsa), a sparse replacement model of\nTransformer attention layers to disentangle original Multi Head Self Attention\n(MHSA) into individually comprehensible components. Lorsa is designed to\naddress the challenge of attention superposition to understand\nattention-mediated interaction between features in different token positions.\nWe show that Lorsa heads find cleaner and finer-grained versions of previously\ndiscovered MHSA behaviors like induction heads, successor heads and attention\nsink behavior (i.e., heavily attending to the first token). Lorsa and Sparse\nAutoencoder (SAE) are both sparse dictionary learning methods applied to\ndifferent Transformer components, and lead to consistent findings in many ways.\nFor instance, we discover a comprehensive family of arithmetic-specific Lorsa\nheads, each corresponding to an atomic operation in Llama-3.1-8B. Automated\ninterpretability analysis indicates that Lorsa achieves parity with SAE in\ninterpretability while Lorsa exhibits superior circuit discovery properties,\nespecially for features computed collectively by multiple MHSA heads. We also\nconduct extensive experiments on architectural design ablation, Lorsa scaling\nlaw and error analysis.",
      "authors": [
        "Zhengfu He",
        "Junxuan Wang",
        "Rui Lin",
        "Xuyang Ge",
        "Wentao Shu",
        "Qiong Tang",
        "Junping Zhang",
        "Xipeng Qiu"
      ],
      "categories": [
        "cs.LG",
        "cs.CL"
      ],
      "links": [
        "http://arxiv.org/abs/2504.20938v1",
        "http://arxiv.org/pdf/2504.20938v1"
      ],
      "primary_search_term": "machine learning",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2504.20906v1",
      "title": "GiBy: A Giant-Step Baby-Step Classifier For Anomaly Detection In\n  Industrial Control Systems",
      "published": "2025-04-29T16:24:11Z",
      "updated": "2025-04-29T16:24:11Z",
      "summary": "The continuous monitoring of the interactions between cyber-physical\ncomponents of any industrial control system (ICS) is required to secure\nautomation of the system controls, and to guarantee plant processes are\nfail-safe and remain in an acceptably safe state. Safety is achieved by\nmanaging actuation (where electric signals are used to trigger physical\nmovement), dependent on corresponding sensor readings; used as ground truth in\ndecision making. Timely detection of anomalies (attacks, faults and\nunascertained states) in ICSs is crucial for the safe running of a plant, the\nsafety of its personnel, and for the safe provision of any services provided.\nWe propose an anomaly detection method that involves accurate linearization of\nthe non-linear forms arising from sensor-actuator(s) relationships, primarily\nbecause solving linear models is easier and well understood. Further, the time\ncomplexity of the anomaly detection scenario/problem at hand is lowered using\ndimensionality reduction of the actuator(s) in relationship with a sensor. We\naccomplish this by using a well-known water treatment testbed as a use case.\nOur experiments show millisecond time response to detect anomalies and provide\nexplainability; that are not simultaneously achieved by other state of the art\nAI/ML models with eXplainable AI (XAI) used for the same purpose. Further, we\npin-point the sensor(s) and its actuation state for which anomaly was detected.",
      "authors": [
        "Sarad Venugopalan",
        "Sridhar Adepu"
      ],
      "categories": [
        "cs.CR",
        "cs.LG"
      ],
      "links": [
        "http://arxiv.org/abs/2504.20906v1",
        "http://arxiv.org/pdf/2504.20906v1"
      ],
      "primary_search_term": "machine learning",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2504.20904v1",
      "title": "Dual Explanations via Subgraph Matching for Malware Detection",
      "published": "2025-04-29T16:20:28Z",
      "updated": "2025-04-29T16:20:28Z",
      "summary": "Interpretable malware detection is crucial for understanding harmful\nbehaviors and building trust in automated security systems. Traditional\nexplainable methods for Graph Neural Networks (GNNs) often highlight important\nregions within a graph but fail to associate them with known benign or\nmalicious behavioral patterns. This limitation reduces their utility in\nsecurity contexts, where alignment with verified prototypes is essential. In\nthis work, we introduce a novel dual prototype-driven explainable framework\nthat interprets GNN-based malware detection decisions. This dual explainable\nframework integrates a base explainer (a state-of-the-art explainer) with a\nnovel second-level explainer which is designed by subgraph matching technique,\ncalled SubMatch explainer. The proposed explainer assigns interpretable scores\nto nodes based on their association with matched subgraphs, offering a\nfine-grained distinction between benign and malicious regions. This\nprototype-guided scoring mechanism enables more interpretable, behavior-aligned\nexplanations. Experimental results demonstrate that our method preserves high\ndetection performance while significantly improving interpretability in malware\nanalysis.",
      "authors": [
        "Hossein Shokouhinejad",
        "Roozbeh Razavi-Far",
        "Griffin Higgins",
        "Ali A. Ghorbani"
      ],
      "categories": [
        "cs.CR",
        "cs.LG"
      ],
      "links": [
        "http://arxiv.org/abs/2504.20904v1",
        "http://arxiv.org/pdf/2504.20904v1"
      ],
      "primary_search_term": "machine learning",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2504.21569v1",
      "title": "A Systematic Literature Review of Parameter-Efficient Fine-Tuning for\n  Large Code Models",
      "published": "2025-04-29T16:19:25Z",
      "updated": "2025-04-29T16:19:25Z",
      "summary": "The rise of Artificial Intelligence (AI)-and particularly Large Language\nModels (LLMs) for code-has reshaped Software Engineering (SE) by enabling the\nautomation of tasks such as code generation, bug detection, and repair.\nHowever, these models require significant computational resources for training\nand fine-tuning, posing challenges for real-world adoption in\nresource-constrained environments. To address this, the research community has\nincreasingly turned to Parameter-Efficient Fine-Tuning (PEFT)-a class of\ntechniques that enables the adaptation of large models by updating only a small\nsubset of parameters, rather than the entire model. In this Systematic\nLiterature Review (SLR), we examine the growing application of PEFT\ntechniques-across a wide range of software engineering tasks. We analyze how\nthese methods are used to optimize various deep learning (DL) architectures,\nfocusing on their impact on both performance and efficiency. Our study\nsynthesizes findings from 27 peer-reviewed papers, identifying patterns in\nconfiguration strategies and adaptation trade-offs. The outcome of this review\nis a comprehensive taxonomy that categorizes PEFT usage by task type,\ndistinguishing between generative (e.g., Code Summarization) and non-generative\n(e.g., Code Clone Detection) scenarios. Our findings aim to inform future\nresearch and guide the practical deployment of PEFT in sustainable, AI-powered\nsoftware development. Our artifacts are publicly available at\nhttps://github.com/alvi75/SLR-PEFT",
      "authors": [
        "Md Zahidul Haque",
        "Saima Afrin",
        "Antonio Mastropaolo"
      ],
      "categories": [
        "cs.SE"
      ],
      "links": [
        "http://arxiv.org/abs/2504.21569v1",
        "http://arxiv.org/pdf/2504.21569v1"
      ],
      "primary_search_term": "machine learning",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2504.21071v1",
      "title": "Automated Parking Trajectory Generation Using Deep Reinforcement\n  Learning",
      "published": "2025-04-29T15:25:34Z",
      "updated": "2025-04-29T15:25:34Z",
      "summary": "Autonomous parking is a key technology in modern autonomous driving systems,\nrequiring high precision, strong adaptability, and efficiency in complex\nenvironments. This paper proposes a Deep Reinforcement Learning (DRL) framework\nbased on the Soft Actor-Critic (SAC) algorithm to optimize autonomous parking\ntasks. SAC, an off-policy method with entropy regularization, is particularly\nwell-suited for continuous action spaces, enabling fine-grained vehicle\ncontrol. We model the parking task as a Markov Decision Process (MDP) and train\nan agent to maximize cumulative rewards while balancing exploration and\nexploitation through entropy maximization. The proposed system integrates\nmultiple sensor inputs into a high-dimensional state space and leverages SAC's\ndual critic networks and policy network to achieve stable learning. Simulation\nresults show that the SAC-based approach delivers high parking success rates,\nreduced maneuver times, and robust handling of dynamic obstacles, outperforming\ntraditional rule-based methods and other DRL algorithms. This study\ndemonstrates SAC's potential in autonomous parking and lays the foundation for\nreal-world applications.",
      "authors": [
        "Zheyu Zhang",
        "Yutong Luo",
        "Yongzhou Chen",
        "Haopeng Zhao",
        "Zhichao Ma",
        "Hao Liu"
      ],
      "categories": [
        "cs.RO"
      ],
      "links": [
        "http://arxiv.org/abs/2504.21071v1",
        "http://arxiv.org/pdf/2504.21071v1"
      ],
      "primary_search_term": "machine learning",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2504.20776v1",
      "title": "ECOSoundSet: a finely annotated dataset for the automated acoustic\n  identification of Orthoptera and Cicadidae in North, Central and temperate\n  Western Europe",
      "published": "2025-04-29T13:53:33Z",
      "updated": "2025-04-29T13:53:33Z",
      "summary": "Currently available tools for the automated acoustic recognition of European\ninsects in natural soundscapes are limited in scope. Large and ecologically\nheterogeneous acoustic datasets are currently needed for these algorithms to\ncross-contextually recognize the subtle and complex acoustic signatures\nproduced by each species, thus making the availability of such datasets a key\nrequisite for their development. Here we present ECOSoundSet (European\nCicadidae and Orthoptera Sound dataSet), a dataset containing 10,653 recordings\nof 200 orthopteran and 24 cicada species (217 and 26 respective taxa when\nincluding subspecies) present in North, Central, and temperate Western Europe\n(Andorra, Belgium, Denmark, mainland France and Corsica, Germany, Ireland,\nLuxembourg, Monaco, Netherlands, United Kingdom, Switzerland), collected partly\nthrough targeted fieldwork in South France and Catalonia and partly through\ncontributions from various European entomologists. The dataset is composed of a\ncombination of coarsely labeled recordings, for which we can only infer the\npresence, at some point, of their target species (weak labeling), and finely\nannotated recordings, for which we know the specific time and frequency range\nof each insect sound present in the recording (strong labeling). We also\nprovide a train/validation/test split of the strongly labeled recordings, with\nrespective approximate proportions of 0.8, 0.1 and 0.1, in order to facilitate\ntheir incorporation in the training and evaluation of deep learning algorithms.\nThis dataset could serve as a meaningful complement to recordings already\navailable online for the training of deep learning algorithms for the acoustic\nclassification of orthopterans and cicadas in North, Central, and temperate\nWestern Europe.",
      "authors": [
        "David Funosas",
        "Elodie Massol",
        "Yves Bas",
        "Svenja Schmidt",
        "Dominik Arend",
        "Alexander Gebhard",
        "Luc Barbaro",
        "Sebastian K\u00f6nig",
        "Rafael Carbonell Font",
        "David Sannier",
        "Fernand Deroussen",
        "J\u00e9r\u00f4me Sueur",
        "Christian Roesti",
        "Tomi Trilar",
        "Wolfgang Forstmeier",
        "Lucas Roger",
        "Elo\u00efsa Matheu",
        "Piotr Guzik",
        "Julien Barataud",
        "Laurent Pelozuelo",
        "St\u00e9phane Puissant",
        "Sandra Mueller",
        "Bj\u00f6rn Schuller",
        "Jose M. Montoya",
        "Andreas Triantafyllopoulos",
        "Maxime Cauchoix"
      ],
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "links": [
        "http://arxiv.org/abs/2504.20776v1",
        "http://arxiv.org/pdf/2504.20776v1"
      ],
      "primary_search_term": "machine learning",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2504.20726v1",
      "title": "Enhancing Vulnerability Reports with Automated and Augmented Description\n  Summarization",
      "published": "2025-04-29T13:08:27Z",
      "updated": "2025-04-29T13:08:27Z",
      "summary": "Public vulnerability databases, such as the National Vulnerability Database\n(NVD), document vulnerabilities and facilitate threat information sharing.\nHowever, they often suffer from short descriptions and outdated or insufficient\ninformation. In this paper, we introduce Zad, a system designed to enrich NVD\nvulnerability descriptions by leveraging external resources. Zad consists of\ntwo pipelines: one collects and filters supplementary data using two encoders\nto build a detailed dataset, while the other fine-tunes a pre-trained model on\nthis dataset to generate enriched descriptions. By addressing brevity and\nimproving content quality, Zad produces more comprehensive and cohesive\nvulnerability descriptions. We evaluate Zad using standard summarization\nmetrics and human assessments, demonstrating its effectiveness in enhancing\nvulnerability information.",
      "authors": [
        "Hattan Althebeiti",
        "Mohammed Alkinoon",
        "Manar Mohaisen",
        "Saeed Salem",
        "DaeHun Nyang",
        "David Mohaisen"
      ],
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG"
      ],
      "links": [
        "http://arxiv.org/abs/2504.20726v1",
        "http://arxiv.org/pdf/2504.20726v1"
      ],
      "primary_search_term": "machine learning",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    }
  ]
}