{
  "query": "all:large language models AND (labor market OR employment OR jobs OR workforce OR automation)",
  "target_period": "2025-03",
  "date_collected": "2025-03-20T15:30:25.927906",
  "papers": [
    {
      "id": "http://arxiv.org/abs/2503.15474v1",
      "title": "Toward task-driven satellite image super-resolution",
      "published": "2025-03-19T17:49:27Z",
      "updated": "2025-03-19T17:49:27Z",
      "summary": "Super-resolution is aimed at reconstructing high-resolution images from\nlow-resolution observations. State-of-the-art approaches underpinned with deep\nlearning allow for obtaining outstanding results, generating images of high\nperceptual quality. However, it often remains unclear whether the reconstructed\ndetails are close to the actual ground-truth information and whether they\nconstitute a more valuable source for image analysis algorithms. In the\nreported work, we address the latter problem, and we present our efforts toward\nlearning super-resolution algorithms in a task-driven way to make them suitable\nfor generating high-resolution images that can be exploited for automated image\nanalysis. In the reported initial research, we propose a methodological\napproach for assessing the existing models that perform computer vision tasks\nin terms of whether they can be used for evaluating super-resolution\nreconstruction algorithms, as well as training them in a task-driven way. We\nsupport our analysis with experimental study and we expect it to establish a\nsolid foundation for selecting appropriate computer vision tasks that will\nadvance the capabilities of real-world super-resolution.",
      "authors": [
        "Maciej Ziaja",
        "Pawel Kowaleczko",
        "Daniel Kostrzewa",
        "Nicolas Long\u00e9p\u00e9",
        "Michal Kawulok"
      ],
      "categories": [
        "cs.CV"
      ],
      "links": [
        "http://dx.doi.org/10.1109/IGARSS53475.2024.10642397",
        "http://arxiv.org/abs/2503.15474v1",
        "http://arxiv.org/pdf/2503.15474v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2503.15415v1",
      "title": "Automated Processing of eXplainable Artificial Intelligence Outputs in\n  Deep Learning Models for Fault Diagnostics of Large Infrastructures",
      "published": "2025-03-19T16:57:00Z",
      "updated": "2025-03-19T16:57:00Z",
      "summary": "Deep Learning (DL) models processing images to recognize the health state of\nlarge infrastructure components can exhibit biases and rely on non-causal\nshortcuts. eXplainable Artificial Intelligence (XAI) can address these issues\nbut manually analyzing explanations generated by XAI techniques is\ntime-consuming and prone to errors. This work proposes a novel framework that\ncombines post-hoc explanations with semi-supervised learning to automatically\nidentify anomalous explanations that deviate from those of correctly classified\nimages and may therefore indicate model abnormal behaviors. This significantly\nreduces the workload for maintenance decision-makers, who only need to manually\nreclassify images flagged as having anomalous explanations. The proposed\nframework is applied to drone-collected images of insulator shells for power\ngrid infrastructure monitoring, considering two different Convolutional Neural\nNetworks (CNNs), GradCAM explanations and Deep Semi-Supervised Anomaly\nDetection. The average classification accuracy on two faulty classes is\nimproved by 8% and maintenance operators are required to manually reclassify\nonly 15% of the images. We compare the proposed framework with a\nstate-of-the-art approach based on the faithfulness metric: the experimental\nresults obtained demonstrate that the proposed framework consistently achieves\nF_1 scores larger than those of the faithfulness-based approach. Additionally,\nthe proposed framework successfully identifies correct classifications that\nresult from non-causal shortcuts, such as the presence of ID tags printed on\ninsulator shells.",
      "authors": [
        "Giovanni Floreale",
        "Piero Baraldi",
        "Enrico Zio",
        "Olga Fink"
      ],
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "links": [
        "http://arxiv.org/abs/2503.15415v1",
        "http://arxiv.org/pdf/2503.15415v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2503.15407v1",
      "title": "Exploiting Prior Knowledge in Preferential Learning of Individualized\n  Autonomous Vehicle Driving Styles",
      "published": "2025-03-19T16:47:56Z",
      "updated": "2025-03-19T16:47:56Z",
      "summary": "Trajectory planning for automated vehicles commonly employs optimization over\na moving horizon - Model Predictive Control - where the cost function\ncritically influences the resulting driving style. However, finding a suitable\ncost function that results in a driving style preferred by passengers remains\nan ongoing challenge. We employ preferential Bayesian optimization to learn the\ncost function by iteratively querying a passenger's preference. Due to\nincreasing dimensionality of the parameter space, preference learning\napproaches might struggle to find a suitable optimum with a limited number of\nexperiments and expose the passenger to discomfort when exploring the parameter\nspace. We address these challenges by incorporating prior knowledge into the\npreferential Bayesian optimization framework. Our method constructs a virtual\ndecision maker from real-world human driving data to guide parameter sampling.\nIn a simulation experiment, we achieve faster convergence of the\nprior-knowledge-informed learning procedure compared to existing preferential\nBayesian optimization approaches and reduce the number of inadequate driving\nstyles sampled.",
      "authors": [
        "Lukas Theiner",
        "Sebastian Hirt",
        "Alexander Steinke",
        "Rolf Findeisen"
      ],
      "categories": [
        "eess.SY",
        "cs.LG",
        "cs.SY"
      ],
      "links": [
        "http://arxiv.org/abs/2503.15407v1",
        "http://arxiv.org/pdf/2503.15407v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2503.15403v1",
      "title": "HQNN-FSP: A Hybrid Classical-Quantum Neural Network for Regression-Based\n  Financial Stock Market Prediction",
      "published": "2025-03-19T16:44:21Z",
      "updated": "2025-03-19T16:44:21Z",
      "summary": "Financial time-series forecasting remains a challenging task due to complex\ntemporal dependencies and market fluctuations. This study explores the\npotential of hybrid quantum-classical approaches to assist in financial trend\nprediction by leveraging quantum resources for improved feature representation\nand learning. A custom Quantum Neural Network (QNN) regressor is introduced,\ndesigned with a novel ansatz tailored for financial applications. Two hybrid\noptimization strategies are proposed: (1) a sequential approach where classical\nrecurrent models (RNN/LSTM) extract temporal dependencies before quantum\nprocessing, and (2) a joint learning framework that optimizes classical and\nquantum parameters simultaneously. Systematic evaluation using TimeSeriesSplit,\nk-fold cross-validation, and predictive error analysis highlights the ability\nof these hybrid models to integrate quantum computing into financial\nforecasting workflows. The findings demonstrate how quantum-assisted learning\ncan contribute to financial modeling, offering insights into the practical role\nof quantum resources in time-series analysis.",
      "authors": [
        "Prashant Kumar Choudhary",
        "Nouhaila Innan",
        "Muhammad Shafique",
        "Rajeev Singh"
      ],
      "categories": [
        "q-fin.ST",
        "cs.LG",
        "quant-ph"
      ],
      "links": [
        "http://arxiv.org/abs/2503.15403v1",
        "http://arxiv.org/pdf/2503.15403v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2503.15378v1",
      "title": "Probabilistic Flexibility Aggregation of DERs for Ancillary Services\n  Provision",
      "published": "2025-03-19T16:14:48Z",
      "updated": "2025-03-19T16:14:48Z",
      "summary": "This paper presents a grid-aware probabilistic approach to compute the\naggregated flexibility at the grid connection point (GCP) of active\ndistribution networks (ADNs) to allow the participation of DERs in ancillary\nservices (AS) markets. Specifically an optimal power flow (OPF) method using a\nlinear network model is used to compute the aggregated capability for the\nprovision of multiple AS. We start from the method proposed in [1] and extend\nit to allow for optimizing the provision of multiple services simultaneously,\nensure cost-effectiveness of the used DERs and handle uncertainties in a\nprobabilistic way. The allocation of individual DERs power flexibilities\naccounts for the operational costs associated to the provision of different\nservices and ensures cost-effectiveness while maximizing the value of the\nadvertised aggregated flexibility, assuming known service prices. Empirical\nuncertainty sets are obtained to achieve a predefined coverage of the\nprobability distribution in line with recent developments in the Nordic AS\nmarkets. Finally, a feeder-decomposition approach is proposed to ensure the\nmethods applicability to realistic distribution networks with a large number of\nbuses. Different case studies show the effectiveness of the method, highlight\nthe importance of accounting for network constraints and illustrate its\napplicability to realistic distribution systems.",
      "authors": [
        "Matthieu Jacobs",
        "Mario Paolone"
      ],
      "categories": [
        "eess.SY",
        "cs.SY"
      ],
      "links": [
        "http://arxiv.org/abs/2503.15378v1",
        "http://arxiv.org/pdf/2503.15378v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2503.15374v1",
      "title": "Real-world validation of a multimodal LLM-powered pipeline for\n  High-Accuracy Clinical Trial Patient Matching leveraging EHR data",
      "published": "2025-03-19T16:12:11Z",
      "updated": "2025-03-19T16:12:11Z",
      "summary": "Background: Patient recruitment in clinical trials is hindered by complex\neligibility criteria and labor-intensive chart reviews. Prior research using\ntext-only models have struggled to address this problem in a reliable and\nscalable way due to (1) limited reasoning capabilities, (2) information loss\nfrom converting visual records to text, and (3) lack of a generic EHR\nintegration to extract patient data.\n  Methods: We introduce a broadly applicable, integration-free, LLM-powered\npipeline that automates patient-trial matching using unprocessed documents\nextracted from EHRs. Our approach leverages (1) the new reasoning-LLM paradigm,\nenabling the assessment of even the most complex criteria, (2) visual\ncapabilities of latest LLMs to interpret medical records without lossy\nimage-to-text conversions, and (3) multimodal embeddings for efficient medical\nrecord search. The pipeline was validated on the n2c2 2018 cohort selection\ndataset (288 diabetic patients) and a real-world dataset composed of 485\npatients from 30 different sites matched against 36 diverse trials.\n  Results: On the n2c2 dataset, our method achieved a new state-of-the-art\ncriterion-level accuracy of 93\\%. In real-world trials, the pipeline yielded an\naccuracy of 87\\%, undermined by the difficulty to replicate human\ndecision-making when medical records lack sufficient information. Nevertheless,\nusers were able to review overall eligibility in under 9 minutes per patient on\naverage, representing an 80\\% improvement over traditional manual chart\nreviews.\n  Conclusion: This pipeline demonstrates robust performance in clinical trial\npatient matching without requiring custom integration with site systems or\ntrial-specific tailoring, thereby enabling scalable deployment across sites\nseeking to leverage AI for patient matching.",
      "authors": [
        "Anatole Callies",
        "Quentin Bodinier",
        "Philippe Ravaud",
        "Kourosh Davarpanah"
      ],
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "links": [
        "http://arxiv.org/abs/2503.15374v1",
        "http://arxiv.org/pdf/2503.15374v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2503.15373v1",
      "title": "Priority-driven Constraints Softening in Safe MPC for Perturbed Systems",
      "published": "2025-03-19T16:11:25Z",
      "updated": "2025-03-19T16:11:25Z",
      "summary": "This paper presents a safe model predictive control (SMPC) framework designed\nto ensure the satisfaction of hard constraints for systems perturbed by an\nexternal disturbance. Such safety guarantees are ensured, despite the\ndisturbance, by online softening a subset of adjustable constraints defined by\nthe designer. The selection of the constraints to be softened is made online\nbased on a predefined priority assigned to each adjustable constraint. The\ndesign of a learning-based algorithm enables real-time computation while\npreserving the original safety properties.\n  Simulations results, obtained from an automated driving application, show\nthat the proposed approach provides guarantees of collision-avoidance hard\nconstraints despite the unpredicted behaviors of the surrounding environment.",
      "authors": [
        "Ying Shuai Quan",
        "Mohammad Jeddi",
        "Francesco Prignoli",
        "Paolo Falcone"
      ],
      "categories": [
        "eess.SY",
        "cs.SY"
      ],
      "links": [
        "http://arxiv.org/abs/2503.15373v1",
        "http://arxiv.org/pdf/2503.15373v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2503.15329v1",
      "title": "Euclid Quick Data Release (Q1). LEMON -- Lens Modelling with Neural\n  networks. Automated and fast modelling of Euclid gravitational lenses with a\n  singular isothermal ellipsoid mass profile",
      "published": "2025-03-19T15:27:24Z",
      "updated": "2025-03-19T15:27:24Z",
      "summary": "The Euclid mission aims to survey around 14000 deg^{2} of extragalactic sky,\nproviding around 10^{5} gravitational lens images. Modelling of gravitational\nlenses is fundamental to estimate the total mass of the lens galaxy, along with\nits dark matter content. Traditional modelling of gravitational lenses is\ncomputationally intensive and requires manual input. In this paper, we use a\nBayesian neural network, LEns MOdelling with Neural networks (LEMON), for\nmodelling Euclid gravitational lenses with a singular isothermal ellipsoid mass\nprofile. Our method estimates key lens mass profile parameters, such as the\nEinstein radius, while also predicting the light parameters of foreground\ngalaxies and their uncertainties. We validate LEMON's performance on both mock\nEuclid data sets, real Euclidised lenses observed with Hubble Space Telescope\n(hereafter HST), and real Euclid lenses found in the Perseus ERO field,\ndemonstrating the ability of LEMON to predict parameters of both simulated and\nreal lenses. Results show promising accuracy and reliability in predicting the\nEinstein radius, axis ratio, position angle, effective radius, S\\'ersic index,\nand lens magnitude for simulated lens galaxies. The application to real data,\nincluding the latest Quick Release 1 strong lens candidates, provides\nencouraging results, particularly for the Einstein radius. We also verified\nthat LEMON has the potential to accelerate traditional modelling methods, by\ngiving to the classical optimiser the LEMON predictions as starting points,\nresulting in a speed-up of up to 26 times the original time needed to model a\nsample of gravitational lenses, a result that would be impossible with randomly\ninitialised guesses. This work represents a significant step towards efficient,\nautomated gravitational lens modelling, which is crucial for handling the large\ndata volumes expected from Euclid.",
      "authors": [
        " Euclid Collaboration",
        "V. Busillo",
        "C. Tortora",
        "R. B. Metcalf",
        "J. W. Nightingale",
        "M. Meneghetti",
        "F. Gentile",
        "R. Gavazzi",
        "F. Zhong",
        "R. Li",
        "B. Cl\u00e9ment",
        "G. Covone",
        "N. R. Napolitano",
        "F. Courbin",
        "M. Walmsley",
        "E. Jullo",
        "J. Pearson",
        "D. Scott",
        "A. M. C. Le Brun",
        "L. Leuzzi",
        "N. Aghanim",
        "B. Altieri",
        "A. Amara",
        "S. Andreon",
        "H. Aussel",
        "C. Baccigalupi",
        "M. Baldi",
        "S. Bardelli",
        "P. Battaglia",
        "A. Biviano",
        "E. Branchini",
        "M. Brescia",
        "J. Brinchmann",
        "S. Camera",
        "G. Ca\u00f1as-Herrera",
        "V. Capobianco",
        "C. Carbone",
        "V. F. Cardone",
        "J. Carretero",
        "S. Casas",
        "M. Castellano",
        "G. Castignani",
        "S. Cavuoti",
        "K. C. Chambers",
        "A. Cimatti",
        "C. Colodro-Conde",
        "G. Congedo",
        "C. J. Conselice",
        "L. Conversi",
        "Y. Copin",
        "H. M. Courtois",
        "M. Cropper",
        "A. Da Silva",
        "H. Degaudenzi",
        "S. de la Torre",
        "G. De Lucia",
        "A. M. Di Giorgio",
        "J. Dinis",
        "H. Dole",
        "F. Dubath",
        "X. Dupac",
        "S. Dusini",
        "S. Escoffier",
        "M. Farina",
        "R. Farinelli",
        "F. Faustini",
        "S. Ferriol",
        "F. Finelli",
        "S. Fotopoulou",
        "M. Frailis",
        "E. Franceschi",
        "S. Galeotta",
        "K. George",
        "W. Gillard",
        "B. Gillis",
        "C. Giocoli",
        "J. Gracia-Carpio",
        "B. R. Granett",
        "A. Grazian",
        "F. Grupp",
        "S. V. H. Haugan",
        "W. Holmes",
        "I. Hook",
        "F. Hormuth",
        "A. Hornstrup",
        "P. Hudelot",
        "K. Jahnke",
        "M. Jhabvala",
        "B. Joachimi",
        "E. Keih\u00e4nen",
        "S. Kermiche",
        "A. Kiessling",
        "B. Kubik",
        "M. K\u00fcmmel",
        "M. Kunz",
        "H. Kurki-Suonio",
        "Q. Le Boulc'h",
        "S. Ligori",
        "P. B. Lilje",
        "V. Lindholm",
        "I. Lloro",
        "G. Mainetti",
        "D. Maino",
        "E. Maiorano",
        "O. Mansutti",
        "O. Marggraf",
        "K. Markovic",
        "M. Martinelli",
        "N. Martinet",
        "F. Marulli",
        "R. Massey",
        "S. Maurogordato",
        "E. Medinaceli",
        "S. Mei",
        "Y. Mellier",
        "E. Merlin",
        "G. Meylan",
        "A. Mora",
        "M. Moresco",
        "L. Moscardini",
        "R. Nakajima",
        "C. Neissner",
        "S. -M. Niemi",
        "C. Padilla",
        "S. Paltani",
        "F. Pasian",
        "K. Pedersen",
        "V. Pettorino",
        "S. Pires",
        "G. Polenta",
        "M. Poncet",
        "L. A. Popa",
        "L. Pozzetti",
        "F. Raison",
        "R. Rebolo",
        "A. Renzi",
        "J. Rhodes",
        "G. Riccio",
        "E. Romelli",
        "M. Roncarelli",
        "R. Saglia",
        "Z. Sakr",
        "A. G. S\u00e1nchez",
        "D. Sapone",
        "B. Sartoris",
        "J. A. Schewtschenko",
        "M. Schirmer",
        "P. Schneider",
        "T. Schrabback",
        "A. Secroun",
        "E. Sefusatti",
        "G. Seidel",
        "M. Seiffert",
        "S. Serrano",
        "P. Simon",
        "C. Sirignano",
        "G. Sirri",
        "G. Smadja",
        "L. Stanco",
        "J. Steinwagner",
        "P. Tallada-Cresp\u00ed",
        "A. N. Taylor",
        "I. Tereno",
        "S. Toft",
        "R. Toledo-Moreo",
        "F. Torradeflot",
        "I. Tutusaus",
        "L. Valenziano",
        "J. Valiviita",
        "T. Vassallo",
        "A. Veropalumbo",
        "Y. Wang",
        "J. Weller",
        "G. Zamorani",
        "E. Zucca",
        "V. Allevato",
        "M. Ballardini",
        "M. Bolzonella",
        "E. Bozzo",
        "C. Burigana",
        "R. Cabanac",
        "M. Calabrese",
        "D. Di Ferdinando",
        "J. A. Escartin Vigo",
        "L. Gabarra",
        "M. Huertas-Company",
        "S. Matthew",
        "N. Mauri",
        "A. A. Nucita",
        "A. Pezzotta",
        "M. P\u00f6ntinen",
        "C. Porciani",
        "V. Scottez",
        "M. Tenti",
        "M. Viel",
        "M. Wiesmann",
        "Y. Akrami",
        "S. Alvi",
        "I. T. Andika",
        "S. Anselmi",
        "M. Archidiacono",
        "F. Atrio-Barandela",
        "D. Bertacca",
        "M. Bethermin",
        "A. Blanchard",
        "L. Blot",
        "S. Borgani",
        "M. L. Brown",
        "S. Bruton",
        "A. Calabro",
        "B. Camacho Quevedo",
        "A. Cappi",
        "F. Caro",
        "C. S. Carvalho",
        "T. Castro",
        "F. Cogato",
        "S. Conseil",
        "S. Contarini",
        "A. R. Cooray",
        "O. Cucciati",
        "F. De Paolis",
        "G. Desprez",
        "A. D\u00edaz-S\u00e1nchez",
        "S. Di Domizio",
        "J. M. Diego",
        "P. Dimauro",
        "A. Enia",
        "Y. Fang",
        "A. G. Ferrari",
        "P. G. Ferreira",
        "A. Finoguenov",
        "A. Franco",
        "K. Ganga",
        "J. Garc\u00eda-Bellido",
        "T. Gasparetto",
        "V. Gautard",
        "E. Gaztanaga",
        "F. Giacomini",
        "F. Gianotti",
        "G. Gozaliasl",
        "M. Guidi",
        "C. M. Gutierrez",
        "A. Hall",
        "W. G. Hartley",
        "S. Hemmati",
        "C. Hern\u00e1ndez-Monteagudo",
        "H. Hildebrandt",
        "J. Hjorth",
        "J. J. E. Kajava",
        "Y. Kang",
        "V. Kansal",
        "D. Karagiannis",
        "K. Kiiveri",
        "C. C. Kirkpatrick",
        "S. Kruk",
        "M. Lattanzi",
        "J. Le Graet",
        "L. Legrand",
        "M. Lembo",
        "F. Lepori",
        "G. Leroy",
        "J. Lesgourgues",
        "T. I. Liaudat",
        "S. J. Liu",
        "A. Loureiro",
        "J. Macias-Perez",
        "G. Maggio",
        "M. Magliocchetti",
        "F. Mannucci",
        "R. Maoli",
        "J. Mart\u00edn-Fleitas",
        "C. J. A. P. Martins",
        "L. Maurin",
        "M. Miluzio",
        "P. Monaco",
        "C. Moretti",
        "G. Morgante",
        "S. Nadathur",
        "K. Naidoo",
        "P. Natoli",
        "A. Navarro-Alsina",
        "S. Nesseris",
        "F. Passalacqua",
        "K. Paterson",
        "L. Patrizii",
        "A. Pisani",
        "D. Potter",
        "S. Quai",
        "M. Radovich",
        "I. Risso",
        "P. -F. Rocci",
        "S. Sacquegna",
        "M. Sahl\u00e9n",
        "E. Sarpa",
        "A. Schneider",
        "M. Schultheis",
        "D. Sciotti",
        "E. Sellentin",
        "M. Sereno",
        "L. C. Smith",
        "J. Stadel",
        "K. Tanidis",
        "C. Tao",
        "G. Testera",
        "R. Teyssier",
        "S. Tosi",
        "A. Troja",
        "M. Tucci",
        "C. Valieri",
        "A. Venhola",
        "D. Vergani",
        "G. Vernardos",
        "G. Verza",
        "P. Vielzeuf",
        "N. A. Walton"
      ],
      "categories": [
        "astro-ph.CO"
      ],
      "links": [
        "http://arxiv.org/abs/2503.15329v1",
        "http://arxiv.org/pdf/2503.15329v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2503.15325v1",
      "title": "Euclid Quick Data Release (Q1) The Strong Lensing Discovery Engine B --\n  Early strong lens candidates from visual inspection of high velocity\n  dispersion galaxies",
      "published": "2025-03-19T15:27:20Z",
      "updated": "2025-03-19T15:27:20Z",
      "summary": "We present a search for strong gravitational lenses in Euclid imaging with\nhigh stellar velocity dispersion ($\\sigma_\\nu > 180$ km/s) reported by SDSS and\nDESI. We performed expert visual inspection and classification of $11\\,660$\n\\Euclid images. We discovered 38 grade A and 40 grade B candidate lenses,\nconsistent with an expected sample of $\\sim$32. Palomar spectroscopy confirmed\n5 lens systems, while DESI spectra confirmed one, provided ambiguous results\nfor another, and help to discard one. The \\Euclid automated lens modeler\nmodelled 53 candidates, confirming 38 as lenses, failing to model 9, and ruling\nout 6 grade B candidates. For the remaining 25 candidates we could not gather\nadditional information. More importantly, our expert-classified non-lenses\nprovide an excellent training set for machine learning lens classifiers. We\ncreate high-fidelity simulations of \\Euclid lenses by painting realistic lensed\nsources behind the expert tagged (non-lens) luminous red galaxies. This\ntraining set is the foundation stone for the \\Euclid galaxy-galaxy strong\nlensing discovery engine.",
      "authors": [
        " Euclid Collaboration",
        "K. Rojas",
        "T. E. Collett",
        "J. A. Acevedo Barroso",
        "J. W. Nightingale",
        "D. Stern",
        "L. A. Moustakas",
        "S. Schuldt",
        "G. Despali",
        "A. Melo",
        "M. Walmsley",
        "D. J. Ballard",
        "W. J. R. Enzi",
        "T. Li",
        "A. Sainz de Murieta",
        "I. T. Andika",
        "B. Cl\u00e9ment",
        "F. Courbin",
        "L. R. Ecker",
        "R. Gavazzi",
        "N. Jackson",
        "A. Kov\u00e1cs",
        "P. Matavulj",
        "M. Meneghetti",
        "S. Serjeant",
        "D. Sluse",
        "C. Tortora",
        "A. Verma",
        "L. Marchetti",
        "C. M. O'Riordan",
        "K. McCarthy",
        "S. H. Suyu",
        "R. B. Metcalf",
        "N. Aghanim",
        "B. Altieri",
        "A. Amara",
        "S. Andreon",
        "N. Auricchio",
        "H. Aussel",
        "C. Baccigalupi",
        "M. Baldi",
        "A. Balestra",
        "S. Bardelli",
        "P. Battaglia",
        "R. Bender",
        "A. Biviano",
        "A. Bonchi",
        "E. Branchini",
        "M. Brescia",
        "J. Brinchmann",
        "S. Camera",
        "G. Ca\u00f1as-Herrera",
        "V. Capobianco",
        "C. Carbone",
        "V. F. Cardone",
        "J. Carretero",
        "S. Casas",
        "M. Castellano",
        "G. Castignani",
        "S. Cavuoti",
        "K. C. Chambers",
        "A. Cimatti",
        "C. Colodro-Conde",
        "G. Congedo",
        "C. J. Conselice",
        "L. Conversi",
        "Y. Copin",
        "H. M. Courtois",
        "M. Cropper",
        "A. Da Silva",
        "H. Degaudenzi",
        "G. De Lucia",
        "A. M. Di Giorgio",
        "C. Dolding",
        "H. Dole",
        "F. Dubath",
        "X. Dupac",
        "S. Escoffier",
        "M. Fabricius",
        "M. Farina",
        "R. Farinelli",
        "F. Faustini",
        "S. Ferriol",
        "F. Finelli",
        "S. Fotopoulou",
        "M. Frailis",
        "E. Franceschi",
        "S. Galeotta",
        "K. George",
        "W. Gillard",
        "B. Gillis",
        "C. Giocoli",
        "P. G\u00f3mez-Alvarez",
        "J. Gracia-Carpio",
        "B. R. Granett",
        "A. Grazian",
        "F. Grupp",
        "L. Guzzo",
        "S. Gwyn",
        "S. V. H. Haugan",
        "W. Holmes",
        "I. M. Hook",
        "F. Hormuth",
        "A. Hornstrup",
        "P. Hudelot",
        "K. Jahnke",
        "M. Jhabvala",
        "E. Keih\u00e4nen",
        "S. Kermiche",
        "A. Kiessling",
        "B. Kubik",
        "K. Kuijken",
        "M. K\u00fcmmel",
        "M. Kunz",
        "H. Kurki-Suonio",
        "Q. Le Boulc'h",
        "A. M. C. Le Brun",
        "D. Le Mignant",
        "P. Liebing",
        "S. Ligori",
        "P. B. Lilje",
        "V. Lindholm",
        "I. Lloro",
        "G. Mainetti",
        "D. Maino",
        "E. Maiorano",
        "O. Mansutti",
        "S. Marcin",
        "O. Marggraf",
        "M. Martinelli",
        "N. Martinet",
        "F. Marulli",
        "R. Massey",
        "S. Maurogordato",
        "H. J. McCracken",
        "E. Medinaceli",
        "S. Mei",
        "M. Melchior",
        "Y. Mellier",
        "E. Merlin",
        "G. Meylan",
        "A. Mora",
        "M. Moresco",
        "L. Moscardini",
        "R. Nakajima",
        "C. Neissner",
        "R. C. Nichol",
        "S. -M. Niemi",
        "C. Padilla",
        "S. Paltani",
        "F. Pasian",
        "K. Pedersen",
        "W. J. Percival",
        "V. Pettorino",
        "S. Pires",
        "G. Polenta",
        "M. Poncet",
        "L. A. Popa",
        "L. Pozzetti",
        "F. Raison",
        "R. Rebolo",
        "A. Renzi",
        "J. Rhodes",
        "G. Riccio",
        "E. Romelli",
        "M. Roncarelli",
        "R. Saglia",
        "Z. Sakr",
        "A. G. S\u00e1nchez",
        "D. Sapone",
        "B. Sartoris",
        "J. A. Schewtschenko",
        "M. Schirmer",
        "P. Schneider",
        "T. Schrabback",
        "A. Secroun",
        "G. Seidel",
        "M. Seiffert",
        "S. Serrano",
        "P. Simon",
        "C. Sirignano",
        "G. Sirri",
        "L. Stanco",
        "J. Steinwagner",
        "P. Tallada-Cresp\u00ed",
        "A. N. Taylor",
        "I. Tereno",
        "S. Toft",
        "R. Toledo-Moreo",
        "F. Torradeflot",
        "I. Tutusaus",
        "L. Valenziano",
        "J. Valiviita",
        "T. Vassallo",
        "G. Verdoes Kleijn",
        "A. Veropalumbo",
        "Y. Wang",
        "J. Weller",
        "A. Zacchei",
        "G. Zamorani",
        "F. M. Zerbi",
        "E. Zucca",
        "M. Ballardini",
        "M. Bolzonella",
        "E. Bozzo",
        "C. Burigana",
        "R. Cabanac",
        "A. Cappi",
        "D. Di Ferdinando",
        "J. A. Escartin Vigo",
        "L. Gabarra",
        "J. Mart\u00edn-Fleitas",
        "S. Matthew",
        "N. Mauri",
        "A. Pezzotta",
        "M. P\u00f6ntinen",
        "C. Porciani",
        "I. Risso",
        "V. Scottez",
        "M. Sereno",
        "M. Tenti",
        "M. Viel",
        "M. Wiesmann",
        "Y. Akrami",
        "S. Alvi",
        "S. Anselmi",
        "M. Archidiacono",
        "F. Atrio-Barandela",
        "C. Benoist",
        "K. Benson",
        "P. Bergamini",
        "D. Bertacca",
        "M. Bethermin",
        "A. Blanchard",
        "L. Blot",
        "M. L. Brown",
        "S. Bruton",
        "A. Calabro",
        "B. Camacho Quevedo",
        "F. Caro",
        "C. S. Carvalho",
        "T. Castro",
        "F. Cogato",
        "A. R. Cooray",
        "O. Cucciati",
        "S. Davini",
        "F. De Paolis",
        "G. Desprez",
        "A. D\u00edaz-S\u00e1nchez",
        "J. J. Diaz",
        "S. Di Domizio",
        "J. M. Diego",
        "P. -A. Duc",
        "A. Enia",
        "Y. Fang",
        "A. G. Ferrari",
        "P. G. Ferreira",
        "A. Finoguenov",
        "A. Fontana",
        "A. Franco",
        "K. Ganga",
        "J. Garc\u00eda-Bellido",
        "T. Gasparetto",
        "V. Gautard",
        "E. Gaztanaga",
        "F. Giacomini",
        "F. Gianotti",
        "G. Gozaliasl",
        "M. Guidi",
        "C. M. Gutierrez",
        "A. Hall",
        "W. G. Hartley",
        "C. Hern\u00e1ndez-Monteagudo",
        "H. Hildebrandt",
        "J. Hjorth",
        "J. J. E. Kajava",
        "Y. Kang",
        "V. Kansal",
        "D. Karagiannis",
        "K. Kiiveri",
        "C. C. Kirkpatrick",
        "S. Kruk",
        "J. Le Graet",
        "L. Legrand",
        "M. Lembo",
        "F. Lepori",
        "G. Leroy",
        "G. F. Lesci",
        "J. Lesgourgues",
        "L. Leuzzi",
        "T. I. Liaudat",
        "A. Loureiro",
        "J. Macias-Perez",
        "G. Maggio",
        "M. Magliocchetti",
        "E. A. Magnier",
        "F. Mannucci",
        "R. Maoli",
        "C. J. A. P. Martins",
        "L. Maurin",
        "M. Miluzio",
        "P. Monaco",
        "C. Moretti",
        "G. Morgante",
        "S. Nadathur",
        "K. Naidoo",
        "A. Navarro-Alsina",
        "S. Nesseris",
        "F. Passalacqua",
        "K. Paterson",
        "L. Patrizii",
        "A. Pisani",
        "D. Potter",
        "S. Quai",
        "M. Radovich",
        "P. -F. Rocci",
        "S. Sacquegna",
        "M. Sahl\u00e9n",
        "D. B. Sanders",
        "E. Sarpa",
        "C. Scarlata",
        "A. Schneider",
        "D. Sciotti",
        "E. Sellentin",
        "L. C. Smith",
        "K. Tanidis",
        "G. Testera",
        "R. Teyssier",
        "A. Troja",
        "M. Tucci",
        "C. Valieri",
        "A. Venhola",
        "D. Vergani",
        "G. Vernardos",
        "G. Verza",
        "P. Vielzeuf",
        "N. A. Walton",
        "J. Wilde",
        "D. Scott"
      ],
      "categories": [
        "astro-ph.GA",
        "astro-ph.CO"
      ],
      "links": [
        "http://arxiv.org/abs/2503.15325v1",
        "http://arxiv.org/pdf/2503.15325v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2503.15310v1",
      "title": "Euclid Quick Data Release (Q1): First visual morphology catalogue",
      "published": "2025-03-19T15:27:05Z",
      "updated": "2025-03-19T15:27:05Z",
      "summary": "We present a detailed visual morphology catalogue for Euclid's Quick Release\n1 (Q1). Our catalogue includes galaxy features such as bars, spiral arms, and\nongoing mergers, for the 378000 bright ($I_E < 20.5$) or extended (area $\\geq\n700\\,$pixels) galaxies in Q1. The catalogue was created by finetuning the\nZoobot galaxy foundation models on annotations from an intensive one month\ncampaign by Galaxy Zoo volunteers. Our measurements are fully automated and\nhence fully scaleable. This catalogue is the first 0.4% of the approximately\n100 million galaxies where Euclid will ultimately resolve detailed morphology.",
      "authors": [
        " Euclid Collaboration",
        "M. Walmsley",
        "M. Huertas-Company",
        "L. Quilley",
        "K. L. Masters",
        "S. Kruk",
        "K. A. Remmelgas",
        "J. J. Popp",
        "E. Romelli",
        "D. O'Ryan",
        "H. J. Dickinson",
        "C. J. Lintott",
        "S. Serjeant",
        "R. J. Smethurst",
        "B. Simmons",
        "J. Shingirai Makechemu",
        "I. L. Garland",
        "H. Roberts",
        "K. Mantha",
        "L. F. Fortson",
        "T. G\u00e9ron",
        "W. Keel",
        "E. M. Baeten",
        "C. Macmillan",
        "J. Bovy",
        "S. Casas",
        "C. De Leo",
        "H. Dom\u00ednguez S\u00e1nchez",
        "J. Katona",
        "A. Kov\u00e1cs",
        "N. Aghanim",
        "B. Altieri",
        "A. Amara",
        "S. Andreon",
        "N. Auricchio",
        "H. Aussel",
        "C. Baccigalupi",
        "M. Baldi",
        "A. Balestra",
        "S. Bardelli",
        "A. Basset",
        "P. Battaglia",
        "R. Bender",
        "A. Biviano",
        "A. Bonchi",
        "E. Branchini",
        "M. Brescia",
        "J. Brinchmann",
        "S. Camera",
        "G. Ca\u00f1as-Herrera",
        "V. Capobianco",
        "C. Carbone",
        "J. Carretero",
        "F. J. Castander",
        "M. Castellano",
        "G. Castignani",
        "S. Cavuoti",
        "K. C. Chambers",
        "A. Cimatti",
        "C. Colodro-Conde",
        "G. Congedo",
        "C. J. Conselice",
        "L. Conversi",
        "Y. Copin",
        "F. Courbin",
        "H. M. Courtois",
        "M. Cropper",
        "A. Da Silva",
        "H. Degaudenzi",
        "G. De Lucia",
        "A. M. Di Giorgio",
        "C. Dolding",
        "H. Dole",
        "F. Dubath",
        "C. A. J. Duncan",
        "X. Dupac",
        "S. Dusini",
        "A. Ealet",
        "S. Escoffier",
        "M. Fabricius",
        "M. Farina",
        "R. Farinelli",
        "F. Faustini",
        "F. Finelli",
        "P. Fosalba",
        "S. Fotopoulou",
        "M. Frailis",
        "E. Franceschi",
        "S. Galeotta",
        "K. George",
        "B. Gillis",
        "C. Giocoli",
        "P. G\u00f3mez-Alvarez",
        "J. Gracia-Carpio",
        "B. R. Granett",
        "A. Grazian",
        "F. Grupp",
        "S. Gwyn",
        "S. V. H. Haugan",
        "H. Hoekstra",
        "W. Holmes",
        "I. M. Hook",
        "F. Hormuth",
        "A. Hornstrup",
        "P. Hudelot",
        "K. Jahnke",
        "M. Jhabvala",
        "B. Joachimi",
        "E. Keih\u00e4nen",
        "S. Kermiche",
        "A. Kiessling",
        "R. Kohley",
        "B. Kubik",
        "K. Kuijken",
        "M. K\u00fcmmel",
        "M. Kunz",
        "H. Kurki-Suonio",
        "O. Lahav",
        "Q. Le Boulc'h",
        "A. M. C. Le Brun",
        "D. Le Mignant",
        "P. Liebing",
        "S. Ligori",
        "P. B. Lilje",
        "V. Lindholm",
        "I. Lloro",
        "G. Mainetti",
        "D. Maino",
        "E. Maiorano",
        "O. Mansutti",
        "S. Marcin",
        "O. Marggraf",
        "M. Martinelli",
        "N. Martinet",
        "F. Marulli",
        "R. Massey",
        "S. Maurogordato",
        "H. J. McCracken",
        "E. Medinaceli",
        "S. Mei",
        "M. Melchior",
        "Y. Mellier",
        "M. Meneghetti",
        "E. Merlin",
        "G. Meylan",
        "A. Mora",
        "M. Moresco",
        "L. Moscardini",
        "R. Nakajima",
        "C. Neissner",
        "R. C. Nichol",
        "S. -M. Niemi",
        "J. W. Nightingale",
        "C. Padilla",
        "S. Paltani",
        "F. Pasian",
        "K. Pedersen",
        "W. J. Percival",
        "V. Pettorino",
        "S. Pires",
        "G. Polenta",
        "M. Poncet",
        "L. A. Popa",
        "L. Pozzetti",
        "F. Raison",
        "R. Rebolo",
        "A. Renzi",
        "J. Rhodes",
        "G. Riccio",
        "M. Roncarelli",
        "B. Rusholme",
        "R. Saglia",
        "Z. Sakr",
        "A. G. S\u00e1nchez",
        "D. Sapone",
        "B. Sartoris",
        "J. A. Schewtschenko",
        "P. Schneider",
        "T. Schrabback",
        "M. Scodeggio",
        "A. Secroun",
        "G. Seidel",
        "M. Seiffert",
        "S. Serrano",
        "P. Simon",
        "C. Sirignano",
        "G. Sirri",
        "L. Stanco",
        "J. Steinwagner",
        "P. Tallada-Cresp\u00ed",
        "D. Tavagnacco",
        "A. N. Taylor",
        "H. I. Teplitz",
        "I. Tereno",
        "N. Tessore",
        "S. Toft",
        "R. Toledo-Moreo",
        "F. Torradeflot",
        "I. Tutusaus",
        "E. A. Valentijn",
        "L. Valenziano",
        "J. Valiviita",
        "T. Vassallo",
        "G. Verdoes Kleijn",
        "A. Veropalumbo",
        "Y. Wang",
        "J. Weller",
        "A. Zacchei",
        "G. Zamorani",
        "F. M. Zerbi",
        "I. A. Zinchenko",
        "E. Zucca",
        "V. Allevato",
        "M. Ballardini",
        "M. Bolzonella",
        "E. Bozzo",
        "C. Burigana",
        "R. Cabanac",
        "A. Cappi",
        "D. Di Ferdinando",
        "J. A. Escartin Vigo",
        "L. Gabarra",
        "J. Mart\u00edn-Fleitas",
        "S. Matthew",
        "N. Mauri",
        "R. B. Metcalf",
        "A. Pezzotta",
        "M. P\u00f6ntinen",
        "C. Porciani",
        "I. Risso",
        "V. Scottez",
        "M. Sereno",
        "M. Tenti",
        "M. Viel",
        "M. Wiesmann",
        "Y. Akrami",
        "I. T. Andika",
        "S. Anselmi",
        "M. Archidiacono",
        "F. Atrio-Barandela",
        "C. Benoist",
        "K. Benson",
        "D. Bertacca",
        "M. Bethermin",
        "L. Bisigello",
        "A. Blanchard",
        "L. Blot",
        "H. B\u00f6hringer",
        "M. L. Brown",
        "S. Bruton",
        "F. Buitrago",
        "A. Calabro",
        "B. Camacho Quevedo",
        "F. Caro",
        "C. S. Carvalho",
        "T. Castro",
        "F. Cogato",
        "A. R. Cooray",
        "O. Cucciati",
        "S. Davini",
        "F. De Paolis",
        "G. Desprez",
        "A. D\u00edaz-S\u00e1nchez",
        "J. J. Diaz",
        "S. Di Domizio",
        "J. M. Diego",
        "P. -A. Duc",
        "A. Enia",
        "Y. Fang",
        "A. G. Ferrari",
        "A. Finoguenov",
        "A. Fontana",
        "A. Franco",
        "K. Ganga",
        "J. Garc\u00eda-Bellido",
        "T. Gasparetto",
        "V. Gautard",
        "E. Gaztanaga",
        "F. Giacomini",
        "G. Gozaliasl",
        "M. Guidi",
        "C. M. Gutierrez",
        "A. Hall",
        "W. G. Hartley",
        "S. Hemmati",
        "C. Hern\u00e1ndez-Monteagudo",
        "H. Hildebrandt",
        "J. Hjorth",
        "J. J. E. Kajava",
        "Y. Kang",
        "V. Kansal",
        "D. Karagiannis",
        "K. Kiiveri",
        "C. C. Kirkpatrick",
        "J. Le Graet",
        "L. Legrand",
        "M. Lembo",
        "F. Lepori",
        "G. Leroy",
        "G. F. Lesci",
        "J. Lesgourgues",
        "L. Leuzzi",
        "T. I. Liaudat",
        "A. Loureiro",
        "J. Macias-Perez",
        "G. Maggio",
        "M. Magliocchetti",
        "F. Mannucci",
        "R. Maoli",
        "C. J. A. P. Martins",
        "L. Maurin",
        "M. Miluzio",
        "P. Monaco",
        "C. Moretti",
        "G. Morgante",
        "C. Murray",
        "S. Nadathur",
        "K. Naidoo",
        "A. Navarro-Alsina",
        "S. Nesseris",
        "F. Passalacqua",
        "K. Paterson",
        "L. Patrizii",
        "A. Pisani",
        "D. Potter",
        "S. Quai",
        "M. Radovich",
        "P. -F. Rocci",
        "G. Rodighiero",
        "S. Sacquegna",
        "M. Sahl\u00e9n",
        "D. B. Sanders",
        "E. Sarpa",
        "C. Scarlata",
        "J. Schaye",
        "A. Schneider",
        "M. Schultheis",
        "D. Sciotti",
        "E. Sellentin",
        "F. Shankar",
        "L. C. Smith",
        "K. Tanidis",
        "G. Testera",
        "R. Teyssier",
        "S. Tosi",
        "A. Troja",
        "M. Tucci",
        "C. Valieri",
        "A. Venhola",
        "D. Vergani",
        "G. Verza",
        "P. Vielzeuf",
        "N. A. Walton",
        "E. Soubrie",
        "D. Scott"
      ],
      "categories": [
        "astro-ph.GA"
      ],
      "links": [
        "http://arxiv.org/abs/2503.15310v1",
        "http://arxiv.org/pdf/2503.15310v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2503.15270v1",
      "title": "Automating Comment Generation for Smart Contract from Bytecode",
      "published": "2025-03-19T14:45:40Z",
      "updated": "2025-03-19T14:45:40Z",
      "summary": "Recently, smart contracts have played a vital role in automatic financial and\nbusiness transactions. To help end users without programming background to\nbetter understand the logic of smart contracts, previous studies have proposed\nmodels for automatically translating smart contract source code into their\ncorresponding code summaries. However, in practice, only 13% of smart contracts\ndeployed on the Ethereum blockchain are associated with source code. The\npractical usage of these existing tools is significantly restricted.\nConsidering that bytecode is always necessary when deploying smart contracts,\nin this paper, we first introduce the task of automatically generating smart\ncontract code summaries from bytecode. We propose a novel approach, named\nSmartBT (Smart contract Bytecode Translator) for automatically translating\nsmart contract bytecode into fine-grained natural language description\ndirectly. Two key challenges are posed for this task: structural code logic\nhidden in bytecode and the huge semantic gap between bytecode and natural\nlanguage descriptions. To address the first challenge, we transform bytecode\ninto CFG (Control-Flow Graph) to learn code structural and logic details.\nRegarding the second challenge, we introduce an information retrieval component\nto fetch similar comments for filling the semantic gap. Then the structural\ninput and semantic input are used to build an attentional sequence-to-sequence\nneural network model. The copy mechanism is employed to copy rare words\ndirectly from similar comments and the coverage mechanism is employed to\neliminate repetitive outputs. The automatic evaluation results show that\nSmartBT outperforms a set of baselines by a large margin, and the human\nevaluation results show the effectiveness and potential of SmartBT in producing\nmeaningful and accurate comments for smart contract code from bytecode\ndirectly.",
      "authors": [
        "Jianhang Xiang",
        "Zhipeng Gao",
        "Lingfeng Bao",
        "Xing Hu",
        "Jiayuan Chen",
        "Xin Xia"
      ],
      "categories": [
        "cs.SE"
      ],
      "links": [
        "http://dx.doi.org/10.1145/3699597",
        "http://arxiv.org/abs/2503.15270v1",
        "http://arxiv.org/pdf/2503.15270v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2503.15248v1",
      "title": "Automated Non-Functional Requirements Generation in Software Engineering\n  with Large Language Models: A Comparative Study",
      "published": "2025-03-19T14:23:22Z",
      "updated": "2025-03-19T14:23:22Z",
      "summary": "Neglecting non-functional requirements (NFRs) early in software development\ncan lead to critical challenges. Despite their importance, NFRs are often\noverlooked or difficult to identify, impacting software quality. To support\nrequirements engineers in eliciting NFRs, we developed a framework that\nleverages Large Language Models (LLMs) to derive quality-driven NFRs from\nfunctional requirements (FRs). Using a custom prompting technique within a\nDeno-based pipeline, the system identifies relevant quality attributes for each\nfunctional requirement and generates corresponding NFRs, aiding systematic\nintegration. A crucial aspect is evaluating the quality and suitability of\nthese generated requirements. Can LLMs produce high-quality NFR suggestions?\nUsing 34 functional requirements - selected as a representative subset of 3,964\nFRs-the LLMs inferred applicable attributes based on the ISO/IEC 25010:2023\nstandard, generating 1,593 NFRs. A horizontal evaluation covered three\ndimensions: NFR validity, applicability of quality attributes, and\nclassification precision. Ten industry software quality evaluators, averaging\n13 years of experience, assessed a subset for relevance and quality. The\nevaluation showed strong alignment between LLM-generated NFRs and expert\nassessments, with median validity and applicability scores of 5.0 (means: 4.63\nand 4.59, respectively) on a 1-5 scale. In the classification task, 80.4% of\nLLM-assigned attributes matched expert choices, with 8.3% near misses and 11.3%\nmismatches. A comparative analysis of eight LLMs highlighted variations in\nperformance, with gemini-1.5-pro exhibiting the highest attribute accuracy,\nwhile llama-3.3-70B achieved higher validity and applicability scores. These\nfindings provide insights into the feasibility of using LLMs for automated NFR\ngeneration and lay the foundation for further exploration of AI-assisted\nrequirements engineering.",
      "authors": [
        "Jomar Thomas Almonte",
        "Santhosh Anitha Boominathan",
        "Nathalia Nascimento"
      ],
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "links": [
        "http://arxiv.org/abs/2503.15248v1",
        "http://arxiv.org/pdf/2503.15248v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2503.15234v1",
      "title": "CoE: Chain-of-Explanation via Automatic Visual Concept Circuit\n  Description and Polysemanticity Quantification",
      "published": "2025-03-19T14:13:02Z",
      "updated": "2025-03-19T14:13:02Z",
      "summary": "Explainability is a critical factor influencing the wide deployment of deep\nvision models (DVMs). Concept-based post-hoc explanation methods can provide\nboth global and local insights into model decisions. However, current methods\nin this field face challenges in that they are inflexible to automatically\nconstruct accurate and sufficient linguistic explanations for global concepts\nand local circuits. Particularly, the intrinsic polysemanticity in semantic\nVisual Concepts (VCs) impedes the interpretability of concepts and DVMs, which\nis underestimated severely. In this paper, we propose a Chain-of-Explanation\n(CoE) approach to address these issues. Specifically, CoE automates the\ndecoding and description of VCs to construct global concept explanation\ndatasets. Further, to alleviate the effect of polysemanticity on model\nexplainability, we design a concept polysemanticity disentanglement and\nfiltering mechanism to distinguish the most contextually relevant concept\natoms. Besides, a Concept Polysemanticity Entropy (CPE), as a measure of model\ninterpretability, is formulated to quantify the degree of concept uncertainty.\nThe modeling of deterministic concepts is upgraded to uncertain concept atom\ndistributions. Finally, CoE automatically enables linguistic local explanations\nof the decision-making process of DVMs by tracing the concept circuit. GPT-4o\nand human-based experiments demonstrate the effectiveness of CPE and the\nsuperiority of CoE, achieving an average absolute improvement of 36% in terms\nof explainability scores.",
      "authors": [
        "Wenlong Yu",
        "Qilong Wang",
        "Chuang Liu",
        "Dong Li",
        "Qinghua Hu"
      ],
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "links": [
        "http://arxiv.org/abs/2503.15234v1",
        "http://arxiv.org/pdf/2503.15234v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2503.15220v1",
      "title": "Entity-aware Cross-lingual Claim Detection for Automated Fact-checking",
      "published": "2025-03-19T14:00:55Z",
      "updated": "2025-03-19T14:00:55Z",
      "summary": "Identifying claims requiring verification is a critical task in automated\nfact-checking, especially given the proliferation of misinformation on social\nmedia platforms. Despite significant progress in the task, there remain open\nchallenges such as dealing with multilingual and multimodal data prevalent in\nonline discourse. Addressing the multilingual challenge, recent efforts have\nfocused on fine-tuning pre-trained multilingual language models. While these\nmodels can handle multiple languages, their ability to effectively transfer\ncross-lingual knowledge for detecting claims spreading on social media remains\nunder-explored. In this paper, we introduce \\textit{EX-Claim}, an entity-aware\ncross-lingual claim detection model that generalizes well to handle claims\nwritten in any language. The model leverages entity information derived from\nnamed entity recognition and entity linking techniques to improve the\nlanguage-level performance of both seen and unseen languages during training.\nExtensive experiments conducted on three datasets from different social media\nplatforms demonstrate that our proposed model significantly outperforms the\nbaselines, across 27 languages, and achieves the highest rate of knowledge\ntransfer, even with limited training data.",
      "authors": [
        "Rrubaa Panchendrarajan",
        "Arkaitz Zubiaga"
      ],
      "categories": [
        "cs.CL"
      ],
      "links": [
        "http://arxiv.org/abs/2503.15220v1",
        "http://arxiv.org/pdf/2503.15220v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2503.15176v1",
      "title": "A Review on Large Language Models for Visual Analytics",
      "published": "2025-03-19T13:02:01Z",
      "updated": "2025-03-19T13:02:01Z",
      "summary": "This paper provides a comprehensive review of the integration of Large\nLanguage Models (LLMs) with visual analytics, addressing their foundational\nconcepts, capabilities, and wide-ranging applications. It begins by outlining\nthe theoretical underpinnings of visual analytics and the transformative\npotential of LLMs, specifically focusing on their roles in natural language\nunderstanding, natural language generation, dialogue systems, and text-to-media\ntransformations. The review further investigates how the synergy between LLMs\nand visual analytics enhances data interpretation, visualization techniques,\nand interactive exploration capabilities. Key tools and platforms including\nLIDA, Chat2VIS, Julius AI, and Zoho Analytics, along with specialized\nmultimodal models such as ChartLlama and CharXIV, are critically evaluated. The\npaper discusses their functionalities, strengths, and limitations in supporting\ndata exploration, visualization enhancement, automated reporting, and insight\nextraction. The taxonomy of LLM tasks, ranging from natural language\nunderstanding (NLU), natural language generation (NLG), to dialogue systems and\ntext-to-media transformations, is systematically explored. This review provides\na SWOT analysis of integrating Large Language Models (LLMs) with visual\nanalytics, highlighting strengths like accessibility and flexibility,\nweaknesses such as computational demands and biases, opportunities in\nmultimodal integration and user collaboration, and threats including privacy\nconcerns and skill degradation. It emphasizes addressing ethical considerations\nand methodological improvements for effective integration.",
      "authors": [
        "Navya Sonal Agarwal",
        "Sanjay Kumar Sonbhadra"
      ],
      "categories": [
        "cs.HC",
        "cs.CL",
        "cs.CV"
      ],
      "links": [
        "http://arxiv.org/abs/2503.15176v1",
        "http://arxiv.org/pdf/2503.15176v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2503.15138v1",
      "title": "VideoGen-of-Thought: Step-by-step generating multi-shot video with\n  minimal manual intervention",
      "published": "2025-03-19T11:59:14Z",
      "updated": "2025-03-19T11:59:14Z",
      "summary": "Current video generation models excel at short clips but fail to produce\ncohesive multi-shot narratives due to disjointed visual dynamics and fractured\nstorylines. Existing solutions either rely on extensive manual\nscripting/editing or prioritize single-shot fidelity over cross-scene\ncontinuity, limiting their practicality for movie-like content. We introduce\nVideoGen-of-Thought (VGoT), a step-by-step framework that automates multi-shot\nvideo synthesis from a single sentence by systematically addressing three core\nchallenges: (1) Narrative Fragmentation: Existing methods lack structured\nstorytelling. We propose dynamic storyline modeling, which first converts the\nuser prompt into concise shot descriptions, then elaborates them into detailed,\ncinematic specifications across five domains (character dynamics, background\ncontinuity, relationship evolution, camera movements, HDR lighting), ensuring\nlogical narrative progression with self-validation. (2) Visual Inconsistency:\nExisting approaches struggle with maintaining visual consistency across shots.\nOur identity-aware cross-shot propagation generates identity-preserving\nportrait (IPP) tokens that maintain character fidelity while allowing trait\nvariations (expressions, aging) dictated by the storyline. (3) Transition\nArtifacts: Abrupt shot changes disrupt immersion. Our adjacent latent\ntransition mechanisms implement boundary-aware reset strategies that process\nadjacent shots' features at transition points, enabling seamless visual flow\nwhile preserving narrative continuity. VGoT generates multi-shot videos that\noutperform state-of-the-art baselines by 20.4% in within-shot face consistency\nand 17.4% in style consistency, while achieving over 100% better cross-shot\nconsistency and 10x fewer manual adjustments than alternatives.",
      "authors": [
        "Mingzhe Zheng",
        "Yongqi Xu",
        "Haojian Huang",
        "Xuran Ma",
        "Yexin Liu",
        "Wenjie Shu",
        "Yatian Pang",
        "Feilong Tang",
        "Qifeng Chen",
        "Harry Yang",
        "Ser-Nam Lim"
      ],
      "categories": [
        "cs.CV"
      ],
      "links": [
        "http://arxiv.org/abs/2503.15138v1",
        "http://arxiv.org/pdf/2503.15138v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2503.15124v1",
      "title": "Evaluating ASR Confidence Scores for Automated Error Detection in\n  User-Assisted Correction Interfaces",
      "published": "2025-03-19T11:33:40Z",
      "updated": "2025-03-19T11:33:40Z",
      "summary": "Despite advances in Automatic Speech Recognition (ASR), transcription errors\npersist and require manual correction. Confidence scores, which indicate the\ncertainty of ASR results, could assist users in identifying and correcting\nerrors. This study evaluates the reliability of confidence scores for error\ndetection through a comprehensive analysis of end-to-end ASR models and a user\nstudy with 36 participants. The results show that while confidence scores\ncorrelate with transcription accuracy, their error detection performance is\nlimited. Classifiers frequently miss errors or generate many false positives,\nundermining their practical utility. Confidence-based error detection neither\nimproved correction efficiency nor was perceived as helpful by participants.\nThese findings highlight the limitations of confidence scores and the need for\nmore sophisticated approaches to improve user interaction and explainability of\nASR results.",
      "authors": [
        "Korbinian Kuhn",
        "Verena Kersken",
        "Gottfried Zimmermann"
      ],
      "categories": [
        "cs.HC",
        "cs.CL",
        "cs.SD",
        "eess.AS",
        "I.2.7"
      ],
      "links": [
        "http://dx.doi.org/10.1145/3706599.3720038",
        "http://arxiv.org/abs/2503.15124v1",
        "http://arxiv.org/pdf/2503.15124v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2503.15112v1",
      "title": "OpenLLM-RTL: Open Dataset and Benchmark for LLM-Aided Design RTL\n  Generation",
      "published": "2025-03-19T11:12:53Z",
      "updated": "2025-03-19T11:12:53Z",
      "summary": "The automated generation of design RTL based on large language model (LLM)\nand natural language instructions has demonstrated great potential in agile\ncircuit design. However, the lack of datasets and benchmarks in the public\ndomain prevents the development and fair evaluation of LLM solutions. This\npaper highlights our latest advances in open datasets and benchmarks from three\nperspectives: (1) RTLLM 2.0, an updated benchmark assessing LLM's capability in\ndesign RTL generation. The benchmark is augmented to 50 hand-crafted designs.\nEach design provides the design description, test cases, and a correct RTL\ncode. (2) AssertEval, an open-source benchmark assessing the LLM's assertion\ngeneration capabilities for RTL verification. The benchmark includes 18\ndesigns, each providing specification, signal definition, and correct RTL code.\n(3) RTLCoder-Data, an extended open-source dataset with 80K instruction-code\ndata samples. Moreover, we propose a new verification-based method to verify\nthe functionality correctness of training data samples. Based on this\ntechnique, we further release a dataset with 7K verified high-quality samples.\nThese three studies are integrated into one framework, providing off-the-shelf\nsupport for the development and evaluation of LLMs for RTL code generation and\nverification. Finally, extensive experiments indicate that LLM performance can\nbe boosted by enlarging the training dataset, improving data quality, and\nimproving the training scheme.",
      "authors": [
        "Shang Liu",
        "Yao Lu",
        "Wenji Fang",
        "Mengming Li",
        "Zhiyao Xie"
      ],
      "categories": [
        "cs.AR"
      ],
      "links": [
        "http://arxiv.org/abs/2503.15112v1",
        "http://arxiv.org/pdf/2503.15112v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2503.15095v1",
      "title": "Diffusion-Based Forecasting for Uncertainty-Aware Model Predictive\n  Control",
      "published": "2025-03-19T10:48:26Z",
      "updated": "2025-03-19T10:48:26Z",
      "summary": "We propose Diffusion-Informed Model Predictive Control (D-I MPC), a generic\nframework for uncertainty-aware prediction and decision-making in partially\nobservable stochastic systems by integrating diffusion-based time series\nforecasting models in Model Predictive Control algorithms. In our approach, a\ndiffusion-based time series forecasting model is used to probabilistically\nestimate the evolution of the system's stochastic components. These forecasts\nare then incorporated into MPC algorithms to estimate future trajectories and\noptimize action selection under the uncertainty of the future. We evaluate the\nframework on the task of energy arbitrage, where a Battery Energy Storage\nSystem participates in the day-ahead electricity market of the New York state.\nExperimental results indicate that our model-based approach with a\ndiffusion-based forecaster significantly outperforms both implementations with\nclassical forecasting methods and model-free reinforcement learning baselines.",
      "authors": [
        "Stelios Zarifis",
        "Ioannis Kordonis",
        "Petros Maragos"
      ],
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.SY",
        "eess.SY",
        "I.2.6; I.5.1"
      ],
      "links": [
        "http://arxiv.org/abs/2503.15095v1",
        "http://arxiv.org/pdf/2503.15095v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2503.15079v1",
      "title": "LogiAgent: Automated Logical Testing for REST Systems with LLM-Based\n  Multi-Agents",
      "published": "2025-03-19T10:24:16Z",
      "updated": "2025-03-19T10:24:16Z",
      "summary": "Automated testing for REST APIs has become essential for ensuring the\ncorrectness and reliability of modern web services. While existing approaches\nprimarily focus on detecting server crashes and error codes, they often\noverlook logical issues that arise due to evolving business logic and\ndomain-specific requirements. To address this limitation, we propose LogiAgent,\na novel approach for logical testing of REST systems. Built upon a large\nlanguage model (LLM)-driven multi-agent framework, LogiAgent integrates a Test\nScenario Generator, API Request Executor, and API Response Validator to\ncollaboratively generate, execute, and validate API test scenarios. Unlike\ntraditional testing methods that focus on status codes like 5xx, LogiAgent\nincorporates logical oracles that assess responses based on business logic,\nensuring more comprehensive testing. The system is further enhanced by an\nExecution Memory component that stores historical API execution data for\ncontextual consistency. We conduct extensive experiments across 12 real-world\nREST systems, demonstrating that LogiAgent effectively identifies 234 logical\nissues with an accuracy of 66.19%. Additionally, it basically excels in\ndetecting server crashes and achieves superior test coverage compared to four\nstate-of-the-art REST API testing tools. An ablation study confirms the\nsignificant contribution of LogiAgent's memory components to improving test\ncoverage.",
      "authors": [
        "Ke Zhang",
        "Chenxi Zhang",
        "Chong Wang",
        "Chi Zhang",
        "YaChen Wu",
        "Zhenchang Xing",
        "Yang Liu",
        "Qingshan Li",
        "Xin Peng"
      ],
      "categories": [
        "cs.SE"
      ],
      "links": [
        "http://arxiv.org/abs/2503.15079v1",
        "http://arxiv.org/pdf/2503.15079v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2503.15050v1",
      "title": "Studying and Understanding the Effectiveness and Failures of\n  Conversational LLM-Based Repair",
      "published": "2025-03-19T09:39:32Z",
      "updated": "2025-03-19T09:39:32Z",
      "summary": "Automated program repair (APR) is designed to automate the process of\nbug-fixing. In recent years, thanks to the rapid development of large language\nmodels (LLMs), automated repair has achieved remarkable progress. Advanced APR\ntechniques powered by conversational LLMs, most notably ChatGPT, have exhibited\nimpressive repair abilities and gained increasing popularity due to the\ncapabilities of the underlying LLMs in providing repair feedback and performing\niterative patch improvement. Despite the superiority, conversational APR\ntechniques still fail to repair a large number of bugs. For example, a\nstate-of-the-art conversational technique ChatRepair does not correctly repair\nover half of the single-function bugs in the Defects4J dataset. To understand\nthe effectiveness and failures of conversational LLM-based repair and provide\npossible directions for improvement, we studied the exemplary ChatRepair with a\nfocus on comparing the effectiveness of its cloze-style and full function\nrepair strategies, assessing its key iterative component for patch improvement,\nand analyzing the repair failures. Our study has led to a series of findings,\nwhich we believe provide key implications for future research.",
      "authors": [
        "Aolin Chen",
        "Haojun Wu",
        "Qi Xin",
        "Steven P. Reiss",
        "Jifeng Xuan"
      ],
      "categories": [
        "cs.SE"
      ],
      "links": [
        "http://arxiv.org/abs/2503.15050v1",
        "http://arxiv.org/pdf/2503.15050v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2503.15044v1",
      "title": "SPADE: Systematic Prompt Framework for Automated Dialogue Expansion in\n  Machine-Generated Text Detection",
      "published": "2025-03-19T09:32:52Z",
      "updated": "2025-03-19T09:32:52Z",
      "summary": "The increasing capability of large language models (LLMs) to generate\nsynthetic content has heightened concerns about their misuse, driving the\ndevelopment of Machine-Generated Text (MGT) detection models. However, these\ndetectors face significant challenges due to the lack of systematically\ngenerated, high-quality datasets for training. To address this issue, we\npropose five novel data augmentation frameworks for synthetic user dialogue\ngeneration through a structured prompting approach, reducing the costs\nassociated with traditional data collection methods. Our proposed method yields\n14 new dialogue datasets, which we benchmark against seven MGT detection\nmodels. The results demonstrate improved generalization performance when\nutilizing a mixed dataset produced by our proposed augmentation framework.\nFurthermore, considering that real-world agents lack knowledge of future\nopponent utterances, we simulate online dialogue detection and examine the\nrelationship between chat history length and detection accuracy. We also\nbenchmark online detection performance with limited chat history on our\nframeworks. Our open-source datasets can be downloaded from\nhttps://github.com/AngieYYF/SPADE-customer-service-dialogue.",
      "authors": [
        "Haoyi Li",
        "Angela Yifei Yuan",
        "Soyeon Caren Han",
        "Christopher Leckie"
      ],
      "categories": [
        "cs.CL"
      ],
      "links": [
        "http://arxiv.org/abs/2503.15044v1",
        "http://arxiv.org/pdf/2503.15044v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2503.15003v1",
      "title": "LLM Alignment for the Arabs: A Homogenous Culture or Diverse Ones?",
      "published": "2025-03-19T08:52:59Z",
      "updated": "2025-03-19T08:52:59Z",
      "summary": "Large language models (LLMs) have the potential of being useful tools that\ncan automate tasks and assist humans. However, these models are more fluent in\nEnglish and more aligned with Western cultures, norms, and values.\nArabic-specific LLMs are being developed to better capture the nuances of the\nArabic language, as well as the views of the Arabs. Yet, Arabs are sometimes\nassumed to share the same culture. In this position paper, I discuss the\nlimitations of this assumption and provide preliminary thoughts for how to\nbuild systems that can better represent the cultural diversity within the Arab\nworld. The invalidity of the cultural homogeneity assumption might seem\nobvious, yet, it is widely adopted in developing multilingual and\nArabic-specific LLMs. I hope that this paper will encourage the NLP community\nto be considerate of the cultural diversity within various communities speaking\nthe same language.",
      "authors": [
        "Amr Keleg"
      ],
      "categories": [
        "cs.CL"
      ],
      "links": [
        "http://arxiv.org/abs/2503.15003v1",
        "http://arxiv.org/pdf/2503.15003v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2503.14966v1",
      "title": "Ultrasound Image-to-Video Synthesis via Latent Dynamic Diffusion Models",
      "published": "2025-03-19T07:58:43Z",
      "updated": "2025-03-19T07:58:43Z",
      "summary": "Ultrasound video classification enables automated diagnosis and has emerged\nas an important research area. However, publicly available ultrasound video\ndatasets remain scarce, hindering progress in developing effective video\nclassification models. We propose addressing this shortage by synthesizing\nplausible ultrasound videos from readily available, abundant ultrasound images.\nTo this end, we introduce a latent dynamic diffusion model (LDDM) to\nefficiently translate static images to dynamic sequences with realistic video\ncharacteristics. We demonstrate strong quantitative results and visually\nappealing synthesized videos on the BUSV benchmark. Notably, training video\nclassification models on combinations of real and LDDM-synthesized videos\nsubstantially improves performance over using real data alone, indicating our\nmethod successfully emulates dynamics critical for discrimination. Our\nimage-to-video approach provides an effective data augmentation solution to\nadvance ultrasound video analysis. Code is available at\nhttps://github.com/MedAITech/U_I2V.",
      "authors": [
        "Tingxiu Chen",
        "Yilei Shi",
        "Zixuan Zheng",
        "Bingcong Yan",
        "Jingliang Hu",
        "Xiao Xiang Zhu",
        "Lichao Mou"
      ],
      "categories": [
        "cs.CV",
        "eess.IV"
      ],
      "links": [
        "http://arxiv.org/abs/2503.14966v1",
        "http://arxiv.org/pdf/2503.14966v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2503.14941v1",
      "title": "UPME: An Unsupervised Peer Review Framework for Multimodal Large\n  Language Model Evaluation",
      "published": "2025-03-19T07:15:41Z",
      "updated": "2025-03-19T07:15:41Z",
      "summary": "Multimodal Large Language Models (MLLMs) have emerged to tackle the\nchallenges of Visual Question Answering (VQA), sparking a new research focus on\nconducting objective evaluations of these models. Existing evaluation methods\nface limitations due to the significant human workload required to design Q&A\npairs for visual images, which inherently restricts the scale and scope of\nevaluations. Although automated MLLM-as-judge approaches attempt to reduce the\nhuman workload through automatic evaluations, they often introduce biases. To\naddress these problems, we propose an Unsupervised Peer review MLLM Evaluation\nframework. It utilizes only image data, allowing models to automatically\ngenerate questions and conduct peer review assessments of answers from other\nmodels, effectively alleviating the reliance on human workload. Additionally,\nwe introduce the vision-language scoring system to mitigate the bias issues,\nwhich focuses on three aspects: (i) response correctness; (ii) visual\nunderstanding and reasoning; and (iii) image-text correlation. Experimental\nresults demonstrate that UPME achieves a Pearson correlation of 0.944 with\nhuman evaluations on the MMstar dataset and 0.814 on the ScienceQA dataset,\nindicating that our framework closely aligns with human-designed benchmarks and\ninherent human preferences.",
      "authors": [
        "Qihui Zhang",
        "Munan Ning",
        "Zheyuan Liu",
        "Yanbo Wang",
        "Jiayi Ye",
        "Yue Huang",
        "Shuo Yang",
        "Xiao Chen",
        "Yibing Song",
        "Li Yuan"
      ],
      "categories": [
        "cs.CV"
      ],
      "links": [
        "http://arxiv.org/abs/2503.14941v1",
        "http://arxiv.org/pdf/2503.14941v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2503.14933v1",
      "title": "A Language Vision Model Approach for Automated Tumor Contouring in\n  Radiation Oncology",
      "published": "2025-03-19T06:41:37Z",
      "updated": "2025-03-19T06:41:37Z",
      "summary": "Background: Lung cancer ranks as the leading cause of cancer-related\nmortality worldwide. The complexity of tumor delineation, crucial for radiation\ntherapy, requires expertise often unavailable in resource-limited settings.\nArtificial Intelligence(AI), particularly with advancements in deep learning\n(DL) and natural language processing (NLP), offers potential solutions yet is\nchallenged by high false positive rates. Purpose: The Oncology Contouring\nCopilot (OCC) system is developed to leverage oncologist expertise for precise\ntumor contouring using textual descriptions, aiming to increase the efficiency\nof oncological workflows by combining the strengths of AI with human oversight.\nMethods: Our OCC system initially identifies nodule candidates from CT scans.\nEmploying Language Vision Models (LVMs) like GPT-4V, OCC then effectively\nreduces false positives with clinical descriptive texts, merging textual and\nvisual data to automate tumor delineation, designed to elevate the quality of\noncology care by incorporating knowledge from experienced domain experts.\nResults: Deployments of the OCC system resulted in a significant reduction in\nthe false discovery rate by 35.0%, a 72.4% decrease in false positives per\nscan, and an F1-score of 0.652 across our dataset for unbiased evaluation.\nConclusions: OCC represents a significant advance in oncology care,\nparticularly through the use of the latest LVMs to improve contouring results\nby (1) streamlining oncology treatment workflows by optimizing tumor\ndelineation, reducing manual processes; (2) offering a scalable and intuitive\nframework to reduce false positives in radiotherapy planning using LVMs; (3)\nintroducing novel medical language vision prompt techniques to minimize LVMs\nhallucinations with ablation study, and (4) conducting a comparative analysis\nof LVMs, highlighting their potential in addressing medical language vision\nchallenges.",
      "authors": [
        "Yi Luo",
        "Hamed Hooshangnejad",
        "Xue Feng",
        "Gaofeng Huang",
        "Xiaojian Chen",
        "Rui Zhang",
        "Quan Chen",
        "Wil Ngwa",
        "Kai Ding"
      ],
      "categories": [
        "eess.IV",
        "cs.CV",
        "physics.med-ph"
      ],
      "links": [
        "http://arxiv.org/abs/2503.14933v1",
        "http://arxiv.org/pdf/2503.14933v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2503.14914v1",
      "title": "Inverse Problems for Mean Field Games",
      "published": "2025-03-19T05:36:45Z",
      "updated": "2025-03-19T05:36:45Z",
      "summary": "In this book, we present a curated collection of existing results on inverse\nproblems for Mean Field Games (MFGs), a cutting-edge and rapidly evolving field\nof research. Our aim is to provide fresh insights, novel perspectives, and a\ncomprehensive foundation for future investigations into this fascinating area.\nMFGs, a class of differential games involving a continuum of non-atomic\nplayers, offer a powerful framework for analyzing the collective behavior of\nlarge populations of symmetric agents as the number of agents approaches\ninfinity. This framework has proven to be an invaluable tool for quantitatively\nmodeling the macroscopic dynamics of agents striving to minimize specific costs\nin complex systems, such as crowd dynamics, financial markets, traffic flows,\nand social networks.\n  The study of MFGs has traditionally focused on forward problems, where the\ngoal is to determine the equilibrium behavior of agents given a set of model\nparameters, such as cost functions, interaction mechanisms, and initial\nconditions. However, the inverse problems for MFGs -- which seek to infer these\nunderlying parameters from observed data -- have received comparatively less\nattention in the literature. This book seeks to address this gap by delving\ninto the fundamental aspects of MFG inverse problems, with a particular\nemphasis on issues of unique identifiability, stability, and reconstruction of\nunknown parameters. These problems are not only mathematically challenging but\nalso of immense practical significance, as they enable the calibration and\nvalidation of MFG models using real-world data.\n  This book is intended to serve as a valuable resource for researchers\ninterested in the theory and applications of MFGs, particularly in inverse\nproblems. Through this book, we hope to inspire further exploration and\ninnovation in this dynamic and interdisciplinary area of study.",
      "authors": [
        "Hongyu Liu",
        "Catharine W. K. Lo",
        "Shen Zhang"
      ],
      "categories": [
        "math.AP",
        "math.OC"
      ],
      "links": [
        "http://arxiv.org/abs/2503.14914v1",
        "http://arxiv.org/pdf/2503.14914v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2503.14899v1",
      "title": "Speed Optimization Algorithm based on Deterministic Markov Decision\n  Process for Automated Highway Merge",
      "published": "2025-03-19T04:57:03Z",
      "updated": "2025-03-19T04:57:03Z",
      "summary": "This study presents a robust optimization algorithm for automated highway\nmerge. The merging scenario is one of the challenging scenes in automated\ndriving, because it requires adjusting ego vehicle's speed to match other\nvehicles before reaching the end point. Then, we model the speed planning\nproblem as a deterministic Markov decision process. The proposed scheme is able\nto compute each state value of the process and reliably derive the optimal\nsequence of actions. In our approach, we adopt jerk as the action of the\nprocess to prevent a sudden change of acceleration. However, since this expands\nthe state space, we also consider ways to achieve a real-time operation. We\ncompared our scheme with a simple algorithm with the Intelligent Driver Model.\nWe not only evaluated the scheme in a simulation environment but also conduct a\nreal world testing.",
      "authors": [
        "Takeru Goto",
        "Kosuke Toda",
        "Takayasu Kumano"
      ],
      "categories": [
        "cs.RO"
      ],
      "links": [
        "http://arxiv.org/abs/2503.14899v1",
        "http://arxiv.org/pdf/2503.14899v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2503.14852v1",
      "title": "UntrustVul: An Automated Approach for Identifying Untrustworthy Alerts\n  in Vulnerability Detection Models",
      "published": "2025-03-19T03:18:45Z",
      "updated": "2025-03-19T03:18:45Z",
      "summary": "Machine learning (ML) has shown promise in detecting vulnerabilities. To\nreview vulnerabilities detected by ML predictions, developers manually assess\nsuspicious lines in their interpretations. However, studies have revealed that\nthese models often learn and predict based on irrelevant features frequently\nappearing in vulnerable code. This leads to predictions that may correctly flag\nvulnerable functions but for the wrong reasons, which we call untrustworthy.\nThese predictions can mislead developers, hindering them from locating the\nvulnerabilities. This increases the efforts of manual assessment and, worse,\nrisks creating flawed patches that fail to address existing vulnerabilities and\neven introduce new ones. Hence, automated approaches are needed to detect\nuntrustworthy predictions, preventing overlooked vulnerabilities and\nalleviating the burden of manual assessment.\n  We propose UntrustVul, the first automated approach to identify untrustworthy\nvulnerability predictions. Given a vulnerability prediction during inference,\nUntrustVul systematically assesses whether suspicious lines annotated by the\nprediction are vulnerability-unrelated. It simulates developers' rationales,\nconsidering a line unrelated if (1) it is absent from historical\nvulnerabilities and (2) it cannot reach any vulnerabilities in execution flows.\nUntrustVul assesses (1) by analysing its syntactic meaning using deep\nrepresentations to determine whether it is syntax-benign. To assess (2),\nUntrustVul traces dependencies of the syntax-benign lines on other suspicious\nlines using static and rule-based analyses. We evaluate UntrustVul on 155K\nvulnerability predictions by four models across three datasets. UntrustVul\neffectively detects untrustworthy predictions with an F1-score of 82%-94% and\nhelps improve the ability of models to detect vulnerabilities by up to 321% in\nF1-score and 100% in trustworthiness.",
      "authors": [
        "Lam Nguyen Tung",
        "Xiaoning Du",
        "Neelofar Neelofar",
        "Aldeida Aleti"
      ],
      "categories": [
        "cs.SE"
      ],
      "links": [
        "http://arxiv.org/abs/2503.14852v1",
        "http://arxiv.org/pdf/2503.14852v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2503.14838v1",
      "title": "Think Like Human Developers: Harnessing Community Knowledge for\n  Structured Code Reasoning",
      "published": "2025-03-19T02:45:13Z",
      "updated": "2025-03-19T02:45:13Z",
      "summary": "Large Language Models (LLMs) have significantly advanced automated code\ngeneration, yet they struggle with complex coding tasks requiring multi-step\nlogical reasoning. High-quality reasoning data is crucial for improving LLMs'\nreasoning capabilities, but such datasets remain scarce. Existing approaches\neither rely on computationally expensive reinforcement learning (RL) or\nerror-prone reasoning chains synthesized by LLMs, posing challenges in\nscalability and accuracy.\n  To address this challenge, we propose SVRC (Structured and Validated\nReasoning Chains for Code Generation), a novel framework that mines,\nrestructures, and enriches reasoning chains from community-driven discussions\non software engineering platforms. SVRC refines unstructured and incomplete\ndiscussions of coding problems by aligning them with Software Development Life\nCycle (SDLC) principles, ensuring that reasoning chains capture real-world\nproblem-solving strategies and support iterative refinement.\n  To evaluate the effectiveness of SVRC, we introduce CodeThinker, an LLM\nfine-tuned on 12,444 reasoning-augmented samples generated by SVRC. Experiments\non LiveCodeBench show that CodeThinker surpasses its base model by 42.86\\% on\nmedium-level code problems in terms of pass@1 and outperforms GPT-4o-mini and\nGPT-4o by 73.14\\% and 115.86\\%, respectively. Our ablation study further\nhighlights that each component of SVRC contributes to the reasoning\ncapabilities of CodeThinker.",
      "authors": [
        "Chengran Yang",
        "Zhensu Sun",
        "Hong Jin Kang",
        "Jieke Shi",
        "David Lo"
      ],
      "categories": [
        "cs.SE"
      ],
      "links": [
        "http://arxiv.org/abs/2503.14838v1",
        "http://arxiv.org/pdf/2503.14838v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2503.14829v1",
      "title": "Stochastic Volatility Model with Sticky Drawdown and Drawup Processes: A\n  Deep Learning Approach",
      "published": "2025-03-19T02:09:34Z",
      "updated": "2025-03-19T02:09:34Z",
      "summary": "We propose a new financial model, the stochastic volatility model with sticky\ndrawdown and drawup processes (SVSDU model), which enables us to capture the\nfeatures of winning and losing streaks that are common across financial markets\nbut can not be captured simultaneously by the existing financial models.\nMoreover, the SVSDU model retains the advantages of the stochastic volatility\nmodels. Since there are not closed-form option pricing formulas under the SVSDU\nmodel and the existing simulation methods for the sticky diffusion processes\nare really time-consuming, we develop a deep neural network to solve the\ncorresponding high-dimensional parametric partial differential equation (PDE),\nwhere the solution to the PDE is the pricing function of a European option\naccording to the Feynman-Kac Theorem, and validate the accuracy and efficiency\nof our deep learning approach. We also propose a novel calibration framework\nfor our model, and demonstrate the calibration performances of our models on\nboth simulated data and historical data. The calibration results on SPX option\ndata show that the SVSDU model is a good representation of the asset value\ndynamic, and both winning and losing streaks are accounted for in option\nvalues. Our model opens new horizons for modeling and predicting the dynamics\nof asset prices in financial markets.",
      "authors": [
        "Yuhao Liu",
        "Pingping Jiang",
        "Gongqiu Zhang"
      ],
      "categories": [
        "q-fin.MF",
        "q-fin.PR"
      ],
      "links": [
        "http://arxiv.org/abs/2503.14829v1",
        "http://arxiv.org/pdf/2503.14829v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2503.14797v1",
      "title": "FACTS&EVIDENCE: An Interactive Tool for Transparent Fine-Grained Factual\n  Verification of Machine-Generated Text",
      "published": "2025-03-19T00:14:55Z",
      "updated": "2025-03-19T00:14:55Z",
      "summary": "With the widespread consumption of AI-generated content, there has been an\nincreased focus on developing automated tools to verify the factual accuracy of\nsuch content. However, prior research and tools developed for fact verification\ntreat it as a binary classification or a linear regression problem. Although\nthis is a useful mechanism as part of automatic guardrails in systems, we argue\nthat such tools lack transparency in the prediction reasoning and diversity in\nsource evidence to provide a trustworthy user experience. We develop\nFacts&Evidence - an interactive and transparent tool for user-driven\nverification of complex text. The tool facilitates the intricate\ndecision-making involved in fact-verification, presenting its users a breakdown\nof complex input texts to visualize the credibility of individual claims along\nwith an explanation of model decisions and attribution to multiple, diverse\nevidence sources. Facts&Evidence aims to empower consumers of machine-generated\ntext and give them agency to understand, verify, selectively trust and use such\ntext.",
      "authors": [
        "Varich Boonsanong",
        "Vidhisha Balachandran",
        "Xiaochuang Han",
        "Shangbin Feng",
        "Lucy Lu Wang",
        "Yulia Tsvetkov"
      ],
      "categories": [
        "cs.CL"
      ],
      "links": [
        "http://arxiv.org/abs/2503.14797v1",
        "http://arxiv.org/pdf/2503.14797v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2503.14740v1",
      "title": "Construction of the Damped Ly$\u03b1$ Absorber Catalog for DESI DR2\n  Ly$\u03b1$ BAO",
      "published": "2025-03-18T21:14:14Z",
      "updated": "2025-03-18T21:14:14Z",
      "summary": "We present the Damped Ly$\\alpha$ Toolkit for automated detection and\ncharacterization of Damped Ly$\\alpha$ absorbers (DLA) in quasar spectra. Our\nmethod uses quasar spectral templates with and without absorption from\nintervening DLAs to reconstruct observed quasar forest regions. The\nbest-fitting model determines whether a DLA is present while estimating the\nredshift and HI column density. With an optimized quality cut on detection\nsignificance ($\\Delta \\chi_{r}^2>0.03$), the technique achieves an estimated\n72% purity and 71% completeness when evaluated on simulated spectra with\nS/N$>2$ that are free of broad absorption lines (BAL). We provide a catalog\ncontaining candidate DLAs from the DLA Toolkit detected in DESI DR1 quasar\nspectra, of which 21,719 were found in S/N$>2$ spectra with predicted\n$\\log_{10} (N_\\texttt{HI}) > 20.3$ and detection significance $\\Delta\n\\chi_{r}^2 >0.03$. We compare the Damped Ly$\\alpha$ Toolkit to two alternative\nDLA finders based on a convolutional neural network (CNN) and Gaussian process\n(GP) models. We present a strategy for combining these three techniques to\nproduce a high-fidelity DLA catalog from DESI DR2 for the Ly$\\alpha$ forest\nbaryon acoustic oscillation measurement. The combined catalog contains 41,152\ncandidate DLAs with $\\log_{10} (N_\\texttt{HI}) > 20.3$ from quasar spectra with\nS/N$>2$. We estimate this sample to be approximately 76% pure and 71% complete\nwhen BAL quasars are excluded.",
      "authors": [
        "A. Brodzeller",
        "M. Wolfson",
        "D. M. Santos",
        "M. Ho",
        "T. Tan",
        "M. M. Pieri",
        "A. Cuceu",
        "M. Abdul Karim",
        "J. Aguilar",
        "S. Ahlen",
        "A. Anand",
        "U. Andrade",
        "E. Armengaud",
        "A. Aviles",
        "S. Bailey",
        "A. Bault",
        "D. Bianchi",
        "D. Brooks",
        "R. Canning",
        "L. Casas",
        "M. Charles",
        "E. Chaussidon",
        "J. Chaves-Montero",
        "D. Chebat",
        "T. Claybaugh",
        "K. S. Dawson",
        "R. de Belsunce",
        "A. de la Macorra",
        "A. de Mattia",
        "Arjun Dey",
        "Biprateep Dey",
        "P. Doel",
        "W. Elbers",
        "S. Ferraro",
        "A. Font-Ribera",
        "J. E. Forero-Romero",
        "C. Garcia-Quintero",
        "L. H. Garrison",
        "E. Gazta\u00f1aga",
        "S. Gontcho A Gontcho",
        "A. X. Gonzalez-Morales",
        "D. Green",
        "G. Gutierrez",
        "J. Guy",
        "C. Hahn",
        "M. Herbold",
        "H. K. Herrera-Alcantar",
        "K. Honscheid",
        "C. Howlett",
        "D. Huterer",
        "M. Ishak",
        "S. Juneau",
        "R. Kehoe",
        "T. Kisner",
        "A. Kremin",
        "O. Lahav",
        "C. Lamman",
        "M. Landriau",
        "J. M. Le Goff",
        "L. Le Guillou",
        "A. Leauthaud",
        "M. E. Levi",
        "Q. Li",
        "M. Manera",
        "P. Martini",
        "A. Meisner",
        "J. Mena-Fernandez",
        "R. Miquel",
        "J. Moustakas",
        "A. Mu\u00f1oz-Guti\u00e9rrez",
        "A. D. Myers",
        "S. Nadathur",
        "L. Napolitano",
        "H. E. Noriega",
        "E. Paillas",
        "N. Palanque-Delabrouille",
        "W. J. Percival",
        "C. Poppett",
        "F. Prada",
        "I. P\u00e9rez-R\u00e0fols",
        "C. Ram\u00edrez-P\u00e9rez",
        "C. Ravoux",
        "J. Rohlf",
        "G. Rossi",
        "E. Sanchez",
        "D. Schlegel",
        "M. Schubnell",
        "F. Sinigaglia",
        "D. Sprayberry",
        "G. Tarl\u00e9",
        "P. Taylor",
        "W. Turner",
        "M. Walther",
        "B. A. Weaver",
        "C. Y\u00e8che",
        "R. Zhou",
        "H. Zou",
        "S. Zou"
      ],
      "categories": [
        "astro-ph.CO",
        "astro-ph.GA"
      ],
      "links": [
        "http://arxiv.org/abs/2503.14740v1",
        "http://arxiv.org/pdf/2503.14740v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2503.14716v1",
      "title": "Construction Site Scaffolding Completeness Detection Based on Mask R-CNN\n  and Hough Transform",
      "published": "2025-03-18T20:27:22Z",
      "updated": "2025-03-18T20:27:22Z",
      "summary": "Construction site scaffolding is essential for many building projects, and\nensuring its safety is crucial to prevent accidents. The safety inspector must\ncheck the scaffolding's completeness and integrity, where most violations\noccur. The inspection process includes ensuring all the components are in the\nright place since workers often compromise safety for convenience and\ndisassemble parts such as cross braces. This paper proposes a deep\nlearning-based approach to detect the scaffolding and its cross braces using\ncomputer vision. A scaffold image dataset with annotated labels is used to\ntrain a convolutional neural network (CNN) model. With the proposed approach,\nwe can automatically detect the completeness of cross braces from images taken\nat construction sites, without the need for manual inspection, saving a\nsignificant amount of time and labor costs. This non-invasive and efficient\nsolution for detecting scaffolding completeness can help improve safety in\nconstruction sites.",
      "authors": [
        "Pei-Hsin Lin",
        "Jacob J. Lin",
        "Shang-Hsien Hsieh"
      ],
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "links": [
        "http://arxiv.org/abs/2503.14716v1",
        "http://arxiv.org/pdf/2503.14716v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2503.14630v1",
      "title": "Assessing Large Language Models for Automated Feedback Generation in\n  Learning Programming Problem Solving",
      "published": "2025-03-18T18:31:36Z",
      "updated": "2025-03-18T18:31:36Z",
      "summary": "Providing effective feedback is important for student learning in programming\nproblem-solving. In this sense, Large Language Models (LLMs) have emerged as\npotential tools to automate feedback generation. However, their reliability and\nability to identify reasoning errors in student code remain not well\nunderstood. This study evaluates the performance of four LLMs (GPT-4o, GPT-4o\nmini, GPT-4-Turbo, and Gemini-1.5-pro) on a benchmark dataset of 45 student\nsolutions. We assessed the models' capacity to provide accurate and insightful\nfeedback, particularly in identifying reasoning mistakes. Our analysis reveals\nthat 63\\% of feedback hints were accurate and complete, while 37\\% contained\nmistakes, including incorrect line identification, flawed explanations, or\nhallucinated issues. These findings highlight the potential and limitations of\nLLMs in programming education and underscore the need for improvements to\nenhance reliability and minimize risks in educational applications.",
      "authors": [
        "Priscylla Silva",
        "Evandro Costa"
      ],
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.LG"
      ],
      "links": [
        "http://arxiv.org/abs/2503.14630v1",
        "http://arxiv.org/pdf/2503.14630v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2503.14499v1",
      "title": "Measuring AI Ability to Complete Long Tasks",
      "published": "2025-03-18T17:59:31Z",
      "updated": "2025-03-18T17:59:31Z",
      "summary": "Despite rapid progress on AI benchmarks, the real-world meaning of benchmark\nperformance remains unclear. To quantify the capabilities of AI systems in\nterms of human capabilities, we propose a new metric: 50%-task-completion time\nhorizon. This is the time humans typically take to complete tasks that AI\nmodels can complete with 50% success rate. We first timed humans with relevant\ndomain expertise on a combination of RE-Bench, HCAST, and 66 novel shorter\ntasks. On these tasks, current frontier AI models such as Claude 3.7 Sonnet\nhave a 50% time horizon of around 50 minutes. Furthermore, frontier AI time\nhorizon has been doubling approximately every seven months since 2019, though\nthe trend may have accelerated in 2024. The increase in AI models' time\nhorizons seems to be primarily driven by greater reliability and ability to\nadapt to mistakes, combined with better logical reasoning and tool use\ncapabilities. We discuss the limitations of our results -- including their\ndegree of external validity -- and the implications of increased autonomy for\ndangerous capabilities. If these results generalize to real-world software\ntasks, extrapolation of this trend predicts that within 5 years, AI systems\nwill be capable of automating many software tasks that currently take humans a\nmonth.",
      "authors": [
        "Thomas Kwa",
        "Ben West",
        "Joel Becker",
        "Amy Deng",
        "Katharyn Garcia",
        "Max Hasin",
        "Sami Jawhar",
        "Megan Kinniment",
        "Nate Rush",
        "Sydney Von Arx",
        "Ryan Bloom",
        "Thomas Broadley",
        "Haoxing Du",
        "Brian Goodrich",
        "Nikola Jurkovic",
        "Luke Harold Miles",
        "Seraphina Nix",
        "Tao Lin",
        "Neev Parikh",
        "David Rein",
        "Lucas Jun Koba Sato",
        "Hjalmar Wijk",
        "Daniel M. Ziegler",
        "Elizabeth Barnes",
        "Lawrence Chan"
      ],
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "links": [
        "http://arxiv.org/abs/2503.14499v1",
        "http://arxiv.org/pdf/2503.14499v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2503.14473v1",
      "title": "EnQode: Fast Amplitude Embedding for Quantum Machine Learning Using\n  Classical Data",
      "published": "2025-03-18T17:48:03Z",
      "updated": "2025-03-18T17:48:03Z",
      "summary": "Amplitude embedding (AE) is essential in quantum machine learning (QML) for\nencoding classical data onto quantum circuits. However, conventional AE methods\nsuffer from deep, variable-length circuits that introduce high output error due\nto extensive gate usage and variable error rates across samples, resulting in\nnoise-driven inconsistencies that degrade model accuracy. We introduce EnQode,\na fast AE technique based on symbolic representation that addresses these\nlimitations by clustering dataset samples and solving for cluster mean states\nthrough a low-depth, machine-specific ansatz. Optimized to reduce physical\ngates and SWAP operations, EnQode ensures all samples face consistent, low\nnoise levels by standardizing circuit depth and composition. With over 90%\nfidelity in data mapping, EnQode enables robust, high-performance QML on noisy\nintermediate-scale quantum (NISQ) devices. Our open-source solution provides a\nscalable and efficient alternative for integrating classical data with quantum\nmodels.",
      "authors": [
        "Jason Han",
        "Nicholas S. DiBrita",
        "Younghyun Cho",
        "Hengrui Luo",
        "Tirthak Patel"
      ],
      "categories": [
        "quant-ph",
        "cs.ET",
        "cs.LG"
      ],
      "links": [
        "http://arxiv.org/abs/2503.14473v1",
        "http://arxiv.org/pdf/2503.14473v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2503.14443v1",
      "title": "EnvBench: A Benchmark for Automated Environment Setup",
      "published": "2025-03-18T17:19:12Z",
      "updated": "2025-03-18T17:19:12Z",
      "summary": "Recent advances in Large Language Models (LLMs) have enabled researchers to\nfocus on practical repository-level tasks in software engineering domain. In\nthis work, we consider a cornerstone task for automating work with software\nrepositories-environment setup, i.e., a task of configuring a\nrepository-specific development environment on a system. Existing studies on\nenvironment setup introduce innovative agentic strategies, but their evaluation\nis often based on small datasets that may not capture the full range of\nconfiguration challenges encountered in practice. To address this gap, we\nintroduce a comprehensive environment setup benchmark EnvBench. It encompasses\n329 Python and 665 JVM-based (Java, Kotlin) repositories, with a focus on\nrepositories that present genuine configuration challenges, excluding projects\nthat can be fully configured by simple deterministic scripts. To enable further\nbenchmark extension and usage for model tuning, we implement two automatic\nmetrics: a static analysis check for missing imports in Python and a\ncompilation check for JVM languages. We demonstrate the applicability of our\nbenchmark by evaluating three environment setup approaches, including a simple\nzero-shot baseline and two agentic workflows, that we test with two powerful\nLLM backbones, GPT-4o and GPT-4o-mini. The best approach manages to\nsuccessfully configure 6.69% repositories for Python and 29.47% repositories\nfor JVM, suggesting that EnvBench remains challenging for current approaches.\nOur benchmark suite is publicly available at\nhttps://github.com/JetBrains-Research/EnvBench. The dataset and experiment\ntrajectories are available at https://jb.gg/envbench.",
      "authors": [
        "Aleksandra Eliseeva",
        "Alexander Kovrigin",
        "Ilia Kholkin",
        "Egor Bogomolov",
        "Yaroslav Zharov"
      ],
      "categories": [
        "cs.LG",
        "cs.SE"
      ],
      "links": [
        "http://arxiv.org/abs/2503.14443v1",
        "http://arxiv.org/pdf/2503.14443v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2503.14434v1",
      "title": "LLM-FE: Automated Feature Engineering for Tabular Data with LLMs as\n  Evolutionary Optimizers",
      "published": "2025-03-18T17:11:24Z",
      "updated": "2025-03-18T17:11:24Z",
      "summary": "Automated feature engineering plays a critical role in improving predictive\nmodel performance for tabular learning tasks. Traditional automated feature\nengineering methods are limited by their reliance on pre-defined\ntransformations within fixed, manually designed search spaces, often neglecting\ndomain knowledge. Recent advances using Large Language Models (LLMs) have\nenabled the integration of domain knowledge into the feature engineering\nprocess. However, existing LLM-based approaches use direct prompting or rely\nsolely on validation scores for feature selection, failing to leverage insights\nfrom prior feature discovery experiments or establish meaningful reasoning\nbetween feature generation and data-driven performance. To address these\nchallenges, we propose LLM-FE, a novel framework that combines evolutionary\nsearch with the domain knowledge and reasoning capabilities of LLMs to\nautomatically discover effective features for tabular learning tasks. LLM-FE\nformulates feature engineering as a program search problem, where LLMs propose\nnew feature transformation programs iteratively, and data-driven feedback\nguides the search process. Our results demonstrate that LLM-FE consistently\noutperforms state-of-the-art baselines, significantly enhancing the performance\nof tabular prediction models across diverse classification and regression\nbenchmarks.",
      "authors": [
        "Nikhil Abhyankar",
        "Parshin Shojaee",
        "Chandan K. Reddy"
      ],
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.NE"
      ],
      "links": [
        "http://arxiv.org/abs/2503.14434v1",
        "http://arxiv.org/pdf/2503.14434v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2503.14432v1",
      "title": "PLAY2PROMPT: Zero-shot Tool Instruction Optimization for LLM Agents via\n  Tool Play",
      "published": "2025-03-18T17:09:57Z",
      "updated": "2025-03-18T17:09:57Z",
      "summary": "Large language models (LLMs) are increasingly integrated with specialized\nexternal tools, yet many tasks demand zero-shot tool usage with minimal or\nnoisy documentation. Existing solutions rely on manual rewriting or labeled\ndata for validation, making them inapplicable in true zero-shot settings. To\naddress these challenges, we propose PLAY2PROMPT, an automated framework that\nsystematically \"plays\" with each tool to explore its input-output behaviors.\nThrough this iterative trial-and-error process, PLAY2PROMPT refines tool\ndocumentation and generates usage examples without any labeled data. These\nexamples not only guide LLM inference but also serve as validation to further\nenhance tool utilization. Extensive experiments on real-world tasks demonstrate\nthat PLAY2PROMPT significantly improves zero-shot tool performance across both\nopen and closed models, offering a scalable and effective solution for\ndomain-specific tool integration.",
      "authors": [
        "Wei Fang",
        "Yang Zhang",
        "Kaizhi Qian",
        "James Glass",
        "Yada Zhu"
      ],
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "links": [
        "http://arxiv.org/abs/2503.14432v1",
        "http://arxiv.org/pdf/2503.14432v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2503.14408v1",
      "title": "Large Language Models for Virtual Human Gesture Selection",
      "published": "2025-03-18T16:49:56Z",
      "updated": "2025-03-18T16:49:56Z",
      "summary": "Co-speech gestures convey a wide variety of meanings and play an important\nrole in face-to-face human interactions. These gestures significantly influence\nthe addressee's engagement, recall, comprehension, and attitudes toward the\nspeaker. Similarly, they impact interactions between humans and embodied\nvirtual agents. The process of selecting and animating meaningful gestures has\nthus become a key focus in the design of these agents. However, automating this\ngesture selection process poses a significant challenge. Prior gesture\ngeneration techniques have varied from fully automated, data-driven methods,\nwhich often struggle to produce contextually meaningful gestures, to more\nmanual approaches that require crafting specific gesture expertise and are\ntime-consuming and lack generalizability. In this paper, we leverage the\nsemantic capabilities of Large Language Models to develop a gesture selection\napproach that suggests meaningful, appropriate co-speech gestures. We first\ndescribe how information on gestures is encoded into GPT-4. Then, we conduct a\nstudy to evaluate alternative prompting approaches for their ability to select\nmeaningful, contextually relevant gestures and to align them appropriately with\nthe co-speech utterance. Finally, we detail and demonstrate how this approach\nhas been implemented within a virtual agent system, automating the selection\nand subsequent animation of the selected gestures for enhanced human-agent\ninteractions.",
      "authors": [
        "Parisa Ghanad Torshizi",
        "Laura B. Hensel",
        "Ari Shapiro",
        "Stacy C. Marsella"
      ],
      "categories": [
        "cs.HC",
        "cs.CL"
      ],
      "links": [
        "http://arxiv.org/abs/2503.14408v1",
        "http://arxiv.org/pdf/2503.14408v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2503.14340v1",
      "title": "MANTRA: Enhancing Automated Method-Level Refactoring with Contextual RAG\n  and Multi-Agent LLM Collaboration",
      "published": "2025-03-18T15:16:51Z",
      "updated": "2025-03-18T15:16:51Z",
      "summary": "Maintaining and scaling software systems relies heavily on effective code\nrefactoring, yet this process remains labor-intensive, requiring developers to\ncarefully analyze existing codebases and prevent the introduction of new\ndefects. Although recent advancements have leveraged Large Language Models\n(LLMs) to automate refactoring tasks, current solutions are constrained in\nscope and lack mechanisms to guarantee code compilability and successful test\nexecution. In this work, we introduce MANTRA, a comprehensive LLM agent-based\nframework that automates method-level refactoring. MANTRA integrates\nContext-Aware Retrieval-Augmented Generation, coordinated Multi-Agent\nCollaboration, and Verbal Reinforcement Learning to emulate human\ndecision-making during refactoring while preserving code correctness and\nreadability. Our empirical study, conducted on 703 instances of \"pure\nrefactorings\" (i.e., code changes exclusively involving structural\nimprovements), drawn from 10 representative Java projects, covers the six most\nprevalent refactoring operations. Experimental results demonstrate that MANTRA\nsubstantially surpasses a baseline LLM model (RawGPT ), achieving an 82.8%\nsuccess rate (582/703) in producing code that compiles and passes all tests,\ncompared to just 8.7% (61/703) with RawGPT. Moreover, in comparison to\nIntelliJ's LLM-powered refactoring tool (EM-Assist), MANTRA exhibits a 50%\nimprovement in generating Extract Method transformations. A usability study\ninvolving 37 professional developers further shows that refactorings performed\nby MANTRA are perceived to be as readable and reusable as human-written code,\nand in certain cases, even more favorable. These results highlight the\npractical advantages of MANTRA and emphasize the growing potential of LLM-based\nsystems in advancing the automation of software refactoring tasks.",
      "authors": [
        "Yisen Xu",
        "Feng Lin",
        "Jinqiu Yang",
        " Tse-Hsun",
        " Chen",
        "Nikolaos Tsantalis"
      ],
      "categories": [
        "cs.SE"
      ],
      "links": [
        "http://arxiv.org/abs/2503.14340v1",
        "http://arxiv.org/pdf/2503.14340v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2503.14573v1",
      "title": "Three-dimensional Reconstruction of the Lumbar Spine with Submillimeter\n  Accuracy Using Biplanar X-ray Images",
      "published": "2025-03-18T15:00:39Z",
      "updated": "2025-03-18T15:00:39Z",
      "summary": "Three-dimensional reconstruction of the spine under weight-bearing conditions\nfrom biplanar X-ray images is of great importance for the clinical assessment\nof spinal diseases. However, the current fully automated reconstruction methods\nhave low accuracy and fail to meet the clinical application standards. This\nstudy developed and validated a fully automated method for high-accuracy 3D\nreconstruction of the lumbar spine from biplanar X-ray images. The method\ninvolves lumbar decomposition and landmark detection from the raw X-ray images,\nfollowed by a deformable model and landmark-weighted 2D-3D registration\napproach. The reconstruction accuracy was validated by the gold standard\nobtained through the registration of CT-segmented vertebral models with the\nbiplanar X-ray images. The proposed method achieved a 3D reconstruction\naccuracy of 0.80 mm, representing a significant improvement over the mainstream\napproaches. This study will contribute to the clinical diagnosis of lumbar in\nweight-bearing positions.",
      "authors": [
        "Wanxin Yu",
        "Zhemin Zhu",
        "Cong Wang",
        "Yihang Bao",
        "Chunjie Xia",
        "Rongshan Cheng",
        "Yan Yu",
        "Tsung-Yuan Tsai"
      ],
      "categories": [
        "eess.IV",
        "cs.CV",
        "cs.GR"
      ],
      "links": [
        "http://arxiv.org/abs/2503.14573v1",
        "http://arxiv.org/pdf/2503.14573v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2503.14273v2",
      "title": "Manual Labelling Artificially Inflates Deep Learning-Based Segmentation\n  Performance on RGB Images of Closed Canopy: Validation Using TLS",
      "published": "2025-03-18T14:09:00Z",
      "updated": "2025-03-19T16:17:19Z",
      "summary": "Monitoring forest dynamics at an individual tree scale is essential for\naccurately assessing ecosystem responses to climate change, yet traditional\nmethods relying on field-based forest inventories are labor-intensive and\nlimited in spatial coverage. Advances in remote sensing using drone-acquired\nRGB imagery combined with deep learning models have promised precise individual\ntree crown (ITC) segmentation; however, existing methods are frequently\nvalidated against human-annotated images, lacking rigorous independent ground\ntruth. In this study, we generate high-fidelity validation labels from\nco-located Terrestrial Laser Scanning (TLS) data for drone imagery of mixed\nunmanaged boreal and Mediterranean forests. We evaluate the performance of two\nwidely used deep learning ITC segmentation models - DeepForest (RetinaNet) and\nDetectree2 (Mask R-CNN) - on these data, and compare to performance on further\nMediterranean forest data labelled manually. When validated against TLS-derived\nground truth from Mediterranean forests, model performance decreased\nsignificantly compared to assessment based on hand-labelled from an\necologically similar site (AP50: 0.094 vs. 0.670). Restricting evaluation to\nonly canopy trees shrank this gap considerably (Canopy AP50: 0.365), although\nperformance was still far lower than on similar hand-labelled data. Models also\nperformed poorly on boreal forest data (AP50: 0.142), although again increasing\nwhen evaluated on canopy trees only (Canopy AP50: 0.308). Both models showed\nvery poor localisation accuracy at stricter IoU thresholds, even when\nrestricted to canopy trees (Max AP75: 0.051). Similar results have been\nobserved in studies using aerial LiDAR data, suggesting fundamental limitations\nin aerial-based segmentation approaches in closed canopy forests.",
      "authors": [
        "Matthew J. Allen",
        "Harry J. F. Owen",
        "Stuart W. D. Grieve",
        "Emily R. Lines"
      ],
      "categories": [
        "cs.CV",
        "cs.AI",
        "I.4; I.4.6; I.4.8; I.4.9; I.5; I.5.4"
      ],
      "links": [
        "http://arxiv.org/abs/2503.14273v2",
        "http://arxiv.org/pdf/2503.14273v2"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2503.14269v1",
      "title": "DARS: Dynamic Action Re-Sampling to Enhance Coding Agent Performance by\n  Adaptive Tree Traversal",
      "published": "2025-03-18T14:02:59Z",
      "updated": "2025-03-18T14:02:59Z",
      "summary": "Large Language Models (LLMs) have revolutionized various domains, including\nnatural language processing, data analysis, and software development, by\nenabling automation. In software engineering, LLM-powered coding agents have\ngarnered significant attention due to their potential to automate complex\ndevelopment tasks, assist in debugging, and enhance productivity. However,\nexisting approaches often struggle with sub-optimal decision-making, requiring\neither extensive manual intervention or inefficient compute scaling strategies.\nTo improve coding agent performance, we present Dynamic Action Re-Sampling\n(DARS), a novel inference time compute scaling approach for coding agents, that\nis faster and more effective at recovering from sub-optimal decisions compared\nto baselines. While traditional agents either follow linear trajectories or\nrely on random sampling for scaling compute, our approach DARS works by\nbranching out a trajectory at certain key decision points by taking an\nalternative action given the history of the trajectory and execution feedback\nof the previous attempt from that point. We evaluate our approach on SWE-Bench\nLite benchmark, demonstrating that this scaling strategy achieves a pass@k\nscore of 55% with Claude 3.5 Sonnet V2. Our framework achieves a pass@1 rate of\n47%, outperforming state-of-the-art (SOTA) open-source frameworks.",
      "authors": [
        "Vaibhav Aggarwal",
        "Ojasv Kamal",
        "Abhinav Japesh",
        "Zhijing Jin",
        "Bernhard Sch\u00f6lkopf"
      ],
      "categories": [
        "cs.CL",
        "cs.SE"
      ],
      "links": [
        "http://arxiv.org/abs/2503.14269v1",
        "http://arxiv.org/pdf/2503.14269v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2503.14231v1",
      "title": "Multi-task Learning for Identification of Porcelain in Song and Yuan\n  Dynasties",
      "published": "2025-03-18T13:09:00Z",
      "updated": "2025-03-18T13:09:00Z",
      "summary": "Chinese porcelain holds immense historical and cultural value, making its\naccurate classification essential for archaeological research and cultural\nheritage preservation. Traditional classification methods rely heavily on\nexpert analysis, which is time-consuming, subjective, and difficult to scale.\nThis paper explores the application of DL and transfer learning techniques to\nautomate the classification of porcelain artifacts across four key attributes:\ndynasty, glaze, ware, and type. We evaluate four Convolutional Neural Networks\n(CNNs) - ResNet50, MobileNetV2, VGG16, and InceptionV3 - comparing their\nperformance with and without pre-trained weights. Our results demonstrate that\ntransfer learning significantly enhances classification accuracy, particularly\nfor complex tasks like type classification, where models trained from scratch\nexhibit lower performance. MobileNetV2 and ResNet50 consistently achieve high\naccuracy and robustness across all tasks, while VGG16 struggles with more\ndiverse classifications. We further discuss the impact of dataset limitations\nand propose future directions, including domain-specific pre-training,\nintegration of attention mechanisms, explainable AI methods, and generalization\nto other cultural artifacts.",
      "authors": [
        "Ziyao Ling",
        "Giovanni Delnevo",
        "Paola Salomoni",
        "Silvia Mirri"
      ],
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "links": [
        "http://arxiv.org/abs/2503.14231v1",
        "http://arxiv.org/pdf/2503.14231v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2503.14184v1",
      "title": "Variable Time-Step MPC for Agile Multi-Rotor UAV Interception of Dynamic\n  Targets",
      "published": "2025-03-18T11:59:24Z",
      "updated": "2025-03-18T11:59:24Z",
      "summary": "Agile trajectory planning can improve the efficiency of multi-rotor Uncrewed\nAerial Vehicles (UAVs) in scenarios with combined task-oriented and kinematic\ntrajectory planning, such as monitoring spatio-temporal phenomena or\nintercepting dynamic targets. Agile planning using existing non-linear model\npredictive control methods is limited by the number of planning steps as it\nbecomes increasingly computationally demanding. That reduces the prediction\nhorizon length, leading to a decrease in solution quality. Besides, the fixed\ntime-step length limits the utilization of the available UAV dynamics in the\ntarget neighborhood. In this paper, we propose to address these limitations by\nintroducing variable time steps and coupling them with the prediction horizon\nlength. A simplified point-mass motion primitive is used to leverage the\ndifferential flatness of quadrotor dynamics and the generation of feasible\ntrajectories in the flat output space. Based on the presented evaluation\nresults and experimentally validated deployment, the proposed method increases\nthe solution quality by enabling planning for long flight segments but allowing\ntightly sampled maneuvering.",
      "authors": [
        "Atharva Ghotavadekar",
        "Franti\u0161ek Nekov\u00e1\u0159",
        "Martin Saska",
        "Jan Faigl"
      ],
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.SY",
        "eess.SY"
      ],
      "links": [
        "http://dx.doi.org/10.1109/LRA.2024.3518096",
        "http://arxiv.org/abs/2503.14184v1",
        "http://arxiv.org/pdf/2503.14184v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2503.14153v1",
      "title": "Speculative Decoding for Verilog: Speed and Quality, All in One",
      "published": "2025-03-18T11:21:53Z",
      "updated": "2025-03-18T11:21:53Z",
      "summary": "The rapid advancement of large language models (LLMs) has revolutionized code\ngeneration tasks across various programming languages. However, the unique\ncharacteristics of programming languages, particularly those like Verilog with\nspecific syntax and lower representation in training datasets, pose significant\nchallenges for conventional tokenization and decoding approaches. In this\npaper, we introduce a novel application of speculative decoding for Verilog\ncode generation, showing that it can improve both inference speed and output\nquality, effectively achieving speed and quality all in one. Unlike standard\nLLM tokenization schemes, which often fragment meaningful code structures, our\napproach aligns decoding stops with syntactically significant tokens, making it\neasier for models to learn the token distribution. This refinement addresses\ninherent tokenization issues and enhances the model's ability to capture\nVerilog's logical constructs more effectively. Our experimental results show\nthat our method achieves up to a 5.05x speedup in Verilog code generation and\nincreases pass@10 functional accuracy on RTLLM by up to 17.19% compared to\nconventional training strategies. These findings highlight speculative decoding\nas a promising approach to bridge the quality gap in code generation for\nspecialized programming languages.",
      "authors": [
        "Changran Xu",
        "Yi Liu",
        "Yunhao Zhou",
        "Shan Huang",
        "Ningyi Xu",
        "Qiang Xu"
      ],
      "categories": [
        "cs.LG",
        "cs.AR",
        "cs.CL"
      ],
      "links": [
        "http://arxiv.org/abs/2503.14153v1",
        "http://arxiv.org/pdf/2503.14153v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2503.14138v1",
      "title": "Exploring Disparity-Accuracy Trade-offs in Face Recognition Systems: The\n  Role of Datasets, Architectures, and Loss Functions",
      "published": "2025-03-18T11:04:57Z",
      "updated": "2025-03-18T11:04:57Z",
      "summary": "Automated Face Recognition Systems (FRSs), developed using deep learning\nmodels, are deployed worldwide for identity verification and facial attribute\nanalysis. The performance of these models is determined by a complex\ninterdependence among the model architecture, optimization/loss function and\ndatasets. Although FRSs have surpassed human-level accuracy, they continue to\nbe disparate against certain demographics. Due to the ubiquity of applications,\nit is extremely important to understand the impact of the three components --\nmodel architecture, loss function and face image dataset on the\naccuracy-disparity trade-off to design better, unbiased platforms. In this\nwork, we perform an in-depth analysis of three FRSs for the task of gender\nprediction, with various architectural modifications resulting in ten\ndeep-learning models coupled with four loss functions and benchmark them on\nseven face datasets across 266 evaluation configurations. Our results show that\nall three components have an individual as well as a combined impact on both\naccuracy and disparity. We identify that datasets have an inherent property\nthat causes them to perform similarly across models, independent of the choice\nof loss functions. Moreover, the choice of dataset determines the model's\nperceived bias -- the same model reports bias in opposite directions for three\ngender-balanced datasets of ``in-the-wild'' face images of popular individuals.\nStudying the facial embeddings shows that the models are unable to generalize a\nuniform definition of what constitutes a ``female face'' as opposed to a ``male\nface'', due to dataset diversity. We provide recommendations to model\ndevelopers on using our study as a blueprint for model development and\nsubsequent deployment.",
      "authors": [
        "Siddharth D Jaiswal",
        "Sagnik Basu",
        "Sandipan Sikdar",
        "Animesh Mukherjee"
      ],
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CY"
      ],
      "links": [
        "http://arxiv.org/abs/2503.14138v1",
        "http://arxiv.org/pdf/2503.14138v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2503.14130v1",
      "title": "Inference-Time Intervention in Large Language Models for Reliable\n  Requirement Verification",
      "published": "2025-03-18T10:49:36Z",
      "updated": "2025-03-18T10:49:36Z",
      "summary": "Steering the behavior of Large Language Models (LLMs) remains a challenge,\nparticularly in engineering applications where precision and reliability are\ncritical. While fine-tuning and prompting methods can modify model behavior,\nthey lack the dynamic and exact control necessary for engineering applications.\nInference-time intervention techniques provide a promising alternative,\nallowing targeted adjustments to LLM outputs. In this work, we demonstrate how\ninterventions enable fine-grained control for automating the usually\ntime-intensive requirement verification process in Model-Based Systems\nEngineering (MBSE). Using two early-stage Capella SysML models of space\nmissions with associated requirements, we apply the intervened LLMs to reason\nover a graph representation of the model to determine whether a requirement is\nfulfilled. Our method achieves robust and reliable outputs, significantly\nimproving over both a baseline model and a fine-tuning approach. By identifying\nand modifying as few as one to three specialised attention heads, we can\nsignificantly change the model's behavior. When combined with self-consistency,\nthis allows us to achieve perfect precision on our holdout test set.",
      "authors": [
        "Paul Darm",
        "James Xie",
        "Annalisa Riccardi"
      ],
      "categories": [
        "cs.AI",
        "cs.SE",
        "H.4.2; I.2.1; I.2.7"
      ],
      "links": [
        "http://arxiv.org/abs/2503.14130v1",
        "http://arxiv.org/pdf/2503.14130v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2503.14078v1",
      "title": "On weak notions of no-arbitrage in a 1D general diffusion market with\n  interest rates",
      "published": "2025-03-18T09:55:39Z",
      "updated": "2025-03-18T09:55:39Z",
      "summary": "We establish deterministic necessary and sufficient conditions for the\nno-arbitrage notions \"no increasing profit\" (NIP), \"no strong arbitrage\" (NSA)\nand \"no unbounded profit with bounded risk\" (NUPBR) in one-dimensional general\ndiffusion markets. These are markets with one risky asset, which is modeled as\na regular continuous strong Markov process that is also a semimartingale, and a\nriskless asset that grows exponentially at a constant rate $r\\in \\mathbb{R}$.\nAll deterministic criteria are provided in terms of the scale function and the\nspeed measure of the risky asset process. Our study reveals a variety of\nsurprising effects. For instance, irrespective of the interest rate, NIP is not\nexcluded by reflecting boundaries or an irregular scale function. In the case\nof non-zero interest rates, it is even possible that NUPBR holds in the\npresence of reflecting boundaries and/or skew thresholds. In the zero interest\nrate regime, we also identify NSA as the minimal no arbitrage notion that\nexcludes reflecting boundaries and that forces the scale function to be\ncontinuously differentiable with strictly positive absolutely continuous\nderivative, meaning that it is of the same form as for a stochastic\ndifferential equation.",
      "authors": [
        "Alexis Anagnostakis",
        "David Criens",
        "Mikhail Urusov"
      ],
      "categories": [
        "q-fin.MF",
        "math.PR"
      ],
      "links": [
        "http://arxiv.org/abs/2503.14078v1",
        "http://arxiv.org/pdf/2503.14078v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2503.14023v1",
      "title": "Synthetic Data Generation Using Large Language Models: Advances in Text\n  and Code",
      "published": "2025-03-18T08:34:03Z",
      "updated": "2025-03-18T08:34:03Z",
      "summary": "Large language models (LLMs) have unlocked new possibilities for generating\nsynthetic training data in both natural language and code. By producing\nartificial but task-relevant examples, these models can significantly augment\nor even replace real-world datasets, especially when labeled data is scarce or\nsensitive. This paper surveys recent advances in using LLMs to create synthetic\ntext and code, emphasizing prompt-based generation, retrieval-augmented\npipelines, and iterative self-refinement. We show how these methods enrich\nlow-resource tasks such as classification and question answering, as well as\ncode-centric applications such as instruction tuning, code translation, and bug\nrepair, by enabling automated verification of functional correctness. Alongside\npotential benefits like cost-effectiveness, broad coverage, and controllable\ndiversity, we address challenges such as factual inaccuracies in generated\ntext, lack of stylistic realism, and the risk of bias amplification. Proposed\nmitigations include filtering and weighting outputs and reinforcement learning\nwith execution feedback for code. We conclude with open research directions\nlike automated prompt engineering, cross-modal data synthesis, and robust\nevaluation frameworks, highlighting the importance of LLM-generated synthetic\ndata in advancing AI while emphasizing ethical and quality safeguards.",
      "authors": [
        "Mihai Nadas",
        "Laura Diosan",
        "Andreea Tomescu"
      ],
      "categories": [
        "cs.CL"
      ],
      "links": [
        "http://arxiv.org/abs/2503.14023v1",
        "http://arxiv.org/pdf/2503.14023v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2503.14002v1",
      "title": "MeshFleet: Filtered and Annotated 3D Vehicle Dataset for Domain Specific\n  Generative Modeling",
      "published": "2025-03-18T08:09:24Z",
      "updated": "2025-03-18T08:09:24Z",
      "summary": "Generative models have recently made remarkable progress in the field of 3D\nobjects. However, their practical application in fields like engineering\nremains limited since they fail to deliver the accuracy, quality, and\ncontrollability needed for domain-specific tasks. Fine-tuning large generative\nmodels is a promising perspective for making these models available in these\nfields. Creating high-quality, domain-specific 3D datasets is crucial for\nfine-tuning large generative models, yet the data filtering and annotation\nprocess remains a significant bottleneck. We present MeshFleet, a filtered and\nannotated 3D vehicle dataset extracted from Objaverse-XL, the most extensive\npublicly available collection of 3D objects. Our approach proposes a pipeline\nfor automated data filtering based on a quality classifier. This classifier is\ntrained on a manually labeled subset of Objaverse, incorporating DINOv2 and\nSigLIP embeddings, refined through caption-based analysis and uncertainty\nestimation. We demonstrate the efficacy of our filtering method through a\ncomparative analysis against caption and image aesthetic score-based techniques\nand fine-tuning experiments with SV3D, highlighting the importance of targeted\ndata selection for domain-specific 3D generative modeling.",
      "authors": [
        "Damian Boborzi",
        "Phillip Mueller",
        "Jonas Emrich",
        "Dominik Schmid",
        "Sebastian Mueller",
        "Lars Mikelsons"
      ],
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "links": [
        "http://arxiv.org/abs/2503.14002v1",
        "http://arxiv.org/pdf/2503.14002v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2503.13989v1",
      "title": "Rethinking Cell Counting Methods: Decoupling Counting and Localization",
      "published": "2025-03-18T07:50:03Z",
      "updated": "2025-03-18T07:50:03Z",
      "summary": "Cell counting in microscopy images is vital in medicine and biology but\nextremely tedious and time-consuming to perform manually. While automated\nmethods have advanced in recent years, state-of-the-art approaches tend to\nincreasingly complex model designs. In this paper, we propose a conceptually\nsimple yet effective decoupled learning scheme for automated cell counting,\nconsisting of separate counter and localizer networks. In contrast to jointly\nlearning counting and density map estimation, we show that decoupling these\nobjectives surprisingly improves results. The counter operates on intermediate\nfeature maps rather than pixel space to leverage global context and produce\ncount estimates, while also generating coarse density maps. The localizer then\nreconstructs high-resolution density maps that precisely localize individual\ncells, conditional on the original images and coarse density maps from the\ncounter. Besides, to boost counting accuracy, we further introduce a global\nmessage passing module to integrate cross-region patterns. Extensive\nexperiments on four datasets demonstrate that our approach, despite its\nsimplicity, challenges common practice and achieves state-of-the-art\nperformance by significant margins. Our key insight is that decoupled learning\nalleviates the need to learn counting on high-resolution density maps directly,\nallowing the model to focus on global features critical for accurate estimates.\nCode is available at https://github.com/MedAITech/DCL.",
      "authors": [
        "Zixuan Zheng",
        "Yilei Shi",
        "Chunlei Li",
        "Jingliang Hu",
        "Xiao Xiang Zhu",
        "Lichao Mou"
      ],
      "categories": [
        "cs.CV"
      ],
      "links": [
        "http://arxiv.org/abs/2503.13989v1",
        "http://arxiv.org/pdf/2503.13989v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2503.13987v1",
      "title": "Striving for Simplicity: Simple Yet Effective Prior-Aware\n  Pseudo-Labeling for Semi-Supervised Ultrasound Image Segmentation",
      "published": "2025-03-18T07:44:09Z",
      "updated": "2025-03-18T07:44:09Z",
      "summary": "Medical ultrasound imaging is ubiquitous, but manual analysis struggles to\nkeep pace. Automated segmentation can help but requires large labeled datasets,\nwhich are scarce. Semi-supervised learning leveraging both unlabeled and\nlimited labeled data is a promising approach. State-of-the-art methods use\nconsistency regularization or pseudo-labeling but grow increasingly complex.\nWithout sufficient labels, these models often latch onto artifacts or allow\nanatomically implausible segmentations. In this paper, we present a simple yet\neffective pseudo-labeling method with an adversarially learned shape prior to\nregularize segmentations. Specifically, we devise an encoder-twin-decoder\nnetwork where the shape prior acts as an implicit shape model, penalizing\nanatomically implausible but not ground-truth-deviating predictions. Without\nbells and whistles, our simple approach achieves state-of-the-art performance\non two benchmarks under different partition protocols. We provide a strong\nbaseline for future semi-supervised medical image segmentation. Code is\navailable at https://github.com/WUTCM-Lab/Shape-Prior-Semi-Seg.",
      "authors": [
        "Yaxiong Chen",
        "Yujie Wang",
        "Zixuan Zheng",
        "Jingliang Hu",
        "Yilei Shi",
        "Shengwu Xiong",
        "Xiao Xiang Zhu",
        "Lichao Mou"
      ],
      "categories": [
        "eess.IV",
        "cs.CV"
      ],
      "links": [
        "http://arxiv.org/abs/2503.13987v1",
        "http://arxiv.org/pdf/2503.13987v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2503.13923v1",
      "title": "ConSCompF: Consistency-focused Similarity Comparison Framework for\n  Generative Large Language Models",
      "published": "2025-03-18T05:38:04Z",
      "updated": "2025-03-18T05:38:04Z",
      "summary": "Large language models (LLMs) have been one of the most important discoveries\nin machine learning in recent years. LLM-based artificial intelligence (AI)\nassistants, such as ChatGPT, have consistently attracted the attention from\nresearchers, investors, and the general public, driving the rapid growth of\nthis industry. With the frequent introduction of new LLMs to the market, it\nbecomes increasingly difficult to differentiate between them, creating a demand\nfor new LLM comparison methods.\n  In this research, the Consistency-focused Similarity Comparison Framework\n(ConSCompF) for generative large language models is proposed. It compares texts\ngenerated by two LLMs and produces a similarity score, indicating the overall\ndegree of similarity between their responses. The main advantage of this\nframework is that it can operate on a small number of unlabeled data, such as\nchatbot instruction prompts, and does not require LLM developers to disclose\nany information about their product.\n  To evaluate the efficacy of ConSCompF, two experiments aimed at identifying\nsimilarities between multiple LLMs are conducted. Additionally, these\nexperiments examine the correlation between the similarity scores generated by\nConSCompF and the differences in the outputs produced by other benchmarking\ntechniques, such as ROUGE-L. Finally, a series of few-shot LLM comparison\nexperiments is conducted to evaluate the performance of ConSCompF in a few-shot\nLLM comparison scenario.\n  The proposed framework can be used for calculating similarity matrices of\nmultiple LLMs, which can be effectively visualized using principal component\nanalysis (PCA). The ConSCompF output may provide useful insights into data that\nmight have been used during LLM training and help detect possible investment\nfraud attempts.",
      "authors": [
        "Alexey Karev",
        "Dong Xu"
      ],
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "links": [
        "http://dx.doi.org/10.1613/jair.1.17028",
        "http://arxiv.org/abs/2503.13923v1",
        "http://arxiv.org/pdf/2503.13923v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2503.13895v1",
      "title": "Exploiting Inherent Class Label: Towards Robust Scribble Supervised\n  Semantic Segmentation",
      "published": "2025-03-18T04:43:07Z",
      "updated": "2025-03-18T04:43:07Z",
      "summary": "Scribble-based weakly supervised semantic segmentation leverages only a few\nannotated pixels as labels to train a segmentation model, presenting\nsignificant potential for reducing the human labor involved in the annotation\nprocess. This approach faces two primary challenges: first, the sparsity of\nscribble annotations can lead to inconsistent predictions due to limited\nsupervision; second, the variability in scribble annotations, reflecting\ndiffering human annotator preferences, can prevent the model from consistently\ncapturing the discriminative regions of objects, potentially leading to\nunstable predictions. To address these issues, we propose a holistic framework,\nthe class-driven scribble promotion network, for robust scribble-supervised\nsemantic segmentation. This framework not only utilizes the provided scribble\nannotations but also leverages their associated class labels to generate\nreliable pseudo-labels. Within the network, we introduce a localization\nrectification module to mitigate noisy labels and a distance perception module\nto identify reliable regions surrounding scribble annotations and\npseudo-labels. In addition, we introduce new large-scale benchmarks,\nScribbleCOCO and ScribbleCityscapes, accompanied by a scribble simulation\nalgorithm that enables evaluation across varying scribble styles. Our method\ndemonstrates competitive performance in both accuracy and robustness,\nunderscoring its superiority over existing approaches. The datasets and the\ncodes will be made publicly available.",
      "authors": [
        "Xinliang Zhang",
        "Lei Zhu",
        "Shuang Zeng",
        "Hangzhou He",
        "Ourui Fu",
        "Zhengjian Yao",
        "Zhaoheng Xie",
        "Yanye Lu"
      ],
      "categories": [
        "cs.CV"
      ],
      "links": [
        "http://arxiv.org/abs/2503.13895v1",
        "http://arxiv.org/pdf/2503.13895v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2503.13882v1",
      "title": "MoK-RAG: Mixture of Knowledge Paths Enhanced Retrieval-Augmented\n  Generation for Embodied AI Environments",
      "published": "2025-03-18T04:27:02Z",
      "updated": "2025-03-18T04:27:02Z",
      "summary": "While human cognition inherently retrieves information from diverse and\nspecialized knowledge sources during decision-making processes, current\nRetrieval-Augmented Generation (RAG) systems typically operate through\nsingle-source knowledge retrieval, leading to a cognitive-algorithmic\ndiscrepancy. To bridge this gap, we introduce MoK-RAG, a novel multi-source RAG\nframework that implements a mixture of knowledge paths enhanced retrieval\nmechanism through functional partitioning of a large language model (LLM)\ncorpus into distinct sections, enabling retrieval from multiple specialized\nknowledge paths. Applied to the generation of 3D simulated environments, our\nproposed MoK-RAG3D enhances this paradigm by partitioning 3D assets into\ndistinct sections and organizing them based on a hierarchical knowledge tree\nstructure. Different from previous methods that only use manual evaluation, we\npioneered the introduction of automated evaluation methods for 3D scenes. Both\nautomatic and human evaluations in our experiments demonstrate that MoK-RAG3D\ncan assist Embodied AI agents in generating diverse scenes.",
      "authors": [
        "Zhengsheng Guo",
        "Linwei Zheng",
        "Xinyang Chen",
        "Xuefeng Bai",
        "Kehai Chen",
        "Min Zhang"
      ],
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "links": [
        "http://arxiv.org/abs/2503.13882v1",
        "http://arxiv.org/pdf/2503.13882v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2503.13879v1",
      "title": "Bridging Social Psychology and LLM Reasoning: Conflict-Aware Meta-Review\n  Generation via Cognitive Alignment",
      "published": "2025-03-18T04:13:11Z",
      "updated": "2025-03-18T04:13:11Z",
      "summary": "The rapid growth of scholarly submissions has overwhelmed traditional peer\nreview systems, driving the need for intelligent automation to preserve\nscientific rigor. While large language models (LLMs) show promise in automating\nmanuscript critiques, their ability to synthesize high-stakes meta-reviews,\nwhich require conflict-aware reasoning and consensus derivation, remains\nunderdeveloped. Existing methods fail to effectively handle conflicting\nviewpoints within differing opinions, and often introduce additional cognitive\nbiases, such as anchoring effects and conformity bias.To overcome these\nlimitations, we propose the Cognitive Alignment Framework (CAF), a dual-process\narchitecture that transforms LLMs into adaptive scientific arbitrators. By\noperationalizing Kahneman's dual-process theory, CAF introduces a three-step\ncognitive pipeline: review initialization, incremental integration, and\ncognitive alignment.Empirical validation shows that CAF outperforms existing\nLLM-based methods, with sentiment consistency gains reaching up to 19.47\\% and\ncontent consistency improving by as much as 12.95\\%.",
      "authors": [
        "Wei Chen",
        "Han Ding",
        "Meng Yuan",
        "Zhao Zhang",
        "Deqing Wang",
        "Fuzhen Zhuang"
      ],
      "categories": [
        "cs.AI"
      ],
      "links": [
        "http://arxiv.org/abs/2503.13879v1",
        "http://arxiv.org/pdf/2503.13879v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2503.13857v2",
      "title": "Enabling Inclusive Systematic Reviews: Incorporating Preprint Articles\n  with Large Language Model-Driven Evaluations",
      "published": "2025-03-18T03:14:23Z",
      "updated": "2025-03-19T15:21:06Z",
      "summary": "Background. Systematic reviews in comparative effectiveness research require\ntimely evidence synthesis. Preprints accelerate knowledge dissemination but\nvary in quality, posing challenges for systematic reviews.\n  Methods. We propose AutoConfidence (automated confidence assessment), an\nadvanced framework for predicting preprint publication, which reduces reliance\non manual curation and expands the range of predictors, including three key\nadvancements: (1) automated data extraction using natural language processing\ntechniques, (2) semantic embeddings of titles and abstracts, and (3) large\nlanguage model (LLM)-driven evaluation scores. Additionally, we employed two\nprediction models: a random forest classifier for binary outcome and a survival\ncure model that predicts both binary outcome and publication risk over time.\n  Results. The random forest classifier achieved AUROC 0.692 with LLM-driven\nscores, improving to 0.733 with semantic embeddings and 0.747 with article\nusage metrics. The survival cure model reached AUROC 0.716 with LLM-driven\nscores, improving to 0.731 with semantic embeddings. For publication risk\nprediction, it achieved a concordance index of 0.658, increasing to 0.667 with\nsemantic embeddings.\n  Conclusion. Our study advances the framework for preprint publication\nprediction through automated data extraction and multiple feature integration.\nBy combining semantic embeddings with LLM-driven evaluations, AutoConfidence\nenhances predictive performance while reducing manual annotation burden. The\nframework has the potential to facilitate systematic incorporation of preprint\narticles in evidence-based medicine, supporting researchers in more effective\nevaluation and utilization of preprint resources.",
      "authors": [
        "Rui Yang",
        "Jiayi Tong",
        "Haoyuan Wang",
        "Hui Huang",
        "Ziyang Hu",
        "Peiyu Li",
        "Nan Liu",
        "Christopher J. Lindsell",
        "Michael J. Pencina",
        "Yong Chen",
        "Chuan Hong"
      ],
      "categories": [
        "cs.CL"
      ],
      "links": [
        "http://arxiv.org/abs/2503.13857v2",
        "http://arxiv.org/pdf/2503.13857v2"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2503.13817v1",
      "title": "VARP: Reinforcement Learning from Vision-Language Model Feedback with\n  Agent Regularized Preferences",
      "published": "2025-03-18T01:51:27Z",
      "updated": "2025-03-18T01:51:27Z",
      "summary": "Designing reward functions for continuous-control robotics often leads to\nsubtle misalignments or reward hacking, especially in complex tasks.\nPreference-based RL mitigates some of these pitfalls by learning rewards from\ncomparative feedback rather than hand-crafted signals, yet scaling human\nannotations remains challenging. Recent work uses Vision-Language Models (VLMs)\nto automate preference labeling, but a single final-state image generally fails\nto capture the agent's full motion. In this paper, we present a two-part\nsolution that both improves feedback accuracy and better aligns reward learning\nwith the agent's policy. First, we overlay trajectory sketches on final\nobservations to reveal the path taken, allowing VLMs to provide more reliable\npreferences-improving preference accuracy by approximately 15-20% in metaworld\ntasks. Second, we regularize reward learning by incorporating the agent's\nperformance, ensuring that the reward model is optimized based on data\ngenerated by the current policy; this addition boosts episode returns by 20-30%\nin locomotion tasks. Empirical studies on metaworld demonstrate that our method\nachieves, for instance, around 70-80% success rate in all tasks, compared to\nbelow 50% for standard approaches. These results underscore the efficacy of\ncombining richer visual representations with agent-aware reward regularization.",
      "authors": [
        "Anukriti Singh",
        "Amisha Bhaskar",
        "Peihong Yu",
        "Souradip Chakraborty",
        "Ruthwik Dasyam",
        "Amrit Bedi",
        "Pratap Tokekar"
      ],
      "categories": [
        "cs.AI",
        "cs.HC",
        "cs.LG",
        "cs.RO"
      ],
      "links": [
        "http://arxiv.org/abs/2503.13817v1",
        "http://arxiv.org/pdf/2503.13817v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2503.13813v1",
      "title": "Automatic MILP Model Construction for Multi-Robot Task Allocation and\n  Scheduling Based on Large Language Models",
      "published": "2025-03-18T01:45:19Z",
      "updated": "2025-03-18T01:45:19Z",
      "summary": "With the accelerated development of Industry 4.0, intelligent manufacturing\nsystems increasingly require efficient task allocation and scheduling in\nmulti-robot systems. However, existing methods rely on domain expertise and\nface challenges in adapting to dynamic production constraints. Additionally,\nenterprises have high privacy requirements for production scheduling data,\nwhich prevents the use of cloud-based large language models (LLMs) for solution\ndevelopment. To address these challenges, there is an urgent need for an\nautomated modeling solution that meets data privacy requirements. This study\nproposes a knowledge-augmented mixed integer linear programming (MILP)\nautomated formulation framework, integrating local LLMs with domain-specific\nknowledge bases to generate executable code from natural language descriptions\nautomatically. The framework employs a knowledge-guided\nDeepSeek-R1-Distill-Qwen-32B model to extract complex spatiotemporal\nconstraints (82% average accuracy) and leverages a supervised fine-tuned\nQwen2.5-Coder-7B-Instruct model for efficient MILP code generation (90% average\naccuracy). Experimental results demonstrate that the framework successfully\nachieves automatic modeling in the aircraft skin manufacturing case while\nensuring data privacy and computational efficiency. This research provides a\nlow-barrier and highly reliable technical path for modeling in complex\nindustrial scenarios.",
      "authors": [
        "Mingming Peng",
        "Zhendong Chen",
        "Jie Yang",
        "Jin Huang",
        "Zhengqi Shi",
        "Qihao Liu",
        "Xinyu Li",
        "Liang Gao"
      ],
      "categories": [
        "cs.AI",
        "cs.RO"
      ],
      "links": [
        "http://arxiv.org/abs/2503.13813v1",
        "http://arxiv.org/pdf/2503.13813v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2503.13806v1",
      "title": "Organ-aware Multi-scale Medical Image Segmentation Using Text Prompt\n  Engineering",
      "published": "2025-03-18T01:35:34Z",
      "updated": "2025-03-18T01:35:34Z",
      "summary": "Accurate segmentation is essential for effective treatment planning and\ndisease monitoring. Existing medical image segmentation methods predominantly\nrely on uni-modal visual inputs, such as images or videos, requiring\nlabor-intensive manual annotations. Additionally, medical imaging techniques\ncapture multiple intertwined organs within a single scan, further complicating\nsegmentation accuracy. To address these challenges, MedSAM, a large-scale\nmedical segmentation model based on the Segment Anything Model (SAM), was\ndeveloped to enhance segmentation accuracy by integrating image features with\nuser-provided prompts. While MedSAM has demonstrated strong performance across\nvarious medical segmentation tasks, it primarily relies on geometric prompts\n(e.g., points and bounding boxes) and lacks support for text-based prompts,\nwhich could help specify subtle or ambiguous anatomical structures. To overcome\nthese limitations, we propose the Organ-aware Multi-scale Text-guided Medical\nImage Segmentation Model (OMT-SAM) for multi-organ segmentation. Our approach\nintroduces CLIP encoders as a novel image-text prompt encoder, operating with\nthe geometric prompt encoder to provide informative contextual guidance. We\npair descriptive textual prompts with corresponding images, processing them\nthrough pre-trained CLIP encoders and a cross-attention mechanism to generate\nfused image-text embeddings. Additionally, we extract multi-scale visual\nfeatures from MedSAM, capturing fine-grained anatomical details at different\nlevels of granularity. We evaluate OMT-SAM on the FLARE 2021 dataset,\nbenchmarking its performance against existing segmentation methods. Empirical\nresults demonstrate that OMT-SAM achieves a mean Dice Similarity Coefficient of\n0.937, outperforming MedSAM (0.893) and other segmentation models, highlighting\nits superior capability in handling complex medical image segmentation tasks.",
      "authors": [
        "Wenjie Zhang",
        "Ziyang Zhang",
        "Mengnan He",
        "Jiancheng Ye"
      ],
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "links": [
        "http://arxiv.org/abs/2503.13806v1",
        "http://arxiv.org/pdf/2503.13806v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2503.13787v1",
      "title": "A Systematic Digital Engineering Approach to Verification & Validation\n  of Autonomous Ground Vehicles in Off-Road Environments",
      "published": "2025-03-18T00:40:35Z",
      "updated": "2025-03-18T00:40:35Z",
      "summary": "The engineering community currently encounters significant challenges in the\nsystematic development and validation of autonomy algorithms for off-road\nground vehicles. These challenges are posed by unusually high test parameters\nand algorithmic variants. In order to address these pain points, this work\npresents an optimized digital engineering framework that tightly couples\ndigital twin simulations with model-based systems engineering (MBSE) and\nmodel-based design (MBD) workflows. The efficacy of the proposed framework is\ndemonstrated through an end-to-end case study of an autonomous light tactical\nvehicle (LTV) performing visual servoing to drive along a dirt road and\nreacting to any obstacles or environmental changes. The presented methodology\nallows for traceable requirements engineering, efficient variant management,\ngranular parameter sweep setup, systematic test-case definition, and automated\nexecution of the simulations. The candidate off-road autonomy algorithm is\nevaluated for satisfying requirements against a battery of 128 test cases,\nwhich is procedurally generated based on the test parameters (times of the day\nand weather conditions) and algorithmic variants (perception, planning, and\ncontrol sub-systems). Finally, the test results and key performance indicators\nare logged, and the test report is generated automatically. This then allows\nfor manual as well as automated data analysis with traceability and\ntractability across the digital thread.",
      "authors": [
        "Tanmay Vilas Samak",
        "Chinmay Vilas Samak",
        "Julia Brault",
        "Cori Harber",
        "Kirsten McCane",
        "Jonathon Smereka",
        "Mark Brudnak",
        "David Gorsich",
        "Venkat Krovi"
      ],
      "categories": [
        "cs.RO"
      ],
      "links": [
        "http://arxiv.org/abs/2503.13787v1",
        "http://arxiv.org/pdf/2503.13787v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2503.13778v1",
      "title": "Using 3D reconstruction from image motion to predict total leaf area in\n  dwarf tomato plants",
      "published": "2025-03-17T23:51:19Z",
      "updated": "2025-03-17T23:51:19Z",
      "summary": "Accurate estimation of total leaf area (TLA) is crucial for evaluating plant\ngrowth, photosynthetic activity, and transpiration. However, it remains\nchallenging for bushy plants like dwarf tomatoes due to their complex canopies.\nTraditional methods are often labor-intensive, damaging to plants, or limited\nin capturing canopy complexity. This study evaluated a non-destructive method\ncombining sequential 3D reconstructions from RGB images and machine learning to\nestimate TLA for three dwarf tomato cultivars: Mohamed, Hahms Gelbe Topftomate,\nand Red Robin -- grown under controlled greenhouse conditions. Two experiments\n(spring-summer and autumn-winter) included 73 plants, yielding 418 TLA\nmeasurements via an \"onion\" approach. High-resolution videos were recorded, and\n500 frames per plant were used for 3D reconstruction. Point clouds were\nprocessed using four algorithms (Alpha Shape, Marching Cubes, Poisson's, Ball\nPivoting), and meshes were evaluated with seven regression models:\nMultivariable Linear Regression, Lasso Regression, Ridge Regression, Elastic\nNet Regression, Random Forest, Extreme Gradient Boosting, and Multilayer\nPerceptron. The Alpha Shape reconstruction ($\\alpha = 3$) with Extreme Gradient\nBoosting achieved the best performance ($R^2 = 0.80$, $MAE = 489 cm^2$).\nCross-experiment validation showed robust results ($R^2 = 0.56$, $MAE = 579\ncm^2$). Feature importance analysis identified height, width, and surface area\nas key predictors. This scalable, automated TLA estimation method is suited for\nurban farming and precision agriculture, offering applications in automated\npruning, resource efficiency, and sustainable food production. The approach\ndemonstrated robustness across variable environmental conditions and canopy\nstructures.",
      "authors": [
        "Dmitrii Usenko",
        "David Helman",
        "Chen Giladi"
      ],
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "links": [
        "http://arxiv.org/abs/2503.13778v1",
        "http://arxiv.org/pdf/2503.13778v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2503.13773v1",
      "title": "Mitigating KV Cache Competition to Enhance User Experience in LLM\n  Inference",
      "published": "2025-03-17T23:38:29Z",
      "updated": "2025-03-17T23:38:29Z",
      "summary": "In Large Language Model (LLM) serving, the KV-cache (KVC) bottleneck causes\nhigh tail Time-to-First-Token (TTFT) and Time-Between-Tokens (TBT), impairing\nuser experience, particularly in time-sensitive applications. However,\nsatisfying both TTFT and TBT service-level objectives (SLOs) is challenging. To\naddress this, we propose a system, named CacheOPT for mitigating KV Cache\ncompetition, based on key insights from our measurements, incorporating novel\ncomponents. First, it estimates a request's output length, bounding the\ndeviation with a high specified probability, adjusted based on the request\narrival rate. Second, it allocates the estimated KVC demand to a request, and\nreuses other requests' allocated KVC to avoid preemptions while reducing\nwaiting time. Third, it proactively allocates KVC before instead of at the time\na request exhausts its allocation and reserves KVC globally to prevent\npreemptions. Fourth, it chooses a request that has long TBT SLO, long job\nremaining time and short preemption time to preempt. Fifth, it selects the\nshortest-latency strategy between swapping and recomputation for preemptions.\nExperiments show that CacheOPT achieves up to 3.29$\\times$ and 2.83$\\times$\nlower tail TBT and tail TTFT, 47\\% and 53\\% higher TTFT and TBT SLO\nattainments, and supports up to 1.58$\\times$ higher request arrival rate than\nthe state-of-the-art methods.",
      "authors": [
        "Haiying Shen",
        "Tanmoy Sen"
      ],
      "categories": [
        "cs.CL"
      ],
      "links": [
        "http://arxiv.org/abs/2503.13773v1",
        "http://arxiv.org/pdf/2503.13773v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2503.13772v1",
      "title": "Do Large Language Models Understand Performance Optimization?",
      "published": "2025-03-17T23:30:23Z",
      "updated": "2025-03-17T23:30:23Z",
      "summary": "Large Language Models (LLMs) have emerged as powerful tools for software\ndevelopment tasks such as code completion, translation, and optimization.\nHowever, their ability to generate efficient and correct code, particularly in\ncomplex High-Performance Computing (HPC) contexts, has remained underexplored.\nTo address this gap, this paper presents a comprehensive benchmark suite\nencompassing multiple critical HPC computational motifs to evaluate the\nperformance of code optimized by state-of-the-art LLMs, including OpenAI o1,\nClaude-3.5, and Llama-3.2. In addition to analyzing basic computational\nkernels, we developed an agent system that integrates LLMs to assess their\neffectiveness in real HPC applications. Our evaluation focused on key criteria\nsuch as execution time, correctness, and understanding of HPC-specific\nconcepts. We also compared the results with those achieved using traditional\nHPC optimization tools. Based on the findings, we recognized the strengths of\nLLMs in understanding human instructions and performing automated code\ntransformations. However, we also identified significant limitations, including\ntheir tendency to generate incorrect code and their challenges in comprehending\ncomplex control and data flows in sophisticated HPC code.",
      "authors": [
        "Bowen Cui",
        "Tejas Ramesh",
        "Oscar Hernandez",
        "Keren Zhou"
      ],
      "categories": [
        "cs.DC",
        "cs.SE"
      ],
      "links": [
        "http://arxiv.org/abs/2503.13772v1",
        "http://arxiv.org/pdf/2503.13772v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2503.13733v1",
      "title": "CoDet-M4: Detecting Machine-Generated Code in Multi-Lingual,\n  Multi-Generator and Multi-Domain Settings",
      "published": "2025-03-17T21:41:37Z",
      "updated": "2025-03-17T21:41:37Z",
      "summary": "Large language models (LLMs) have revolutionized code generation, automating\nprogramming with remarkable efficiency. However, these advancements challenge\nprogramming skills, ethics, and assessment integrity, making the detection of\nLLM-generated code essential for maintaining accountability and standards.\nWhile, there has been some research on this problem, it generally lacks domain\ncoverage and robustness, and only covers a small number of programming\nlanguages. To this end, we propose a framework capable of distinguishing\nbetween human- and LLM-written code across multiple programming languages, code\ngenerators, and domains. We use a large-scale dataset from renowned platforms\nand LLM-based code generators, alongside applying rigorous data quality checks,\nfeature engineering, and comparative analysis using evaluation of traditional\nmachine learning models, pre-trained language models (PLMs), and LLMs for code\ndetection. We perform an evaluation on out-of-domain scenarios, such as\ndetecting the authorship and hybrid authorship of generated code and\ngeneralizing to unseen models, domains, and programming languages. Moreover,\nour extensive experiments show that our framework effectively distinguishes\nhuman- from LLM-written code and sets a new benchmark for this task.",
      "authors": [
        "Daniil Orel",
        "Dilshod Azizov",
        "Preslav Nakov"
      ],
      "categories": [
        "cs.CL"
      ],
      "links": [
        "http://arxiv.org/abs/2503.13733v1",
        "http://arxiv.org/pdf/2503.13733v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2503.13705v2",
      "title": "Exploring the Potential of Carbon-Aware Execution for Scientific\n  Workflows",
      "published": "2025-03-17T20:24:20Z",
      "updated": "2025-03-19T09:48:27Z",
      "summary": "Scientific workflows are widely used to automate scientific data analysis and\noften involve processing large quantities of data on compute clusters. As such,\ntheir execution tends to be long-running and resource intensive, leading to\nsignificant energy consumption and carbon emissions.\n  Meanwhile, a wealth of carbon-aware computing methods have been proposed, yet\nlittle work has focused specifically on scientific workflows, even though they\npresent a substantial opportunity for carbon-aware computing because they are\ninherently delay tolerant, efficiently interruptible, and highly scalable.\n  In this study, we demonstrate the potential for carbon-aware workflow\nexecution. For this, we estimate the carbon footprint of two real-world\nNextflow workflows executed on cluster infrastructure. We use a linear power\nmodel for energy consumption estimates and real-world average and marginal CI\ndata for two regions. We evaluate the impact of carbon-aware temporal shifting,\npausing and resuming, and resource scaling. Our findings highlight significant\npotential for reducing emissions of workflows and workflow tasks.",
      "authors": [
        "Kathleen West",
        "Fabian Lehmann",
        "Vasilis Bountris",
        "Ulf Leser",
        "Yehia Elkhatib",
        "Lauritz Thamsen"
      ],
      "categories": [
        "cs.DC"
      ],
      "links": [
        "http://arxiv.org/abs/2503.13705v2",
        "http://arxiv.org/pdf/2503.13705v2"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2503.13693v1",
      "title": "Adapting to the Unknown: Training-Free Audio-Visual Event Perception\n  with Dynamic Thresholds",
      "published": "2025-03-17T20:06:48Z",
      "updated": "2025-03-17T20:06:48Z",
      "summary": "In the domain of audio-visual event perception, which focuses on the temporal\nlocalization and classification of events across distinct modalities (audio and\nvisual), existing approaches are constrained by the vocabulary available in\ntheir training data. This limitation significantly impedes their capacity to\ngeneralize to novel, unseen event categories. Furthermore, the annotation\nprocess for this task is labor-intensive, requiring extensive manual labeling\nacross modalities and temporal segments, limiting the scalability of current\nmethods. Current state-of-the-art models ignore the shifts in event\ndistributions over time, reducing their ability to adjust to changing video\ndynamics. Additionally, previous methods rely on late fusion to combine audio\nand visual information. While straightforward, this approach results in a\nsignificant loss of multimodal interactions. To address these challenges, we\npropose Audio-Visual Adaptive Video Analysis ($\\text{AV}^2\\text{A}$), a\nmodel-agnostic approach that requires no further training and integrates a\nscore-level fusion technique to retain richer multimodal interactions.\n$\\text{AV}^2\\text{A}$ also includes a within-video label shift algorithm,\nleveraging input video data and predictions from prior frames to dynamically\nadjust event distributions for subsequent frames. Moreover, we present the\nfirst training-free, open-vocabulary baseline for audio-visual event\nperception, demonstrating that $\\text{AV}^2\\text{A}$ achieves substantial\nimprovements over naive training-free baselines. We demonstrate the\neffectiveness of $\\text{AV}^2\\text{A}$ on both zero-shot and weakly-supervised\nstate-of-the-art methods, achieving notable improvements in performance metrics\nover existing approaches.",
      "authors": [
        "Eitan Shaar",
        "Ariel Shaulov",
        "Gal Chechik",
        "Lior Wolf"
      ],
      "categories": [
        "cs.CV"
      ],
      "links": [
        "http://arxiv.org/abs/2503.13693v1",
        "http://arxiv.org/pdf/2503.13693v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2503.13660v1",
      "title": "INPROVF: Leveraging Large Language Models to Repair High-level Robot\n  Controllers from Assumption Violations",
      "published": "2025-03-17T19:08:36Z",
      "updated": "2025-03-17T19:08:36Z",
      "summary": "This paper presents INPROVF, an automatic framework that combines large\nlanguage models (LLMs) and formal methods to speed up the repair process of\nhigh-level robot controllers. Previous approaches based solely on formal\nmethods are computationally expensive and cannot scale to large state spaces.\nIn contrast, INPROVF uses LLMs to generate repair candidates, and formal\nmethods to verify their correctness. To improve the quality of these\ncandidates, our framework first translates the symbolic representations of the\nenvironment and controllers into natural language descriptions. If a candidate\nfails the verification, INPROVF provides feedback on potential unsafe behaviors\nor unsatisfied tasks, and iteratively prompts LLMs to generate improved\nsolutions. We demonstrate the effectiveness of INPROVF through 12 violations\nwith various workspaces, tasks, and state space sizes.",
      "authors": [
        "Qian Meng",
        "Jin Peng Zhou",
        "Kilian Q. Weinberger",
        "Hadas Kress-Gazit"
      ],
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.FL",
        "cs.SY",
        "eess.SY"
      ],
      "links": [
        "http://arxiv.org/abs/2503.13660v1",
        "http://arxiv.org/pdf/2503.13660v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2503.13654v1",
      "title": "SOSecure: Safer Code Generation with RAG and StackOverflow Discussions",
      "published": "2025-03-17T19:03:36Z",
      "updated": "2025-03-17T19:03:36Z",
      "summary": "Large Language Models (LLMs) are widely used for automated code generation.\nTheir reliance on infrequently updated pretraining data leaves them unaware of\nnewly discovered vulnerabilities and evolving security standards, making them\nprone to producing insecure code. In contrast, developer communities on Stack\nOverflow (SO) provide an ever-evolving repository of knowledge, where security\nvulnerabilities are actively discussed and addressed through collective\nexpertise. These community-driven insights remain largely untapped by LLMs.\nThis paper introduces SOSecure, a Retrieval-Augmented Generation (RAG) system\nthat leverages the collective security expertise found in SO discussions to\nimprove the security of LLM-generated code. We build a security-focused\nknowledge base by extracting SO answers and comments that explicitly identify\nvulnerabilities. Unlike common uses of RAG, SOSecure triggers after code has\nbeen generated to find discussions that identify flaws in similar code. These\nare used in a prompt to an LLM to consider revising the code. Evaluation across\nthree datasets (SALLM, LLMSecEval, and LMSys) show that SOSecure achieves\nstrong fix rates of 71.7%, 91.3%, and 96.7% respectively, compared to prompting\nGPT-4 without relevant discussions (49.1%, 56.5%, and 37.5%), and outperforms\nmultiple other baselines. SOSecure operates as a language-agnostic complement\nto existing LLMs, without requiring retraining or fine-tuning, making it easy\nto deploy. Our results underscore the importance of maintaining active\ndeveloper forums, which have dropped substantially in usage with LLM adoptions.",
      "authors": [
        "Manisha Mukherjee",
        "Vincent J. Hellendoorn"
      ],
      "categories": [
        "cs.SE",
        "cs.CR"
      ],
      "links": [
        "http://arxiv.org/abs/2503.13654v1",
        "http://arxiv.org/pdf/2503.13654v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2503.13441v1",
      "title": "Humanoid Policy ~ Human Policy",
      "published": "2025-03-17T17:59:09Z",
      "updated": "2025-03-17T17:59:09Z",
      "summary": "Training manipulation policies for humanoid robots with diverse data enhances\ntheir robustness and generalization across tasks and platforms. However,\nlearning solely from robot demonstrations is labor-intensive, requiring\nexpensive tele-operated data collection which is difficult to scale. This paper\ninvestigates a more scalable data source, egocentric human demonstrations, to\nserve as cross-embodiment training data for robot learning. We mitigate the\nembodiment gap between humanoids and humans from both the data and modeling\nperspectives. We collect an egocentric task-oriented dataset (PH2D) that is\ndirectly aligned with humanoid manipulation demonstrations. We then train a\nhuman-humanoid behavior policy, which we term Human Action Transformer (HAT).\nThe state-action space of HAT is unified for both humans and humanoid robots\nand can be differentiably retargeted to robot actions. Co-trained with\nsmaller-scale robot data, HAT directly models humanoid robots and humans as\ndifferent embodiments without additional supervision. We show that human data\nimproves both generalization and robustness of HAT with significantly better\ndata collection efficiency. Code and data: https://human-as-robot.github.io/",
      "authors": [
        "Ri-Zhao Qiu",
        "Shiqi Yang",
        "Xuxin Cheng",
        "Chaitanya Chawla",
        "Jialong Li",
        "Tairan He",
        "Ge Yan",
        "Lars Paulsen",
        "Ge Yang",
        "Sha Yi",
        "Guanya Shi",
        "Xiaolong Wang"
      ],
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV"
      ],
      "links": [
        "http://arxiv.org/abs/2503.13441v1",
        "http://arxiv.org/pdf/2503.13441v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2503.13413v3",
      "title": "DLPO: Towards a Robust, Efficient, and Generalizable Prompt Optimization\n  Framework from a Deep-Learning Perspective",
      "published": "2025-03-17T17:42:51Z",
      "updated": "2025-03-19T14:18:01Z",
      "summary": "Large Language Models (LLMs) have achieved remarkable success across diverse\ntasks, largely driven by well-designed prompts. However, crafting and selecting\nsuch prompts often requires considerable human effort, significantly limiting\nits scalability. To mitigate this, recent studies have explored automated\nprompt optimization as a promising solution. Despite these efforts, existing\nmethods still face critical challenges in robustness, efficiency, and\ngeneralization. To systematically address these challenges, we first conduct an\nempirical analysis to identify the limitations of current reflection-based\nprompt optimization paradigm. Building on these insights, we propose 7\ninnovative approaches inspired by traditional deep learning paradigms for\nprompt optimization (DLPO), seamlessly integrating these concepts into\ntext-based gradient optimization. Through these advancements, we progressively\ntackle the aforementioned challenges and validate our methods through extensive\nexperimentation. We hope our study not only provides valuable guidance for\nfuture research but also offers a comprehensive understanding of the challenges\nand potential solutions in prompt optimization. Our code is available at\nhttps://github.com/sfasfaffa/DLPO.",
      "authors": [
        "Dengyun Peng",
        "Yuhang Zhou",
        "Qiguang Chen",
        "Jinhao Liu",
        "Jingjing Chen",
        "Libo Qin"
      ],
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "links": [
        "http://arxiv.org/abs/2503.13413v3",
        "http://arxiv.org/pdf/2503.13413v3"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2503.13389v1",
      "title": "Investigating the effect of CPT in lateral spreading prediction using\n  Explainable AI",
      "published": "2025-03-17T17:22:15Z",
      "updated": "2025-03-17T17:22:15Z",
      "summary": "This study proposes an autoencoder approach to extract latent features from\ncone penetration test profiles to evaluate the potential of incorporating CPT\ndata in an AI model. We employ autoencoders to compress 200 CPT profiles of\nsoil behavior type index (Ic) and normalized cone resistance (qc1Ncs) into ten\nlatent features while preserving critical information. We then utilize the\nextracted latent features with site parameters to train XGBoost models for\npredicting lateral spreading occurrences in the 2011 Christchurch earthquake.\nModels using the latent CPT features outperformed models with conventional CPT\nmetrics or no CPT data, achieving over 83% accuracy. Explainable AI revealed\nthe most crucial latent feature corresponding to soil behavior between 1-3\nmeter depths, highlighting this depth range's criticality for liquefaction\nevaluation. The autoencoder approach provides an automated technique for\ncondensing CPT profiles into informative latent features for machine-learning\nliquefaction models.",
      "authors": [
        "Cheng-Hsi Hsiao",
        "Ellen Rathje",
        "Krishna Kumar"
      ],
      "categories": [
        "cs.LG",
        "physics.geo-ph"
      ],
      "links": [
        "http://arxiv.org/abs/2503.13389v1",
        "http://arxiv.org/pdf/2503.13389v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2503.13580v1",
      "title": "LLM Test Generation via Iterative Hybrid Program Analysis",
      "published": "2025-03-17T16:10:38Z",
      "updated": "2025-03-17T16:10:38Z",
      "summary": "Automating unit test generation remains a significant challenge, particularly\nfor complex methods in real-world projects. While Large Language Models (LLMs)\nhave made strides in code generation, they struggle to achieve high branch\ncoverage due to their limited ability to reason about intricate control flow\nstructures. To address this limitation, we introduce Panta, a technique that\nemulates the iterative process human developers follow when analyzing code and\nconstructing test cases. Panta integrates static control flow analysis and\ndynamic code coverage analysis to systematically guide LLMs in identifying\nuncovered execution paths and generating better test cases. By incorporating an\niterative feedback-driven mechanism, our technique continuously refines test\ngeneration based on static and dynamic path coverage insights, ensuring more\ncomprehensive and effective testing. Our empirical evaluation, conducted on\nclasses with high cyclomatic complexity from open-source projects, demonstrates\nthat Panta achieves 26% higher line coverage and 23% higher branch coverage\ncompared to the state-of-the-art.",
      "authors": [
        "Sijia Gu",
        "Noor Nashid",
        "Ali Mesbah"
      ],
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "links": [
        "http://arxiv.org/abs/2503.13580v1",
        "http://arxiv.org/pdf/2503.13580v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2503.13301v1",
      "title": "LIMCA: LLM for Automating Analog In-Memory Computing Architecture Design\n  Exploration",
      "published": "2025-03-17T15:45:17Z",
      "updated": "2025-03-17T15:45:17Z",
      "summary": "Resistive crossbars enabling analog In-Memory Computing (IMC) have emerged as\na promising architecture for Deep Neural Network (DNN) acceleration, offering\nhigh memory bandwidth and in-situ computation. However, the manual,\nknowledge-intensive design process and the lack of high-quality circuit\nnetlists have significantly constrained design space exploration and\noptimization to behavioral system-level tools. In this work, we introduce\nLIMCA, a novel fine-tune-free Large Language Model (LLM)-driven framework for\nautomating the design and evaluation of IMC crossbar architectures. Unlike\ntraditional approaches, LIMCA employs a No-Human-In-Loop (NHIL) automated\npipeline to generate and validate circuit netlists for SPICE simulations,\neliminating manual intervention. LIMCA systematically explores the IMC design\nspace by leveraging a structured dataset and LLM-based performance evaluation.\nOur experimental results on MNIST classification demonstrate that LIMCA\nsuccessfully generates crossbar designs achieving $\\geq$96% accuracy while\nmaintaining a power consumption $\\leq$3W, making this the first work in\nLLM-assisted IMC design space exploration. Compared to existing frameworks,\nLIMCA provides an automated, scalable, and hardware-aware solution, reducing\ndesign exploration time while ensuring user-constrained performance trade-offs.",
      "authors": [
        "Deepak Vungarala",
        "Md Hasibul Amin",
        "Pietro Mercati",
        "Arnob Ghosh",
        "Arman Roohi",
        "Ramtin Zand",
        "Shaahin Angizi"
      ],
      "categories": [
        "cs.AR"
      ],
      "links": [
        "http://arxiv.org/abs/2503.13301v1",
        "http://arxiv.org/pdf/2503.13301v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2503.13226v1",
      "title": "Auto-Configuring Entity Resolution Pipelines",
      "published": "2025-03-17T14:41:37Z",
      "updated": "2025-03-17T14:41:37Z",
      "summary": "The same real-world entity (e.g., a movie, a restaurant, a person) may be\ndescribed in various ways on different datasets. Entity Resolution (ER) aims to\nfind such different descriptions of the same entity, this way improving data\nquality and, therefore, data value. However, an ER pipeline typically involves\nseveral steps (e.g., blocking, similarity estimation, clustering), with each\nstep requiring its own configurations and tuning. The choice of the best\nconfiguration, among a vast number of possible combinations, is a\ndataset-specific and labor-intensive task both for novice and expert users,\nwhile it often requires some ground truth knowledge of real matches. In this\nwork, we examine ways of automatically configuring a state of-the-art\nend-to-end ER pipeline based on pre-trained language models under two settings:\n(i) When ground truth is available. In this case, sampling strategies that are\ntypically used for hyperparameter optimization can significantly restrict the\nsearch of the configuration space. We experimentally compare their relative\neffectiveness and time efficiency, applying them to ER pipelines for the first\ntime. (ii) When no ground truth is available. In this case, labelled data\nextracted from other datasets with available ground truth can be used to train\na regression model that predicts the relative effectiveness of parameter\nconfigurations. Experimenting with 11 ER benchmark datasets, we evaluate the\nrelative performance of existing techniques that address each problem, but have\nnot been applied to ER before.",
      "authors": [
        "Konstantinos Nikoletos",
        "Vasilis Efthymiou",
        "George Papadakis",
        "Kostas Stefanidis"
      ],
      "categories": [
        "cs.DB"
      ],
      "links": [
        "http://arxiv.org/abs/2503.13226v1",
        "http://arxiv.org/pdf/2503.13226v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2503.14541v1",
      "title": "Regulating Ai In Financial Services: Legal Frameworks And Compliance\n  Challenges",
      "published": "2025-03-17T14:29:09Z",
      "updated": "2025-03-17T14:29:09Z",
      "summary": "This article examines the evolving landscape of artificial intelligence (AI)\nregulation in financial services, detailing the legal frameworks and compliance\nchallenges posed by rapid technological adoption. By reviewing current\nlegislation, industry guidelines, and real-world use cases, it highlights how\nAI-driven processes, from fraud detection to algorithmic trading, offer\nefficiency gains yet introduce significant risks, including algorithmic bias,\ndata privacy breaches, and lack of transparency in automated decision-making.\nThe study compares regulatory approaches across major jurisdictions such as the\nEuropean Union, United States, and United Kingdom, identifying both universal\nconcerns, like the need for explainability and robust data protection, and\nregion-specific compliance requirements that impact the implementation of\nhigh-risk AI applications. Additionally, it underscores emerging areas of\nfocus, such as liability for AI-driven errors, systemic risks posed by\ninterlinked AI systems, and the ethical considerations of technology-driven\nfinancial exclusion. The findings reveal gaps in existing rules and emphasize\nthe necessity for adaptive, technology-neutral policies capable of fostering\ninnovation while safeguarding consumer rights and market integrity. The article\nconcludes by proposing a principled regulatory model that balances flexibility\nwith enforceable standards, advocating closer collaboration between\npolicymakers, financial institutions, and AI developers to ensure a secure,\nfair, and forward-looking framework for AI in finance.",
      "authors": [
        "Shahmar Mirishli"
      ],
      "categories": [
        "cs.CY",
        "q-fin.GN"
      ],
      "links": [
        "http://arxiv.org/abs/2503.14541v1",
        "http://arxiv.org/pdf/2503.14541v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2503.14538v1",
      "title": "Vision-Language Models for Acute Tuberculosis Diagnosis: A Multimodal\n  Approach Combining Imaging and Clinical Data",
      "published": "2025-03-17T14:08:35Z",
      "updated": "2025-03-17T14:08:35Z",
      "summary": "Background: This study introduces a Vision-Language Model (VLM) leveraging\nSIGLIP and Gemma-3b architectures for automated acute tuberculosis (TB)\nscreening. By integrating chest X-ray images and clinical notes, the model aims\nto enhance diagnostic accuracy and efficiency, particularly in resource-limited\nsettings.\n  Methods: The VLM combines visual data from chest X-rays with clinical context\nto generate detailed, context-aware diagnostic reports. The architecture\nemploys SIGLIP for visual encoding and Gemma-3b for decoding, ensuring\neffective representation of acute TB-specific pathologies and clinical\ninsights.\n  Results: Key acute TB pathologies, including consolidation, cavities, and\nnodules, were detected with high precision (97percent) and recall (96percent).\nThe model demonstrated strong spatial localization capabilities and robustness\nin distinguishing TB-positive cases, making it a reliable tool for acute TB\ndiagnosis.\n  Conclusion: The multimodal capability of the VLM reduces reliance on\nradiologists, providing a scalable solution for acute TB screening. Future work\nwill focus on improving the detection of subtle pathologies and addressing\ndataset biases to enhance its generalizability and application in diverse\nglobal healthcare settings.",
      "authors": [
        "Ananya Ganapthy",
        "Praveen Shastry",
        "Naveen Kumarasami",
        "Anandakumar D",
        "Keerthana R",
        "Mounigasri M",
        "Varshinipriya M",
        "Kishore Prasath Venkatesh",
        "Bargava Subramanian",
        "Kalyan Sivasailam"
      ],
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV",
        "cs.LG",
        "68T07, 68T45, 92C55, 92C50, 68U10"
      ],
      "links": [
        "http://arxiv.org/abs/2503.14538v1",
        "http://arxiv.org/pdf/2503.14538v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2503.13171v1",
      "title": "HybridGen: VLM-Guided Hybrid Planning for Scalable Data Generation of\n  Imitation Learning",
      "published": "2025-03-17T13:49:43Z",
      "updated": "2025-03-17T13:49:43Z",
      "summary": "The acquisition of large-scale and diverse demonstration data are essential\nfor improving robotic imitation learning generalization. However, generating\nsuch data for complex manipulations is challenging in real-world settings. We\nintroduce HybridGen, an automated framework that integrates Vision-Language\nModel (VLM) and hybrid planning. HybridGen uses a two-stage pipeline: first,\nVLM to parse expert demonstrations, decomposing tasks into expert-dependent\n(object-centric pose transformations for precise control) and plannable\nsegments (synthesizing diverse trajectories via path planning); second, pose\ntransformations substantially expand the first-stage data. Crucially, HybridGen\ngenerates a large volume of training data without requiring specific data\nformats, making it broadly applicable to a wide range of imitation learning\nalgorithms, a characteristic which we also demonstrate empirically across\nmultiple algorithms. Evaluations across seven tasks and their variants\ndemonstrate that agents trained with HybridGen achieve substantial performance\nand generalization gains, averaging a 5% improvement over state-of-the-art\nmethods. Notably, in the most challenging task variants, HybridGen achieves\nsignificant improvement, reaching a 59.7% average success rate, significantly\noutperforming Mimicgen's 49.5%. These results demonstrating its effectiveness\nand practicality.",
      "authors": [
        "Wensheng Wang",
        "Ning Tan"
      ],
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "links": [
        "http://arxiv.org/abs/2503.13171v1",
        "http://arxiv.org/pdf/2503.13171v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2503.14536v1",
      "title": "Advancing Chronic Tuberculosis Diagnostics Using Vision-Language Models:\n  A Multi modal Framework for Precision Analysis",
      "published": "2025-03-17T13:49:29Z",
      "updated": "2025-03-17T13:49:29Z",
      "summary": "Background This study proposes a Vision-Language Model (VLM) leveraging the\nSIGLIP encoder and Gemma-3b transformer decoder to enhance automated chronic\ntuberculosis (TB) screening. By integrating chest X-ray images with clinical\ndata, the model addresses the challenges of manual interpretation, improving\ndiagnostic consistency and accessibility, particularly in resource-constrained\nsettings.\n  Methods The VLM architecture combines a Vision Transformer (ViT) for visual\nencoding and a transformer-based text encoder to process clinical context, such\nas patient histories and treatment records. Cross-modal attention mechanisms\nalign radiographic features with textual information, while the Gemma-3b\ndecoder generates comprehensive diagnostic reports. The model was pre-trained\non 5 million paired medical images and texts and fine-tuned using 100,000\nchronic TB-specific chest X-rays.\n  Results The model demonstrated high precision (94 percent) and recall (94\npercent) for detecting key chronic TB pathologies, including fibrosis,\ncalcified granulomas, and bronchiectasis. Area Under the Curve (AUC) scores\nexceeded 0.93, and Intersection over Union (IoU) values were above 0.91,\nvalidating its effectiveness in detecting and localizing TB-related\nabnormalities.\n  Conclusion The VLM offers a robust and scalable solution for automated\nchronic TB diagnosis, integrating radiographic and clinical data to deliver\nactionable and context-aware insights. Future work will address subtle\npathologies and dataset biases to enhance the model's generalizability,\nensuring equitable performance across diverse populations and healthcare\nsettings.",
      "authors": [
        "Praveen Shastry",
        "Sowmya Chowdary Muthulur",
        "Naveen Kumarasami",
        "Anandakumar D",
        "Mounigasri M",
        "Keerthana R",
        "Kishore Prasath Venkatesh",
        "Bargava Subramanian",
        "Kalyan Sivasailam",
        "Revathi Ezhumalai",
        "Abitha Marimuthu"
      ],
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV",
        "cs.LG",
        "68T07, 92C55, 68U10, 92C50, 60G35"
      ],
      "links": [
        "http://arxiv.org/abs/2503.14536v1",
        "http://arxiv.org/pdf/2503.14536v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2503.13156v1",
      "title": "DynSTG-Mamba: Dynamic Spatio-Temporal Graph Mamba with Cross-Graph\n  Knowledge Distillation for Gait Disorders Recognition",
      "published": "2025-03-17T13:26:47Z",
      "updated": "2025-03-17T13:26:47Z",
      "summary": "Gait disorder recognition plays a crucial role in the early diagnosis and\nmonitoring of movement disorders. Existing approaches, including\nspatio-temporal graph convolutional networks (ST-GCNs), often face high memory\ndemands and struggle to capture complex spatio-temporal dependencies, limiting\ntheir efficiency in clinical applications. To address these challenges, we\nintroduce DynSTG-Mamba (Dynamic Spatio-Temporal Graph Mamba), a novel framework\nthat combines DF-STGNN and STG-Mamba to enhance motion sequence modeling. The\nDF-STGNN incorporates a dynamic spatio-temporal filter that adaptively adjusts\nspatial connections between skeletal joints and temporal interactions across\ndifferent movement phases. This approach ensures better feature propagation\nthrough dynamic graph structures by considering the hierarchical nature and\ndynamics of skeletal gait data. Meanwhile, STG-Mamba, an extension of Mamba\nadapted for skeletal motion data, ensures a continuous propagation of states,\nfacilitating the capture of long-term dependencies while reducing computational\ncomplexity. To reduce the number of model parameters and computational costs\nwhile maintaining consistency, we propose Cross-Graph Relational Knowledge\nDistillation, a novel knowledge transfer mechanism that aligns relational\ninformation between teacher (large architecture) and student models (small\narchitecture) while using shared memory. This ensures that the interactions and\nmovement patterns of the joints are accurately preserved in the motion\nsequences. We validate our DynSTG-Mamba on KOA-NM, PD-WALK, and ATAXIA\ndatasets, where it outperforms state-of-the-art approaches by achieving in\nterms of Accuracy, F1-score, and Recall. Our results highlight the efficiency\nand robustness of our approach, offering a lightweight yet highly accurate\nsolution for automated gait analysis and movement disorder assessment.",
      "authors": [
        "Zakariae Zrimek",
        "Youssef Mourchid",
        "Mohammed El Hassouni"
      ],
      "categories": [
        "cs.CV"
      ],
      "links": [
        "http://arxiv.org/abs/2503.13156v1",
        "http://arxiv.org/pdf/2503.13156v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2503.13127v1",
      "title": "An Online Integrated Development Environment for Automated Programming\n  Assessment Systems",
      "published": "2025-03-17T12:50:51Z",
      "updated": "2025-03-17T12:50:51Z",
      "summary": "The increasing demand for programmers has led to a surge in participants in\nprogramming courses, making it increasingly challenging for instructors to\nassess student code manually. As a result, automated programming assessment\nsystems (APASs) have been developed to streamline this process. These APASs\nsupport lecturers by managing and evaluating student programming exercises at\nscale. However, these tools often do not provide feature-rich online editors\ncompared to their traditional integrated development environments (IDEs)\ncounterparts. This absence of key features, such as syntax highlighting and\nautocompletion, can negatively impact the learning experience, as these tools\nare crucial for effective coding practice. To address this gap, this research\ncontributes to the field of programming education by extracting and defining\nrequirements for an online IDE in an educational context and presenting a\nprototypical implementation of an open-source solution for a scalable and\nsecure online IDE. The usability of the new online IDE was assessed using the\nTechnology Acceptance Model (TAM), gathering feedback from 27 first-year\nstudents through a structured survey. In addition to these qualitative\ninsights, quantitative measures such as memory (RAM) usage were evaluated to\ndetermine the efficiency and scalability of the tool under varying usage\nconditions.",
      "authors": [
        "Eduard Frankford",
        "Daniel Crazzolara",
        "Michael Vierhauser",
        "Niklas Meissner",
        "Stephan Krusche",
        "Ruth Breu"
      ],
      "categories": [
        "cs.SE"
      ],
      "links": [
        "http://arxiv.org/abs/2503.13127v1",
        "http://arxiv.org/pdf/2503.13127v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2503.13119v1",
      "title": "OSLO-IC: On-the-Sphere Learned Omnidirectional Image Compression with\n  Attention Modules and Spatial Context",
      "published": "2025-03-17T12:46:14Z",
      "updated": "2025-03-17T12:46:14Z",
      "summary": "Developing effective 360-degree (spherical) image compression techniques is\ncrucial for technologies like virtual reality and automated driving. This paper\nadvances the state-of-the-art in on-the-sphere learning (OSLO) for\nomnidirectional image compression framework by proposing spherical attention\nmodules, residual blocks, and a spatial autoregressive context model. These\nimprovements achieve a 23.1% bit rate reduction in terms of WS-PSNR BD rate.\nAdditionally, we introduce a spherical transposed convolution operator for\nupsampling, which reduces trainable parameters by a factor of four compared to\nthe pixel shuffling used in the OSLO framework, while maintaining similar\ncompression performance. Therefore, in total, our proposed method offers\nsignificant rate savings with a smaller architecture and can be applied to any\nspherical convolutional application.",
      "authors": [
        "Paul Wawerek-L\u00f3pez",
        "Navid Mahmoudian Bidgoli",
        "Pascal Frossard",
        "Andr\u00e9 Kaup",
        "Thomas Maugey"
      ],
      "categories": [
        "eess.IV"
      ],
      "links": [
        "http://arxiv.org/abs/2503.13119v1",
        "http://arxiv.org/pdf/2503.13119v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2503.13081v1",
      "title": "A Framework to Assess Multilingual Vulnerabilities of LLMs",
      "published": "2025-03-17T11:39:44Z",
      "updated": "2025-03-17T11:39:44Z",
      "summary": "Large Language Models (LLMs) are acquiring a wider range of capabilities,\nincluding understanding and responding in multiple languages. While they\nundergo safety training to prevent them from answering illegal questions,\nimbalances in training data and human evaluation resources can make these\nmodels more susceptible to attacks in low-resource languages (LRL). This paper\nproposes a framework to automatically assess the multilingual vulnerabilities\nof commonly used LLMs. Using our framework, we evaluated six LLMs across eight\nlanguages representing varying levels of resource availability. We validated\nthe assessments generated by our automated framework through human evaluation\nin two languages, demonstrating that the framework's results align with human\njudgments in most cases. Our findings reveal vulnerabilities in LRL; however,\nthese may pose minimal risk as they often stem from the model's poor\nperformance, resulting in incoherent responses.",
      "authors": [
        "Likai Tang",
        "Niruth Bogahawatta",
        "Yasod Ginige",
        "Jiarui Xu",
        "Shixuan Sun",
        "Surangika Ranathunga",
        "Suranga Seneviratne"
      ],
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "links": [
        "http://arxiv.org/abs/2503.13081v1",
        "http://arxiv.org/pdf/2503.13081v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2503.13028v1",
      "title": "Beyond Role-Based Surgical Domain Modeling: Generalizable\n  Re-Identification in the Operating Room",
      "published": "2025-03-17T10:30:26Z",
      "updated": "2025-03-17T10:30:26Z",
      "summary": "Surgical domain models improve workflow optimization through automated\npredictions of each staff member's surgical role. However, mounting evidence\nindicates that team familiarity and individuality impact surgical outcomes. We\npresent a novel staff-centric modeling approach that characterizes individual\nteam members through their distinctive movement patterns and physical\ncharacteristics, enabling long-term tracking and analysis of surgical personnel\nacross multiple procedures. To address the challenge of inter-clinic\nvariability, we develop a generalizable re-identification framework that\nencodes sequences of 3D point clouds to capture shape and articulated motion\npatterns unique to each individual. Our method achieves 86.19% accuracy on\nrealistic clinical data while maintaining 75.27% accuracy when transferring\nbetween different environments - a 12% improvement over existing methods. When\nused to augment markerless personnel tracking, our approach improves accuracy\nby over 50%. Through extensive validation across three datasets and the\nintroduction of a novel workflow visualization technique, we demonstrate how\nour framework can reveal novel insights into surgical team dynamics and space\nutilization patterns, advancing methods to analyze surgical workflows and team\ncoordination.",
      "authors": [
        "Tony Danjun Wang",
        "Lennart Bastian",
        "Tobias Czempiel",
        "Christian Heiliger",
        "Nassir Navab"
      ],
      "categories": [
        "cs.CV",
        "J.3"
      ],
      "links": [
        "http://arxiv.org/abs/2503.13028v1",
        "http://arxiv.org/pdf/2503.13028v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2503.12989v1",
      "title": "A Multi-Stage Framework with Taxonomy-Guided Reasoning for Occupation\n  Classification Using Large Language Models",
      "published": "2025-03-17T09:44:50Z",
      "updated": "2025-03-17T09:44:50Z",
      "summary": "Automatically annotating job data with standardized occupations from\ntaxonomies, known as occupation classification, is crucial for labor market\nanalysis. However, this task is often hindered by data scarcity and the\nchallenges of manual annotations. While large language models (LLMs) hold\npromise due to their extensive world knowledge and in-context learning\ncapabilities, their effectiveness depends on their knowledge of occupational\ntaxonomies, which remains unclear. In this study, we assess the ability of LLMs\nto generate precise taxonomic entities from taxonomy, highlighting their\nlimitations. To address these challenges, we propose a multi-stage framework\nconsisting of inference, retrieval, and reranking stages, which integrates\ntaxonomy-guided reasoning examples to enhance performance by aligning outputs\nwith taxonomic knowledge. Evaluations on a large-scale dataset show significant\nimprovements in classification accuracy. Furthermore, we demonstrate the\nframework's adaptability for multi-label skill classification. Our results\nindicate that the framework outperforms existing LLM-based methods, offering a\npractical and scalable solution for occupation classification and related tasks\nacross LLMs.",
      "authors": [
        "Palakorn Achananuparp",
        "Ee-Peng Lim"
      ],
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.SI"
      ],
      "links": [
        "http://arxiv.org/abs/2503.12989v1",
        "http://arxiv.org/pdf/2503.12989v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2503.12978v1",
      "title": "Enhancing Job Salary Prediction with Disentangled Composition Effect\n  Modeling: A Neural Prototyping Approach",
      "published": "2025-03-17T09:36:07Z",
      "updated": "2025-03-17T09:36:07Z",
      "summary": "In the era of the knowledge economy, understanding how job skills influence\nsalary is crucial for promoting recruitment with competitive salary systems and\naligned salary expectations. Despite efforts on salary prediction based on job\npositions and talent demographics, there still lacks methods to effectively\ndiscern the set-structured skills' intricate composition effect on job salary.\nWhile recent advances in neural networks have significantly improved accurate\nset-based quantitative modeling, their lack of explainability hinders obtaining\ninsights into the skills' composition effects. Indeed, model explanation for\nset data is challenging due to the combinatorial nature, rich semantics, and\nunique format. To this end, in this paper, we propose a novel intrinsically\nexplainable set-based neural prototyping approach, namely \\textbf{LGDESetNet},\nfor explainable salary prediction that can reveal disentangled skill sets that\nimpact salary from both local and global perspectives. Specifically, we propose\na skill graph-enhanced disentangled discrete subset selection layer to identify\nmulti-faceted influential input subsets with varied semantics. Furthermore, we\npropose a set-oriented prototype learning method to extract globally\ninfluential prototypical sets. The resulting output is transparently derived\nfrom the semantic interplay between these input subsets and global prototypes.\nExtensive experiments on four real-world datasets demonstrate that our method\nachieves superior performance than state-of-the-art baselines in salary\nprediction while providing explainable insights into salary-influencing\npatterns.",
      "authors": [
        "Yang Ji",
        "Ying Sun",
        "Hengshu Zhu"
      ],
      "categories": [
        "cs.LG"
      ],
      "links": [
        "http://arxiv.org/abs/2503.12978v1",
        "http://arxiv.org/pdf/2503.12978v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2503.12875v1",
      "title": "An interpretable approach to automating the assessment of biofouling in\n  video footage",
      "published": "2025-03-17T07:11:13Z",
      "updated": "2025-03-17T07:11:13Z",
      "summary": "Biofouling$\\unicode{x2013}$communities of organisms that grow on hard\nsurfaces immersed in water$\\unicode{x2013}$provides a pathway for the spread of\ninvasive marine species and diseases. To address this risk, international\nvessels are increasingly being obligated to provide evidence of their\nbiofouling management practices. Verification that these activities are\neffective requires underwater inspections, using divers or underwater remotely\noperated vehicles (ROVs), and the collection and analysis of large amounts of\nimagery and footage. Automated assessment using computer vision techniques can\nsignificantly streamline this process, and this work shows how this challenge\ncan be addressed efficiently and effectively using the interpretable Component\nFeatures (ComFe) approach with a DINOv2 Vision Transformer (ViT) foundation\nmodel. ComFe is able to obtain improved performance in comparison to previous\nnon-interpretable Convolutional Neural Network (CNN) methods, with\nsignificantly fewer weights and greater transparency$\\unicode{x2013}$through\nidentifying which regions of the image contribute to the classification, and\nwhich images in the training data lead to that conclusion. All code, data and\nmodel weights are publicly released.",
      "authors": [
        "Evelyn J. Mannix",
        "Bartholomew A. Woodham"
      ],
      "categories": [
        "cs.CV"
      ],
      "links": [
        "http://arxiv.org/abs/2503.12875v1",
        "http://arxiv.org/pdf/2503.12875v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2503.12873v1",
      "title": "SeeAction: Towards Reverse Engineering How-What-Where of HCI Actions\n  from Screencasts for UI Automation",
      "published": "2025-03-17T07:07:38Z",
      "updated": "2025-03-17T07:07:38Z",
      "summary": "UI automation is a useful technique for UI testing, bug reproduction, and\nrobotic process automation. Recording user actions with an application assists\nrapid development of UI automation scripts, but existing recording techniques\nare intrusive, rely on OS or GUI framework accessibility support, or assume\nspecific app implementations. Reverse engineering user actions from screencasts\nis non-intrusive, but a key reverse-engineering step is currently missing -\nrecognizing human-understandable structured user actions ([command] [widget]\n[location]) from action screencasts. To fill the gap, we propose a deep\nlearning-based computer vision model that can recognize 11 commands and 11\nwidgets, and generate location phrases from action screencasts, through joint\nlearning and multi-task learning. We label a large dataset with 7260\nvideo-action pairs, which record user interactions with Word, Zoom, Firefox,\nPhotoshop, and Windows 10 Settings. Through extensive experiments, we confirm\nthe effectiveness and generality of our model, and demonstrate the usefulness\nof a screencast-to-action-script tool built upon our model for bug\nreproduction.",
      "authors": [
        "Dehai Zhao",
        "Zhenchang Xing",
        "Qinghua Lu",
        "Xiwei Xu",
        "Liming Zhu"
      ],
      "categories": [
        "cs.SE"
      ],
      "links": [
        "http://arxiv.org/abs/2503.12873v1",
        "http://arxiv.org/pdf/2503.12873v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2503.12852v1",
      "title": "ACT360: An Efficient 360-Degree Action Detection and Summarization\n  Framework for Mission-Critical Training and Debriefing",
      "published": "2025-03-17T06:12:36Z",
      "updated": "2025-03-17T06:12:36Z",
      "summary": "Effective training and debriefing are critical in high-stakes,\nmission-critical environments such as disaster response, military simulations,\nand industrial safety, where precision and minimizing errors are paramount. The\ntraditional post-training analysis relies on manually reviewing 2D videos, a\ntime-consuming process that lacks comprehensive situational awareness. To\naddress these limitations, we introduce ACT360, a system that leverages\n360-degree videos and machine learning for automated action detection and\nstructured debriefing. ACT360 integrates 360YOWO, an enhanced You Only Watch\nOnce (YOWO) model with spatial attention and equirectangular-aware convolution\n(EAC) to mitigate panoramic video distortions. To enable deployment in\nresource-constrained environments, we apply quantization and model pruning,\nreducing the model size by 74% while maintaining robust accuracy (mAP drop of\nonly 1.5%, from 0.865 to 0.850) and improving inference speed. We validate our\napproach on a publicly available dataset of 55 labeled 360-degree videos\ncovering seven key operational actions, recorded across various real-world\ntraining sessions and environmental conditions. Additionally, ACT360 integrates\n360AIE (Action Insight Explorer), a web-based interface for automatic action\ndetection, retrieval, and textual summarization using large language models\n(LLMs), significantly enhancing post-incident analysis efficiency. ACT360\nserves as a generalized framework for mission-critical debriefing,\nincorporating EAC, spatial attention, summarization, and model optimization.\nThese innovations apply to any training environment requiring lightweight\naction detection and structured post-exercise analysis.",
      "authors": [
        "Aditi Tiwari",
        "Klara Nahrstedt"
      ],
      "categories": [
        "cs.CV",
        "cs.MM"
      ],
      "links": [
        "http://arxiv.org/abs/2503.12852v1",
        "http://arxiv.org/pdf/2503.12852v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2503.12844v1",
      "title": "GuideDog: A Real-World Egocentric Multimodal Dataset for Blind and\n  Low-Vision Accessibility-Aware Guidance",
      "published": "2025-03-17T05:43:40Z",
      "updated": "2025-03-17T05:43:40Z",
      "summary": "Mobility remains a significant challenge for the 2.2 billion people worldwide\naffected by blindness and low vision (BLV), with 7% of visually impaired\nindividuals experiencing falls at least once a month. While recent advances in\nMultimodal Large Language Models (MLLMs) offer promising opportunities for BLV\nassistance, their development has been hindered by limited datasets. This\nlimitation stems from the fact that BLV-aware annotation requires specialized\ndomain knowledge and intensive labor. To address this gap, we introduce\nGuideDog, a novel accessibility-aware guide dataset containing 22K\nimage-description pairs (including 2K human-annotated pairs) that capture\ndiverse real-world scenes from a pedestrian's viewpoint. Our approach shifts\nthe annotation burden from generation to verification through a collaborative\nhuman-AI framework grounded in established accessibility standards,\nsignificantly improving efficiency while maintaining high-quality annotations.\nWe also develop GuideDogQA, a subset of 818 samples featuring multiple-choice\nquestions designed to evaluate fine-grained visual perception capabilities,\nspecifically object recognition and relative depth perception. Our experimental\nresults highlight the importance of accurate spatial understanding for\neffective BLV guidance. GuideDog and GuideDogQA will advance research in\nMLLM-based assistive technologies for BLV individuals while contributing to\nbroader applications in understanding egocentric scenes for robotics and\naugmented reality. The code and dataset will be publicly available.",
      "authors": [
        "Junhyeok Kim",
        "Jaewoo Park",
        "Junhee Park",
        "Sangeyl Lee",
        "Jiwan Chung",
        "Jisung Kim",
        "Ji Hoon Joung",
        "Youngjae Yu"
      ],
      "categories": [
        "cs.CV"
      ],
      "links": [
        "http://arxiv.org/abs/2503.12844v1",
        "http://arxiv.org/pdf/2503.12844v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2503.12797v2",
      "title": "DeepPerception: Advancing R1-like Cognitive Visual Perception in MLLMs\n  for Knowledge-Intensive Visual Grounding",
      "published": "2025-03-17T04:06:34Z",
      "updated": "2025-03-18T05:06:22Z",
      "summary": "Human experts excel at fine-grained visual discrimination by leveraging\ndomain knowledge to refine perceptual features, a capability that remains\nunderdeveloped in current Multimodal Large Language Models (MLLMs). Despite\npossessing vast expert-level knowledge, MLLMs struggle to integrate reasoning\ninto visual perception, often generating direct responses without deeper\nanalysis. To bridge this gap, we introduce knowledge-intensive visual grounding\n(KVG), a novel visual grounding task that requires both fine-grained perception\nand domain-specific knowledge integration. To address the challenges of KVG, we\npropose DeepPerception, an MLLM enhanced with cognitive visual perception\ncapabilities. Our approach consists of (1) an automated data synthesis pipeline\nthat generates high-quality, knowledge-aligned training samples, and (2) a\ntwo-stage training framework combining supervised fine-tuning for cognitive\nreasoning scaffolding and reinforcement learning to optimize\nperception-cognition synergy. To benchmark performance, we introduce KVG-Bench\na comprehensive dataset spanning 10 domains with 1.3K manually curated test\ncases. Experimental results demonstrate that DeepPerception significantly\noutperforms direct fine-tuning, achieving +8.08\\% accuracy improvements on\nKVG-Bench and exhibiting +4.60\\% superior cross-domain generalization over\nbaseline approaches. Our findings highlight the importance of integrating\ncognitive processes into MLLMs for human-like visual perception and open new\ndirections for multimodal reasoning research. The data, codes, and models are\nreleased at https://github.com/thunlp/DeepPerception.",
      "authors": [
        "Xinyu Ma",
        "Ziyang Ding",
        "Zhicong Luo",
        "Chi Chen",
        "Zonghao Guo",
        "Derek F. Wong",
        "Xiaoyi Feng",
        "Maosong Sun"
      ],
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "links": [
        "http://arxiv.org/abs/2503.12797v2",
        "http://arxiv.org/pdf/2503.12797v2"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2503.12679v1",
      "title": "Discovering uncertainty: Gaussian constitutive neural networks with\n  correlated weights",
      "published": "2025-03-16T22:34:16Z",
      "updated": "2025-03-16T22:34:16Z",
      "summary": "When characterizing materials, it can be important to not only predict their\nmechanical properties, but also to estimate the probability distribution of\nthese properties across a set of samples. Constitutive neural networks allow\nfor the automated discovery of constitutive models that exactly satisfy\nphysical laws given experimental testing data, but are only capable of\npredicting the mean stress response. Stochastic methods treat each weight as a\nrandom variable and are capable of learning their probability distributions.\nBayesian constitutive neural networks combine both methods, but their weights\nlack physical interpretability and we must sample each weight from a\nprobability distribution to train or evaluate the model. Here we introduce a\nmore interpretable network with fewer parameters, simpler training, and the\npotential to discover correlated weights: Gaussian constitutive neural\nnetworks. We demonstrate the performance of our new Gaussian network on biaxial\ntesting data, and discover a sparse and interpretable four-term model with\ncorrelated weights. Importantly, the discovered distributions of material\nparameters across a set of samples can serve as priors to discover better\nconstitutive models for new samples with limited data. We anticipate that\nGaussian constitutive neural networks are a natural first step towards\ngenerative constitutive models informed by physical laws and parameter\nuncertainty.",
      "authors": [
        "Jeremy A. McCulloch",
        "Ellen Kuhl"
      ],
      "categories": [
        "cs.CE",
        "cs.LG"
      ],
      "links": [
        "http://arxiv.org/abs/2503.12679v1",
        "http://arxiv.org/pdf/2503.12679v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2503.12667v1",
      "title": "Plausibility Vaccine: Injecting LLM Knowledge for Event Plausibility",
      "published": "2025-03-16T21:55:17Z",
      "updated": "2025-03-16T21:55:17Z",
      "summary": "Despite advances in language modelling, distributional methods that build\nsemantic representations from co-occurrences fail to discriminate between\nplausible and implausible events. In this work, we investigate how plausibility\nprediction can be improved by injecting latent knowledge prompted from large\nlanguage models using parameter-efficient fine-tuning. We train 12 task\nadapters to learn various physical properties and association measures and\nperform adapter fusion to compose latent semantic knowledge from each task on\ntop of pre-trained AlBERT embeddings. We automate auxiliary task data\ngeneration, which enables us to scale our approach and fine-tune our learned\nrepresentations across two plausibility datasets. Our code is available at\nhttps://github.com/Jacob-Chmura/plausibility-vaccine.",
      "authors": [
        "Jacob Chmura",
        "Jonah Dauvet",
        "Sebastian Sabry"
      ],
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "links": [
        "http://arxiv.org/abs/2503.12667v1",
        "http://arxiv.org/pdf/2503.12667v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2503.12626v1",
      "title": "Automated Planning for Optimal Data Pipeline Instantiation",
      "published": "2025-03-16T19:43:12Z",
      "updated": "2025-03-16T19:43:12Z",
      "summary": "Data pipeline frameworks provide abstractions for implementing sequences of\ndata-intensive transformation operators, automating the deployment and\nexecution of such transformations in a cluster. Deploying a data pipeline,\nhowever, requires computing resources to be allocated in a data center, ideally\nminimizing the overhead for communicating data and executing operators in the\npipeline while considering each operator's execution requirements. In this\npaper, we model the problem of optimal data pipeline deployment as planning\nwith action costs, where we propose heuristics aiming to minimize total\nexecution time. Experimental results indicate that the heuristics can\noutperform the baseline deployment and that a heuristic based on connections\noutperforms other strategies.",
      "authors": [
        "Leonardo Rosa Amado",
        "Adriano Vogel",
        "Dalvan Griebler",
        "Gabriel Paludo Licks",
        "Eric Simon",
        "Felipe Meneguzzi"
      ],
      "categories": [
        "cs.AI",
        "cs.DC"
      ],
      "links": [
        "http://arxiv.org/abs/2503.12626v1",
        "http://arxiv.org/pdf/2503.12626v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2503.12611v1",
      "title": "Functional Factor Regression with an Application to Electricity Price\n  Curve Modeling",
      "published": "2025-03-16T18:48:18Z",
      "updated": "2025-03-16T18:48:18Z",
      "summary": "We propose a function-on-function linear regression model for time-dependent\ncurve data that is consistently estimated by imposing factor structures on the\nregressors. An integral operator based on cross-covariances identifies two\ncomponents for each functional regressor: a predictive low-dimensional\ncomponent, along with associated factors that are guaranteed to be correlated\nwith the dependent variable, and an infinite-dimensional component that has no\npredictive power. In order to consistently estimate the correct number of\nfactors for each regressor, we introduce a functional eigenvalue difference\ntest. Our setting allows us to construct a novel central limit theorem for the\nregression parameters in a fully functional model, making it possible to\nconstruct confidence bands and conduct statistical inference. The model is\napplied to forecast electricity price curves in three different energy markets.\nIts prediction accuracy is found to be comparable to popular machine learning\napproaches, while providing statistically valid inference and interpretable\ninsights into the conditional correlation structures of electricity prices.",
      "authors": [
        "Sven Otto",
        "Luis Winter"
      ],
      "categories": [
        "econ.EM",
        "stat.ME"
      ],
      "links": [
        "http://arxiv.org/abs/2503.12611v1",
        "http://arxiv.org/pdf/2503.12611v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2503.12541v1",
      "title": "Histogram Transporter: Learning Rotation-Equivariant Orientation\n  Histograms for High-Precision Robotic Kitting",
      "published": "2025-03-16T15:21:50Z",
      "updated": "2025-03-16T15:21:50Z",
      "summary": "Robotic kitting is a critical task in industrial automation that requires the\nprecise arrangement of objects into kits to support downstream production\nprocesses. However, when handling complex kitting tasks that involve\nfine-grained orientation alignment, existing approaches often suffer from\nlimited accuracy and computational efficiency. To address these challenges, we\npropose Histogram Transporter, a novel kitting framework that learns\nhigh-precision pick-and-place actions from scratch using only a few\ndemonstrations. First, our method extracts rotation-equivariant orientation\nhistograms (EOHs) from visual observations using an efficient Fourier-based\ndiscretization strategy. These EOHs serve a dual purpose: improving picking\nefficiency by directly modeling action success probabilities over\nhigh-resolution orientations and enhancing placing accuracy by serving as\nlocal, discriminative feature descriptors for object-to-placement matching.\nSecond, we introduce a subgroup alignment strategy in the place model that\ncompresses the full spectrum of EOHs into a compact orientation representation,\nenabling efficient feature matching while preserving accuracy. Finally, we\nexamine the proposed framework on the simulated Hand-Tool Kitting Dataset\n(HTKD), where it outperforms competitive baselines in both success rates and\ncomputational efficiency. Further experiments on five Raven-10 tasks exhibits\nthe remarkable adaptability of our approach, with real-robot trials confirming\nits applicability for real-world deployment.",
      "authors": [
        "Jiadong Zhou",
        "Yadan Zeng",
        "Huixu Dong",
        "I-Ming Chen"
      ],
      "categories": [
        "cs.RO"
      ],
      "links": [
        "http://arxiv.org/abs/2503.12541v1",
        "http://arxiv.org/pdf/2503.12541v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2503.12515v1",
      "title": "AI-Powered Automated Model Construction for Patient-Specific CFD\n  Simulations of Aortic Flows",
      "published": "2025-03-16T14:18:25Z",
      "updated": "2025-03-16T14:18:25Z",
      "summary": "Image-based modeling is essential for understanding cardiovascular\nhemodynamics and advancing the diagnosis and treatment of cardiovascular\ndiseases. Constructing patient-specific vascular models remains\nlabor-intensive, error-prone, and time-consuming, limiting their clinical\napplications. This study introduces a deep-learning framework that automates\nthe creation of simulation-ready vascular models from medical images. The\nframework integrates a segmentation module for accurate voxel-based vessel\ndelineation with a surface deformation module that performs anatomically\nconsistent and unsupervised surface refinements guided by medical image data.\nBy unifying voxel segmentation and surface deformation into a single cohesive\npipeline, the framework addresses key limitations of existing methods,\nenhancing geometric accuracy and computational efficiency. Evaluated on\npublicly available datasets, the proposed approach demonstrates\nstate-of-the-art performance in segmentation and mesh quality while\nsignificantly reducing manual effort and processing time. This work advances\nthe scalability and reliability of image-based computational modeling,\nfacilitating broader applications in clinical and research settings.",
      "authors": [
        "Pan Du",
        "Delin An",
        "Chaoli Wang",
        "Jian-Xun Wang"
      ],
      "categories": [
        "cs.CV",
        "cs.LG",
        "physics.med-ph"
      ],
      "links": [
        "http://arxiv.org/abs/2503.12515v1",
        "http://arxiv.org/pdf/2503.12515v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    }
  ]
}