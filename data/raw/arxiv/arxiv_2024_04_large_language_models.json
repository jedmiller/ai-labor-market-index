{
  "query": "all:large language models AND (labor market OR employment OR jobs OR workforce OR automation)",
  "date_collected": "2025-03-21T23:00:34.894134",
  "target_period": "2024-04",
  "papers": [
    {
      "id": "http://arxiv.org/abs/2405.00253v4",
      "title": "CodeHalu: Investigating Code Hallucinations in LLMs via Execution-based\n  Verification",
      "published": "2024-04-30T23:56:38Z",
      "updated": "2025-01-21T02:45:49Z",
      "summary": "Large Language Models (LLMs) have made significant progress in code\ngeneration, offering developers groundbreaking automated programming support.\nHowever, LLMs often generate code that is syntactically correct and even\nsemantically plausible, but may not execute as expected or fulfill specified\nrequirements. This phenomenon of hallucinations in the code domain has not been\nsystematically explored. To advance the community's understanding and research\non this issue, we introduce the concept of code hallucinations and propose a\nclassification method for code hallucination based on execution verification.\nWe categorize code hallucinations into four main types: mapping, naming,\nresource, and logic hallucinations, with each category further divided into\ndifferent subcategories to understand and address the unique challenges faced\nby LLMs in code generation with finer granularity. Additionally, we present a\ndynamic detection algorithm called CodeHalu designed to detect and quantify\ncode hallucinations. We also introduce the CodeHaluEval benchmark, which\nincludes 8,883 samples from 699 tasks, to systematically and quantitatively\nevaluate code hallucinations. By evaluating 17 popular LLMs using this\nbenchmark, we reveal significant differences in their accuracy and reliability\nin code generation, offering detailed insights for further improving the code\ngeneration capabilities of LLMs. The CodeHalu benchmark and code are publicly\navailable at https://github.com/yuchen814/CodeHalu.",
      "authors": [
        "Yuchen Tian",
        "Weixiang Yan",
        "Qian Yang",
        "Xuandong Zhao",
        "Qian Chen",
        "Wen Wang",
        "Ziyang Luo",
        "Lei Ma",
        "Dawn Song"
      ],
      "categories": [
        "cs.CL",
        "cs.SE"
      ],
      "links": [
        "http://arxiv.org/abs/2405.00253v4",
        "http://arxiv.org/pdf/2405.00253v4"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2406.08739v1",
      "title": "At the edge of a generative cultural precipice",
      "published": "2024-04-30T23:26:24Z",
      "updated": "2024-04-30T23:26:24Z",
      "summary": "Since NFTs and large generative models (such as DALLE2 and Stable Diffusion)\nhave been publicly available, artists have seen their jobs threatened and\nstolen. While artists depend on sharing their art on online platforms such as\nDeviantart, Pixiv, and Artstation, many slowed down sharing their work or\ndownright removed their past work therein, especially if these platforms fail\nto provide certain guarantees regarding the copyright of their uploaded work.\nText-to-image (T2I) generative models are trained using human-produced content\nto better guide the style and themes they can produce. Still, if the trend\ncontinues where data found online is generated by a machine instead of a human,\nthis will have vast repercussions in culture. Inspired by recent work in\ngenerative models, we wish to tell a cautionary tale and ask what will happen\nto the visual arts if generative models continue on the path to be (eventually)\ntrained solely on generated content.",
      "authors": [
        "Diego Porres",
        "Alex Gomez-Villa"
      ],
      "categories": [
        "cs.CY",
        "cs.LG"
      ],
      "links": [
        "http://arxiv.org/abs/2406.08739v1",
        "http://arxiv.org/pdf/2406.08739v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2405.00235v1",
      "title": "Blockchain Price vs. Quantity Controls",
      "published": "2024-04-30T23:03:00Z",
      "updated": "2024-04-30T23:03:00Z",
      "summary": "This paper studies the optimal transaction fee mechanisms for blockchains,\nfocusing on the distinction between price-based ($\\mathcal{P}$) and\nquantity-based ($\\mathcal{Q}$) controls. By analyzing factors such as demand\nuncertainty, validator costs, cryptocurrency price fluctuations, price\nelasticity of demand, and levels of decentralization, we establish criteria\nthat determine the selection of transaction fee mechanisms. We present a model\nframed around a Nash bargaining game, exploring how blockchain designers and\nvalidators negotiate fee structures to balance network welfare with\nprofitability. Our findings suggest that the choice between $\\mathcal{P}$ and\n$\\mathcal{Q}$ mechanisms depends critically on the blockchain's specific\ntechnical and economic features. The study concludes that no single mechanism\nsuits all contexts and highlights the potential for hybrid approaches that\nadaptively combine features of both $\\mathcal{P}$ and $\\mathcal{Q}$ to meet\nvarying demands and market conditions.",
      "authors": [
        "Abdoulaye Ndiaye"
      ],
      "categories": [
        "econ.GN",
        "q-fin.EC"
      ],
      "links": [
        "http://dx.doi.org/10.1007/978-3-031-78676-1_17",
        "http://arxiv.org/abs/2405.00235v1",
        "http://arxiv.org/pdf/2405.00235v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2405.00232v1",
      "title": "Constraining the giant radio galaxy population with machine learning and\n  Bayesian inference",
      "published": "2024-04-30T22:47:07Z",
      "updated": "2024-04-30T22:47:07Z",
      "summary": "Large-scale sky surveys at low frequencies, like the LOFAR Two-metre Sky\nSurvey (LoTSS), allow for the detection and characterisation of unprecedented\nnumbers of giant radio galaxies (GRGs, or 'giants'). In this work, by\nautomating the creation of radio--optical catalogues, we aim to significantly\nexpand the census of known giants. We then combine this sample with a forward\nmodel to constrain GRG properties of cosmological interest. In particular, we\nautomate radio source component association through machine learning and\noptical host identification for resolved radio sources. We create a\nradio--optical catalogue for the full LoTSS Data Release 2 (DR2) and select all\npossible giants. We combine our candidates with an existing catalogue of LoTSS\nDR2 crowd-sourced GRG candidates and visually confirm or reject them. To infer\nintrinsic GRG properties from GRG observations, we develop further a\npopulation-based forward model that takes into account selection effects and\nconstrain its parameters using Bayesian inference. We confirm 5,647 previously\nunknown giants from the crowd-sourced catalogue and 2,597 previously unknown\ngiants from the ML-driven catalogue. Our confirmations and discoveries bring\nthe total number of known giants to at least 11,585. We predict a comoving GRG\nnumber density $n_\\mathrm{GRG} = 13 \\pm 10\\ (100\\ \\mathrm{Mpc})^{-3}$, close to\na recent estimate of the number density of luminous non-giant radio galaxies.\nWe derive a current-day GRG lobe volume-filling fraction $V_\\mathrm{GRG-CW}(z =\n0) = 1.4 \\pm 1.1 \\cdot 10^{-5}$ in clusters and filaments of the Cosmic Web.\nOur analysis suggests that giants are more common than previously thought.\nMoreover, tentative results imply that it is possible that magnetic fields once\ncontained in giants pervade a significant ($\\gtrsim 10\\%$) fraction of today's\nCosmic Web.",
      "authors": [
        "Rafa\u00ebl I. J. Mostert",
        "Martijn S. S. L. Oei",
        "B. Barkus",
        "Lara Alegre",
        "Martin J. Hardcastle",
        "Kenneth J. Duncan",
        "Huub J. A. R\u00f6ttgering",
        "Reinout J. van Weeren",
        "Maya Horton"
      ],
      "categories": [
        "astro-ph.GA",
        "astro-ph.CO",
        "astro-ph.HE"
      ],
      "links": [
        "http://dx.doi.org/10.1051/0004-6361/202348897",
        "http://arxiv.org/abs/2405.00232v1",
        "http://arxiv.org/pdf/2405.00232v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2405.00229v1",
      "title": "Aptly: Making Mobile Apps from Natural Language",
      "published": "2024-04-30T22:33:34Z",
      "updated": "2024-04-30T22:33:34Z",
      "summary": "We present Aptly, an extension of the MIT App Inventor platform enabling\nmobile app development via natural language powered by code-generating large\nlanguage models (LLMs). Aptly complements App Inventor's block language with a\ntext language designed to allow visual code generation via text-based LLMs. We\ndetail the technical aspects of how the Aptly server integrates LLMs with a\nrealtime collaboration function to facilitate the automated creation and\nediting of mobile apps given user instructions. The paper concludes with\ninsights from a study of a pilot implementation involving high school students,\nwhich examines Aptly's practicality and user experience. The findings\nunderscore Aptly's potential as a tool that democratizes app development and\nfosters technological creativity.",
      "authors": [
        "Evan W. Patton",
        "David Y. J. Kim",
        "Ashley Granquist",
        "Robin Liu",
        "Arianna Scott",
        "Jennet Zamanova",
        "Harold Abelson"
      ],
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.PL"
      ],
      "links": [
        "http://arxiv.org/abs/2405.00229v1",
        "http://arxiv.org/pdf/2405.00229v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2405.00223v2",
      "title": "Confides: A Visual Analytics Solution for Automated Speech Recognition\n  Analysis and Exploration",
      "published": "2024-04-30T22:03:17Z",
      "updated": "2024-07-24T22:34:31Z",
      "summary": "Confidence scores of automatic speech recognition (ASR) outputs are often\ninadequately communicated, preventing its seamless integration into analytical\nworkflows. In this paper, we introduce ConFides, a visual analytic system\ndeveloped in collaboration with intelligence analysts to address this issue.\nConFides aims to aid exploration and post-AI-transcription editing by visually\nrepresenting the confidence associated with the transcription. We demonstrate\nhow our tool can assist intelligence analysts who use ASR outputs in their\nanalytical and exploratory tasks and how it can help mitigate misinterpretation\nof crucial information. We also discuss opportunities for improving textual\ndata cleaning and model transparency for human-machine collaboration.",
      "authors": [
        "Sunwoo Ha",
        "Chaehun Lim",
        "R. Jordan Crouser",
        "Alvitta Ottley"
      ],
      "categories": [
        "cs.HC"
      ],
      "links": [
        "http://arxiv.org/abs/2405.00223v2",
        "http://arxiv.org/pdf/2405.00223v2"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2405.00186v1",
      "title": "Credentials in the Occupation Ontology",
      "published": "2024-04-30T20:23:18Z",
      "updated": "2024-04-30T20:23:18Z",
      "summary": "The term credential encompasses educational certificates, degrees,\ncertifications, and government-issued licenses. An occupational credential is a\nverification of an individuals qualification or competence issued by a third\nparty with relevant authority. Job seekers often leverage such credentials as\nevidence that desired qualifications are satisfied by their holders. Many U.S.\neducation and workforce development organizations have recognized the\nimportance of credentials for employment and the challenges of understanding\nthe value of credentials. In this study, we identified and ontologically\ndefined credential and credential-related terms at the textual and semantic\nlevels based on the Occupation Ontology (OccO), a BFO-based ontology. Different\ncredential types and their authorization logic are modeled. We additionally\ndefined a high-level hierarchy of credential related terms and relations among\nmany terms, which were initiated in concert with the Alabama Talent Triad (ATT)\nprogram, which aims to connect learners, earners, employers and\neducation/training providers through credentials and skills. To our knowledge,\nour research provides for the first time systematic ontological modeling of the\nimportant domain of credentials and related contents, supporting enhanced\ncredential data and knowledge integration in the future.",
      "authors": [
        "John Beverley",
        "Robin McGill",
        "Sam Smith",
        "Jie Zheng",
        "Giacomo De Colle",
        "Finn Wilson",
        "Matthew Diller",
        "William D. Duncan",
        "William R. Hogan",
        "Yongqun He"
      ],
      "categories": [
        "cs.AI",
        "cs.DB",
        "cs.IR"
      ],
      "links": [
        "http://arxiv.org/abs/2405.00186v1",
        "http://arxiv.org/pdf/2405.00186v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2405.00166v1",
      "title": "Discovering intrinsic multi-compartment pharmacometric models using\n  Physics Informed Neural Networks",
      "published": "2024-04-30T19:31:31Z",
      "updated": "2024-04-30T19:31:31Z",
      "summary": "Pharmacometric models are pivotal across drug discovery and development,\nplaying a decisive role in determining the progression of candidate molecules.\nHowever, the derivation of mathematical equations governing the system is a\nlabor-intensive trial-and-error process, often constrained by tight timelines.\nIn this study, we introduce PKINNs, a novel purely data-driven\npharmacokinetic-informed neural network model. PKINNs efficiently discovers and\nmodels intrinsic multi-compartment-based pharmacometric structures, reliably\nforecasting their derivatives. The resulting models are both interpretable and\nexplainable through Symbolic Regression methods. Our computational framework\ndemonstrates the potential for closed-form model discovery in pharmacometric\napplications, addressing the labor-intensive nature of traditional model\nderivation. With the increasing availability of large datasets, this framework\nholds the potential to significantly enhance model-informed drug discovery.",
      "authors": [
        "Imran Nasim",
        "Adam Nasim"
      ],
      "categories": [
        "cs.LG",
        "cs.AI",
        "q-bio.QM"
      ],
      "links": [
        "http://arxiv.org/abs/2405.00166v1",
        "http://arxiv.org/pdf/2405.00166v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2405.00145v3",
      "title": "GUing: A Mobile GUI Search Engine using a Vision-Language Model",
      "published": "2024-04-30T18:42:18Z",
      "updated": "2024-10-06T15:59:06Z",
      "summary": "Graphical User Interfaces (GUIs) are central to app development projects. App\ndevelopers may use the GUIs of other apps as a means of requirements refinement\nand rapid prototyping or as a source of inspiration for designing and improving\ntheir own apps. Recent research has thus suggested retrieving relevant GUI\ndesigns that match a certain text query from screenshot datasets acquired\nthrough crowdsourced or automated exploration of GUIs. However, such\ntext-to-GUI retrieval approaches only leverage the textual information of the\nGUI elements, neglecting visual information such as icons or background images.\nIn addition, retrieved screenshots are not steered by app developers and lack\napp features that require particular input data.\n  To overcome these limitations, this paper proposes GUing, a GUI search engine\nbased on a vision-language model called GUIClip, which we trained specifically\nfor the problem of designing app GUIs. For this, we first collected from Google\nPlay app introduction images which display the most representative screenshots\nand are often captioned (i.e.~labelled) by app vendors. Then, we developed an\nautomated pipeline to classify, crop, and extract the captions from these\nimages. This resulted in a large dataset which we share with this paper:\nincluding 303k app screenshots, out of which 135k have captions. We used this\ndataset to train a novel vision-language model, which is, to the best of our\nknowledge, the first of its kind for GUI retrieval. We evaluated our approach\non various datasets from related work and in a manual experiment. The results\ndemonstrate that our model outperforms previous approaches in text-to-GUI\nretrieval achieving a Recall@10 of up to 0.69 and a HIT@10 of 0.91. We also\nexplored the performance of GUIClip for other GUI tasks including GUI\nclassification and sketch-to-GUI retrieval with encouraging results.",
      "authors": [
        "Jialiang Wei",
        "Anne-Lise Courbis",
        "Thomas Lambolais",
        "Binbin Xu",
        "Pierre Louis Bernard",
        "G\u00e9rard Dray",
        "Walid Maalej"
      ],
      "categories": [
        "cs.SE",
        "cs.CV"
      ],
      "links": [
        "http://dx.doi.org/10.1145/3702993",
        "http://arxiv.org/abs/2405.00145v3",
        "http://arxiv.org/pdf/2405.00145v3"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2407.01555v1",
      "title": "Unveiling Patterns in European Airbnb Prices: A Comprehensive Analytical\n  Study Using Machine Learning Techniques",
      "published": "2024-04-30T17:11:30Z",
      "updated": "2024-04-30T17:11:30Z",
      "summary": "In the burgeoning market of short-term rentals, understanding pricing\ndynamics is crucial for a range of stake-holders. This study delves into the\nfactors influencing Airbnb pricing in major European cities, employing a\ncomprehensive dataset sourced from Kaggle. We utilize advanced regression\ntechniques, including linear, polynomial, and random forest models, to analyze\na diverse array of determinants, such as location characteristics, property\ntypes, and host-related factors. Our findings reveal nuanced insights into the\nvariables most significantly impacting pricing, highlighting the varying roles\nof geographical, structural, and host-specific attributes. This research not\nonly sheds light on the complex pricing landscape of Airbnb accommodations in\nEurope but also offers valuable implications for hosts seeking to optimize\npricing strategies and for travelers aiming to understand pricing trends.\nFurthermore, the study contributes to the broader discourse on pricing\nmechanisms in the shared economy, suggesting avenues for future research in\nthis rapidly evolving sector.",
      "authors": [
        "Trinath Sai Subhash Reddy Pittala",
        "Uma Maheswara R Meleti",
        "Hemanth Vasireddy"
      ],
      "categories": [
        "econ.GN",
        "q-fin.EC"
      ],
      "links": [
        "http://arxiv.org/abs/2407.01555v1",
        "http://arxiv.org/pdf/2407.01555v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2404.19715v1",
      "title": "Assessing LLMs in Malicious Code Deobfuscation of Real-world Malware\n  Campaigns",
      "published": "2024-04-30T17:06:27Z",
      "updated": "2024-04-30T17:06:27Z",
      "summary": "The integration of large language models (LLMs) into various pipelines is\nincreasingly widespread, effectively automating many manual tasks and often\nsurpassing human capabilities. Cybersecurity researchers and practitioners have\nrecognised this potential. Thus, they are actively exploring its applications,\ngiven the vast volume of heterogeneous data that requires processing to\nidentify anomalies, potential bypasses, attacks, and fraudulent incidents. On\ntop of this, LLMs' advanced capabilities in generating functional code,\ncomprehending code context, and summarising its operations can also be\nleveraged for reverse engineering and malware deobfuscation. To this end, we\ndelve into the deobfuscation capabilities of state-of-the-art LLMs. Beyond\nmerely discussing a hypothetical scenario, we evaluate four LLMs with\nreal-world malicious scripts used in the notorious Emotet malware campaign. Our\nresults indicate that while not absolutely accurate yet, some LLMs can\nefficiently deobfuscate such payloads. Thus, fine-tuning LLMs for this task can\nbe a viable potential for future AI-powered threat intelligence pipelines in\nthe fight against obfuscated malware.",
      "authors": [
        "Constantinos Patsakis",
        "Fran Casino",
        "Nikolaos Lykousas"
      ],
      "categories": [
        "cs.CR"
      ],
      "links": [
        "http://arxiv.org/abs/2404.19715v1",
        "http://arxiv.org/pdf/2404.19715v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2404.19713v2",
      "title": "Automated Generation of High-Quality Medical Simulation Scenarios\n  Through Integration of Semi-Structured Data and Large Language Models",
      "published": "2024-04-30T17:06:11Z",
      "updated": "2024-05-06T17:58:48Z",
      "summary": "This study introduces a transformative framework for medical education by\nintegrating semi-structured data with Large Language Models (LLMs), primarily\nOpenAIs ChatGPT3.5, to automate the creation of medical simulation scenarios.\nTraditionally, developing these scenarios was a time-intensive process with\nlimited flexibility to meet diverse educational needs. The proposed approach\nutilizes AI to efficiently generate detailed, clinically relevant scenarios\nthat are tailored to specific educational objectives. This innovation has\nsignificantly reduced the time and resources required for scenario development,\nallowing for a broader variety of simulations. Preliminary feedback from\neducators and learners has shown enhanced engagement and improved knowledge\nacquisition, confirming the effectiveness of this AI-enhanced methodology in\nsimulation-based learning. The integration of structured data with LLMs not\nonly streamlines the creation process but also offers a scalable, dynamic\nsolution that could revolutionize medical training, highlighting the critical\nrole of AI in advancing educational outcomes and patient care standards.",
      "authors": [
        "Scott Sumpter"
      ],
      "categories": [
        "cs.CL"
      ],
      "links": [
        "http://arxiv.org/abs/2404.19713v2",
        "http://arxiv.org/pdf/2404.19713v2"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2404.19656v1",
      "title": "Towards Scenario- and Capability-Driven Dataset Development and\n  Evaluation: An Approach in the Context of Mapless Automated Driving",
      "published": "2024-04-30T15:52:49Z",
      "updated": "2024-04-30T15:52:49Z",
      "summary": "The foundational role of datasets in defining the capabilities of deep\nlearning models has led to their rapid proliferation. At the same time,\npublished research focusing on the process of dataset development for\nenvironment perception in automated driving has been scarce, thereby reducing\nthe applicability of openly available datasets and impeding the development of\neffective environment perception systems. Sensor-based, mapless automated\ndriving is one of the contexts where this limitation is evident. While\nleveraging real-time sensor data, instead of pre-defined HD maps promises\nenhanced adaptability and safety by effectively navigating unexpected\nenvironmental changes, it also increases the demands on the scope and\ncomplexity of the information provided by the perception system.\n  To address these challenges, we propose a scenario- and capability-based\napproach to dataset development. Grounded in the principles of ISO 21448\n(safety of the intended functionality, SOTIF), extended by ISO/TR 4804, our\napproach facilitates the structured derivation of dataset requirements. This\nnot only aids in the development of meaningful new datasets but also enables\nthe effective comparison of existing ones. Applying this methodology to a broad\nrange of existing lane detection datasets, we identify significant limitations\nin current datasets, particularly in terms of real-world applicability, a lack\nof labeling of critical features, and an absence of comprehensive information\nfor complex driving maneuvers.",
      "authors": [
        "Felix Gr\u00fcn",
        "Marcus Nolte",
        "Markus Maurer"
      ],
      "categories": [
        "cs.CV"
      ],
      "links": [
        "http://arxiv.org/abs/2404.19656v1",
        "http://arxiv.org/pdf/2404.19656v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2404.19482v1",
      "title": "FactCheck Editor: Multilingual Text Editor with End-to-End fact-checking",
      "published": "2024-04-30T11:55:20Z",
      "updated": "2024-04-30T11:55:20Z",
      "summary": "We introduce 'FactCheck Editor', an advanced text editor designed to automate\nfact-checking and correct factual inaccuracies. Given the widespread issue of\nmisinformation, often a result of unintentional mistakes by content creators,\nour tool aims to address this challenge. It supports over 90 languages and\nutilizes transformer models to assist humans in the labor-intensive process of\nfact verification. This demonstration showcases a complete workflow that\ndetects text claims in need of verification, generates relevant search engine\nqueries, and retrieves appropriate documents from the web. It employs Natural\nLanguage Inference (NLI) to predict the veracity of claims and uses LLMs to\nsummarize the evidence and suggest textual revisions to correct any errors in\nthe text. Additionally, the effectiveness of models used in claim detection and\nveracity assessment is evaluated across multiple languages.",
      "authors": [
        "Vinay Setty"
      ],
      "categories": [
        "cs.CL"
      ],
      "links": [
        "http://arxiv.org/abs/2404.19482v1",
        "http://arxiv.org/pdf/2404.19482v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2404.19403v1",
      "title": "Transformer-Enhanced Motion Planner: Attention-Guided Sampling for\n  State-Specific Decision Making",
      "published": "2024-04-30T09:48:11Z",
      "updated": "2024-04-30T09:48:11Z",
      "summary": "Sampling-based motion planning (SBMP) algorithms are renowned for their\nrobust global search capabilities. However, the inherent randomness in their\nsampling mechanisms often result in inconsistent path quality and limited\nsearch efficiency. In response to these challenges, this work proposes a novel\ndeep learning-based motion planning framework, named Transformer-Enhanced\nMotion Planner (TEMP), which synergizes an Environmental Information Semantic\nEncoder (EISE) with a Motion Planning Transformer (MPT). EISE converts\nenvironmental data into semantic environmental information (SEI), providing MPT\nwith an enriched environmental comprehension. MPT leverages an attention\nmechanism to dynamically recalibrate its focus on SEI, task objectives, and\nhistorical planning data, refining the sampling node generation. To demonstrate\nthe capabilities of TEMP, we train our model using a dataset comprised of\nplanning results produced by the RRT*. EISE and MPT are collaboratively\ntrained, enabling EISE to autonomously learn and extract patterns from\nenvironmental data, thereby forming semantic representations that MPT could\nmore effectively interpret and utilize for motion planning. Subsequently, we\nconducted a systematic evaluation of TEMP's efficacy across diverse task\ndimensions, which demonstrates that TEMP achieves exceptional performance\nmetrics and a heightened degree of generalizability compared to\nstate-of-the-art SBMPs.",
      "authors": [
        "Lei Zhuang",
        "Jingdong Zhao",
        "Yuntao Li",
        "Zichun Xu",
        "Liangliang Zhao",
        "Hong Liu"
      ],
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "links": [
        "http://dx.doi.org/10.1109/LRA.2024.3450305",
        "http://arxiv.org/abs/2404.19403v1",
        "http://arxiv.org/pdf/2404.19403v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2404.19356v2",
      "title": "A Concept for Semi-Automatic Configuration of Sufficiently Valid\n  Simulation Setups for Automated Driving Systems",
      "published": "2024-04-30T08:37:53Z",
      "updated": "2024-07-30T20:20:42Z",
      "summary": "As simulation is increasingly used in scenario-based approaches to test\nAutomated Driving Systems, the credibility of simulation results is a major\nconcern. Arguably, credibility depends on the validity of the simulation setup\nand simulation models. When selecting appropriate simulation models, a\ntrade-off must be made between validity, often connected to the model's\nfidelity, and cost of computation. However, due to the large number of test\ncases, expert-based methods to create sufficiently valid simulation setups seem\ninfeasible. We propose using design contracts in order to semi-automatically\ncompose simulation setups for given test cases from simulation models and to\nderive requirements for the simulation models, supporting separation of\nconcerns between simulation model developers and users. Simulation model\ncontracts represent their validity domains by capturing a validity guarantee\nand the associated operating conditions in an assumption. We then require the\ncomposition of the simulation model contracts to refine a test case contract.\nThe latter contract captures the operating conditions of the test case in its\nassumption and validity requirements in its guarantee. Based on this idea, we\npresent a framework that supports the compositional configuration of simulation\nsetups based on the contracts and a method to derive runtime monitors for these\nsimulation setups.",
      "authors": [
        "Niklas Braun",
        "Markus Steimle",
        "Martin T\u00f6rngren",
        "Markus Maurer"
      ],
      "categories": [
        "eess.SY",
        "cs.SY"
      ],
      "links": [
        "http://arxiv.org/abs/2404.19356v2",
        "http://arxiv.org/pdf/2404.19356v2"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2404.19318v2",
      "title": "Enhancing Trust in LLM-Generated Code Summaries with Calibrated\n  Confidence Scores",
      "published": "2024-04-30T07:38:08Z",
      "updated": "2024-12-03T23:53:19Z",
      "summary": "A good summary can often be very useful during program comprehension. While a\nbrief, fluent, and relevant summary can be helpful, it does require significant\nhuman effort to produce. Often, good summaries are unavailable in software\nprojects, thus making maintenance more difficult. There has been a considerable\nbody of research into automated AI-based methods, using Large Language models\n(LLMs), to generate summaries of code; there also has been quite a bit work on\nways to measure the performance of such summarization methods, with special\nattention paid to how closely these AI-generated summaries resemble a summary a\nhuman might have produced. Measures such as BERTScore and BLEU have been\nsuggested and evaluated with human-subject studies.\n  However, LLM-produced summaries can be too long, irrelevant, etc: generally,\ntoo dissimilar to what a human might say. Given an LLM-produced code summary,\nhow can we judge if a summary is good enough? Given some input source code, and\nan LLM-generated summary, existing approaches can help judge brevity, fluency\nand relevance; however, it's difficult to gauge whether an LLM-produced summary\nsufficiently resembles what a human might produce, without a \"golden\"\nhuman-produced summary to compare against. We study this resemblance question\nas a calibration problem: given just the summary from an LLM, can we compute a\nconfidence measure, that provides a reliable indication of whether the summary\nsufficiently resembles what a human would have produced in this situation? We\nexamine this question using several LLMs, for several languages, and in several\ndifferent settings. Our investigation suggests approaches to provide reliable\npredictions of the likelihood that an LLM-generated summary would sufficiently\nresemble a summary a human might write for the same code.",
      "authors": [
        "Yuvraj Virk",
        "Premkumar Devanbu",
        "Toufique Ahmed"
      ],
      "categories": [
        "cs.SE",
        "cs.CL"
      ],
      "links": [
        "http://arxiv.org/abs/2404.19318v2",
        "http://arxiv.org/pdf/2404.19318v2"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2405.01593v1",
      "title": "Large Language Model Agent for Fake News Detection",
      "published": "2024-04-30T06:55:27Z",
      "updated": "2024-04-30T06:55:27Z",
      "summary": "In the current digital era, the rapid spread of misinformation on online\nplatforms presents significant challenges to societal well-being, public trust,\nand democratic processes, influencing critical decision making and public\nopinion. To address these challenges, there is a growing need for automated\nfake news detection mechanisms. Pre-trained large language models (LLMs) have\ndemonstrated exceptional capabilities across various natural language\nprocessing (NLP) tasks, prompting exploration into their potential for\nverifying news claims. Instead of employing LLMs in a non-agentic way, where\nLLMs generate responses based on direct prompts in a single shot, our work\nintroduces FactAgent, an agentic approach of utilizing LLMs for fake news\ndetection. FactAgent enables LLMs to emulate human expert behavior in verifying\nnews claims without any model training, following a structured workflow. This\nworkflow breaks down the complex task of news veracity checking into multiple\nsub-steps, where LLMs complete simple tasks using their internal knowledge or\nexternal tools. At the final step of the workflow, LLMs integrate all findings\nthroughout the workflow to determine the news claim's veracity. Compared to\nmanual human verification, FactAgent offers enhanced efficiency. Experimental\nstudies demonstrate the effectiveness of FactAgent in verifying claims without\nthe need for any training process. Moreover, FactAgent provides transparent\nexplanations at each step of the workflow and during final decision-making,\noffering insights into the reasoning process of fake news detection for end\nusers. FactAgent is highly adaptable, allowing for straightforward updates to\nits tools that LLMs can leverage within the workflow, as well as updates to the\nworkflow itself using domain knowledge. This adaptability enables FactAgent's\napplication to news verification across various domains.",
      "authors": [
        "Xinyi Li",
        "Yongfeng Zhang",
        "Edward C. Malthouse"
      ],
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ],
      "links": [
        "http://arxiv.org/abs/2405.01593v1",
        "http://arxiv.org/pdf/2405.01593v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2404.19236v2",
      "title": "On the Effect of Bounded Rationality in Electricity Markets",
      "published": "2024-04-30T03:32:37Z",
      "updated": "2024-07-11T22:26:41Z",
      "summary": "Nash equilibrium is a common solution concept that captures strategic\ninteraction in electricity market analysis. However, it requires a fundamental\nbut impractical assumption that all market participants are fully rational,\nimplying unlimited computational resources and cognitive abilities. To tackle\nthe limitation, level-k reasoning is proposed and studied to model the bounded\nrational behaviors. In this paper, we consider a Cournot competition in\nelectricity markets with two suppliers, both following level-k reasoning. One\nis a self-interested firm and the other serves as a benevolent social planner.\nFirst, we observe that the optimal strategy of the social planner corresponds\nto a particular rationality level, where being either less or more rational may\nboth result in reduced social welfare. We then investigate the effect of\nbounded rationality on social welfare performance and find that it can largely\ndeviate from that at the Nash equilibrium point. From the perspective of the\nsocial planner, we characterize optimal, expectation maximizing and robust\nmaximin strategies, when having access to different information. Finally, by\ndesigning its utility function, we find that social welfare is better off if\nthe social planner cooperates with or fights the self-interested firm.\nNumerical experiments further demonstrate and validate our findings.",
      "authors": [
        "Lihui Yi",
        "Ermin Wei"
      ],
      "categories": [
        "cs.GT",
        "cs.SY",
        "eess.SY"
      ],
      "links": [
        "http://arxiv.org/abs/2404.19236v2",
        "http://arxiv.org/pdf/2404.19236v2"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2404.19230v1",
      "title": "Deep Lead Optimization: Leveraging Generative AI for Structural\n  Modification",
      "published": "2024-04-30T03:17:42Z",
      "updated": "2024-04-30T03:17:42Z",
      "summary": "The idea of using deep-learning-based molecular generation to accelerate\ndiscovery of drug candidates has attracted extraordinary attention, and many\ndeep generative models have been developed for automated drug design, termed\nmolecular generation. In general, molecular generation encompasses two main\nstrategies: de novo design, which generates novel molecular structures from\nscratch, and lead optimization, which refines existing molecules into drug\ncandidates. Among them, lead optimization plays an important role in real-world\ndrug design. For example, it can enable the development of me-better drugs that\nare chemically distinct yet more effective than the original drugs. It can also\nfacilitate fragment-based drug design, transforming virtual-screened small\nligands with low affinity into first-in-class medicines. Despite its\nimportance, automated lead optimization remains underexplored compared to the\nwell-established de novo generative models, due to its reliance on complex\nbiological and chemical knowledge. To bridge this gap, we conduct a systematic\nreview of traditional computational methods for lead optimization, organizing\nthese strategies into four principal sub-tasks with defined inputs and outputs.\nThis review delves into the basic concepts, goals, conventional CADD\ntechniques, and recent advancements in AIDD. Additionally, we introduce a\nunified perspective based on constrained subgraph generation to harmonize the\nmethodologies of de novo design and lead optimization. Through this lens, de\nnovo design can incorporate strategies from lead optimization to address the\nchallenge of generating hard-to-synthesize molecules; inversely, lead\noptimization can benefit from the innovations in de novo design by approaching\nit as a task of generating molecules conditioned on certain substructures.",
      "authors": [
        "Odin Zhang",
        "Haitao Lin",
        "Hui Zhang",
        "Huifeng Zhao",
        "Yufei Huang",
        "Yuansheng Huang",
        "Dejun Jiang",
        "Chang-yu Hsieh",
        "Peichen Pan",
        "Tingjun Hou"
      ],
      "categories": [
        "q-bio.BM",
        "cs.AI"
      ],
      "links": [
        "http://arxiv.org/abs/2404.19230v1",
        "http://arxiv.org/pdf/2404.19230v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2404.19146v1",
      "title": "Automated Construction of Theme-specific Knowledge Graphs",
      "published": "2024-04-29T23:14:14Z",
      "updated": "2024-04-29T23:14:14Z",
      "summary": "Despite widespread applications of knowledge graphs (KGs) in various tasks\nsuch as question answering and intelligent conversational systems, existing KGs\nface two major challenges: information granularity and deficiency in\ntimeliness. These hinder considerably the retrieval and analysis of in-context,\nfine-grained, and up-to-date knowledge from KGs, particularly in highly\nspecialized themes (e.g., specialized scientific research) and rapidly evolving\ncontexts (e.g., breaking news or disaster tracking). To tackle such challenges,\nwe propose a theme-specific knowledge graph (i.e., ThemeKG), a KG constructed\nfrom a theme-specific corpus, and design an unsupervised framework for ThemeKG\nconstruction (named TKGCon). The framework takes raw theme-specific corpus and\ngenerates a high-quality KG that includes salient entities and relations under\nthe theme. Specifically, we start with an entity ontology of the theme from\nWikipedia, based on which we then generate candidate relations by Large\nLanguage Models (LLMs) to construct a relation ontology. To parse the documents\nfrom the theme corpus, we first map the extracted entity pairs to the ontology\nand retrieve the candidate relations. Finally, we incorporate the context and\nontology to consolidate the relations for entity pairs. We observe that\ndirectly prompting GPT-4 for theme-specific KG leads to inaccurate entities\n(such as \"two main types\" as one entity in the query result) and unclear (such\nas \"is\", \"has\") or wrong relations (such as \"have due to\", \"to start\"). In\ncontrast, by constructing the theme-specific KG step by step, our model\noutperforms GPT-4 and could consistently identify accurate entities and\nrelations. Experimental results also show that our framework excels in\nevaluations compared with various KG construction baselines.",
      "authors": [
        "Linyi Ding",
        "Sizhe Zhou",
        "Jinfeng Xiao",
        "Jiawei Han"
      ],
      "categories": [
        "cs.AI",
        "cs.IR"
      ],
      "links": [
        "http://arxiv.org/abs/2404.19146v1",
        "http://arxiv.org/pdf/2404.19146v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2404.19083v1",
      "title": "Longitudinal Mammogram Risk Prediction",
      "published": "2024-04-29T19:52:09Z",
      "updated": "2024-04-29T19:52:09Z",
      "summary": "Breast cancer is one of the leading causes of mortality among women\nworldwide. Early detection and risk assessment play a crucial role in improving\nsurvival rates. Therefore, annual or biennial mammograms are often recommended\nfor screening in high-risk groups. Mammograms are typically interpreted by\nexpert radiologists based on the Breast Imaging Reporting and Data System\n(BI-RADS), which provides a uniform way to describe findings and categorizes\nthem to indicate the level of concern for breast cancer. Recently, machine\nlearning (ML) and computational approaches have been developed to automate and\nimprove the interpretation of mammograms. However, both BI-RADS and the\nML-based methods focus on the analysis of data from the present and sometimes\nthe most recent prior visit. While it is clear that temporal changes in image\nfeatures of the longitudinal scans should carry value for quantifying breast\ncancer risk, no prior work has conducted a systematic study of this. In this\npaper, we extend a state-of-the-art ML model to ingest an arbitrary number of\nlongitudinal mammograms and predict future breast cancer risk. On a large-scale\ndataset, we demonstrate that our model, LoMaR, achieves state-of-the-art\nperformance when presented with only the present mammogram. Furthermore, we use\nLoMaR to characterize the predictive value of prior visits. Our results show\nthat longer histories (e.g., up to four prior annual mammograms) can\nsignificantly boost the accuracy of predicting future breast cancer risk,\nparticularly beyond the short-term. Our code and model weights are available at\nhttps://github.com/batuhankmkaraman/LoMaR.",
      "authors": [
        "Batuhan K. Karaman",
        "Katerina Dodelzon",
        "Gozde B. Akar",
        "Mert R. Sabuncu"
      ],
      "categories": [
        "eess.IV",
        "cs.CV"
      ],
      "links": [
        "http://arxiv.org/abs/2404.19083v1",
        "http://arxiv.org/pdf/2404.19083v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2404.19066v1",
      "title": "Revolutionizing Traffic Sign Recognition: Unveiling the Potential of\n  Vision Transformers",
      "published": "2024-04-29T19:18:52Z",
      "updated": "2024-04-29T19:18:52Z",
      "summary": "This research introduces an innovative method for Traffic Sign Recognition\n(TSR) by leveraging deep learning techniques, with a particular emphasis on\nVision Transformers. TSR holds a vital role in advancing driver assistance\nsystems and autonomous vehicles. Traditional TSR approaches, reliant on manual\nfeature extraction, have proven to be labor-intensive and costly. Moreover,\nmethods based on shape and color have inherent limitations, including\nsusceptibility to various factors and changes in lighting conditions. This\nstudy explores three variants of Vision Transformers (PVT, TNT, LNL) and six\nconvolutional neural networks (AlexNet, ResNet, VGG16, MobileNet, EfficientNet,\nGoogleNet) as baseline models. To address the shortcomings of traditional\nmethods, a novel pyramid EATFormer backbone is proposed, amalgamating\nEvolutionary Algorithms (EAs) with the Transformer architecture. The introduced\nEA-based Transformer block captures multi-scale, interactive, and individual\ninformation through its components: Feed-Forward Network, Global and Local\nInteraction, and Multi-Scale Region Aggregation modules. Furthermore, a\nModulated Deformable MSA module is introduced to dynamically model irregular\nlocations. Experimental evaluations on the GTSRB and BelgiumTS datasets\ndemonstrate the efficacy of the proposed approach in enhancing both prediction\nspeed and accuracy. This study concludes that Vision Transformers hold\nsignificant promise in traffic sign classification and contributes a fresh\nalgorithmic framework for TSR. These findings set the stage for the development\nof precise and dependable TSR algorithms, benefiting driver assistance systems\nand autonomous vehicles.",
      "authors": [
        "Susano Mingwin",
        "Yulong Shisu",
        "Yongshuai Wanwag",
        "Sunshin Huing"
      ],
      "categories": [
        "cs.CV"
      ],
      "links": [
        "http://arxiv.org/abs/2404.19066v1",
        "http://arxiv.org/pdf/2404.19066v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2404.19063v1",
      "title": "SuperCLUE-Fin: Graded Fine-Grained Analysis of Chinese LLMs on Diverse\n  Financial Tasks and Applications",
      "published": "2024-04-29T19:04:35Z",
      "updated": "2024-04-29T19:04:35Z",
      "summary": "The SuperCLUE-Fin (SC-Fin) benchmark is a pioneering evaluation framework\ntailored for Chinese-native financial large language models (FLMs). It assesses\nFLMs across six financial application domains and twenty-five specialized\ntasks, encompassing theoretical knowledge and practical applications such as\ncompliance, risk management, and investment analysis. Using multi-turn,\nopen-ended conversations that mimic real-life scenarios, SC-Fin measures models\non a range of criteria, including accurate financial understanding, logical\nreasoning, clarity, computational efficiency, business acumen, risk perception,\nand compliance with Chinese regulations.\n  In a rigorous evaluation involving over a thousand questions, SC-Fin\nidentifies a performance hierarchy where domestic models like GLM-4 and\nMoonShot-v1-128k outperform others with an A-grade, highlighting the potential\nfor further development in transforming theoretical knowledge into pragmatic\nfinancial solutions. This benchmark serves as a critical tool for refining FLMs\nin the Chinese context, directing improvements in financial knowledge\ndatabases, standardizing financial interpretations, and promoting models that\nprioritize compliance, risk management, and secure practices.\n  We create a contextually relevant and comprehensive benchmark that drives the\ndevelopment of AI in the Chinese financial sector. SC-Fin facilitates the\nadvancement and responsible deployment of FLMs, offering valuable insights for\nenhancing model performance and usability for both individual and institutional\nusers in the Chinese market..~\\footnote{Our benchmark can be found at\n\\url{https://www.CLUEbenchmarks.com}}.",
      "authors": [
        "Liang Xu",
        "Lei Zhu",
        "Yaotong Wu",
        "Hang Xue"
      ],
      "categories": [
        "cs.CL"
      ],
      "links": [
        "http://arxiv.org/abs/2404.19063v1",
        "http://arxiv.org/pdf/2404.19063v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2404.19043v1",
      "title": "Improving Interpretability of Deep Active Learning for Flood Inundation\n  Mapping Through Class Ambiguity Indices Using Multi-spectral Satellite\n  Imagery",
      "published": "2024-04-29T18:33:17Z",
      "updated": "2024-04-29T18:33:17Z",
      "summary": "Flood inundation mapping is a critical task for responding to the increasing\nrisk of flooding linked to global warming. Significant advancements of deep\nlearning in recent years have triggered its extensive applications, including\nflood inundation mapping. To cope with the time-consuming and labor-intensive\ndata labeling process in supervised learning, deep active learning strategies\nare one of the feasible approaches. However, there remains limited exploration\ninto the interpretability of how deep active learning strategies operate, with\na specific focus on flood inundation mapping in the field of remote sensing. In\nthis study, we introduce a novel framework of Interpretable Deep Active\nLearning for Flood inundation Mapping (IDAL-FIM), specifically in terms of\nclass ambiguity of multi-spectral satellite images. In the experiments, we\nutilize Sen1Floods11 dataset, and adopt U-Net with MC-dropout. In addition, we\nemploy five acquisition functions, which are the random, K-means, BALD,\nentropy, and margin acquisition functions. Based on the experimental results,\nwe demonstrate that two proposed class ambiguity indices are effective\nvariables to interpret the deep active learning by establishing statistically\nsignificant correlation with the predictive uncertainty of the deep learning\nmodel at the tile level. Then, we illustrate the behaviors of deep active\nlearning through visualizing two-dimensional density plots and providing\ninterpretations regarding the operation of deep active learning, in flood\ninundation mapping.",
      "authors": [
        "Hyunho Lee",
        "Wenwen Li"
      ],
      "categories": [
        "cs.CV"
      ],
      "links": [
        "http://dx.doi.org/10.1016/j.rse.2024.114213",
        "http://arxiv.org/abs/2404.19043v1",
        "http://arxiv.org/pdf/2404.19043v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2404.18992v2",
      "title": "Unifying Simulation and Inference with Normalizing Flows",
      "published": "2024-04-29T18:00:00Z",
      "updated": "2024-05-09T21:41:49Z",
      "summary": "There have been many applications of deep neural networks to detector\ncalibrations and a growing number of studies that propose deep generative\nmodels as automated fast detector simulators. We show that these two tasks can\nbe unified by using maximum likelihood estimation (MLE) from conditional\ngenerative models for energy regression. Unlike direct regression techniques,\nthe MLE approach is prior-independent and non-Gaussian resolutions can be\ndetermined from the shape of the likelihood near the maximum. Using an\nATLAS-like calorimeter simulation, we demonstrate this concept in the context\nof calorimeter energy calibration.",
      "authors": [
        "Haoxing Du",
        "Claudius Krause",
        "Vinicius Mikuni",
        "Benjamin Nachman",
        "Ian Pang",
        "David Shih"
      ],
      "categories": [
        "hep-ph",
        "hep-ex",
        "physics.data-an",
        "physics.ins-det",
        "stat.ML"
      ],
      "links": [
        "http://arxiv.org/abs/2404.18992v2",
        "http://arxiv.org/pdf/2404.18992v2"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2404.18926v1",
      "title": "Point Cloud Models Improve Visual Robustness in Robotic Learners",
      "published": "2024-04-29T17:59:11Z",
      "updated": "2024-04-29T17:59:11Z",
      "summary": "Visual control policies can encounter significant performance degradation\nwhen visual conditions like lighting or camera position differ from those seen\nduring training -- often exhibiting sharp declines in capability even for minor\ndifferences. In this work, we examine robustness to a suite of these types of\nvisual changes for RGB-D and point cloud based visual control policies. To\nperform these experiments on both model-free and model-based reinforcement\nlearners, we introduce a novel Point Cloud World Model (PCWM) and point cloud\nbased control policies. Our experiments show that policies that explicitly\nencode point clouds are significantly more robust than their RGB-D\ncounterparts. Further, we find our proposed PCWM significantly outperforms\nprior works in terms of sample efficiency during training. Taken together,\nthese results suggest reasoning about the 3D scene through point clouds can\nimprove performance, reduce learning time, and increase robustness for robotic\nlearners. Project Webpage: https://pvskand.github.io/projects/PCWM",
      "authors": [
        "Skand Peri",
        "Iain Lee",
        "Chanho Kim",
        "Li Fuxin",
        "Tucker Hermans",
        "Stefan Lee"
      ],
      "categories": [
        "cs.RO",
        "cs.CV",
        "cs.LG"
      ],
      "links": [
        "http://arxiv.org/abs/2404.18926v1",
        "http://arxiv.org/pdf/2404.18926v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2404.18897v2",
      "title": "Neural network prediction of model parameters for strong lensing samples\n  from Hyper Suprime-Cam Survey",
      "published": "2024-04-29T17:34:24Z",
      "updated": "2024-11-27T08:32:27Z",
      "summary": "Strong lensing of background galaxies provides important information about\nthe matter distribution around lens galaxies. Traditional modelling of such\nstrong lenses is both time and resource intensive. Fast and automated analysis\nmethods are the need of the hour given large upcoming surveys. In this work, we\nbuild and train a simple convolutional neural network with an aim of rapidly\npredicting model parameters of gravitational lenses. We focus on the inference\nof the Einstein radius, and ellipticity components of the mass distribution. We\ntrain our network on a variety of simulated data with increasing degree of\nrealism and compare its performance on simulated test data in a quantitative\nmanner. We also model 182 gravitational lenses from Subaru HSC survey using\n{\\sc YattaLens} pipeline to infer their model parameters, which allow a\nbenchmark to compare the predictions of the network. Given all considerations,\nwe conclude that the network trained on simulated samples with lensed sources\ninjected in empty HSC cutouts is the most robust, reproducing Einstein radii\nwith an accuracy of about $10-20$ percent, a bias less than 5 percent, and an\noutlier fraction of the order of 10 percent. We argue in favour of the\nsubtraction of the lens light before modelling the lens mass distribution. Our\ncomparisons of the inferred parameters of 10 HSC lenses previously modelled in\nthe literature, demonstrate agreement on the Einstein radius. However, the\nellipticity components from the network as well as the individual modelling\nmethods, seem to have systematic uncertainties beyond the quoted errors.",
      "authors": [
        "Priyanka Gawade",
        "Anupreeta More",
        "Surhud More",
        "Akisato Kimura",
        "Alessandro Sonnenfeld",
        "Masamune Oguri",
        "Naoki Yoshida"
      ],
      "categories": [
        "astro-ph.CO"
      ],
      "links": [
        "http://arxiv.org/abs/2404.18897v2",
        "http://arxiv.org/pdf/2404.18897v2"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2404.18851v1",
      "title": "A Comprehensive Rubric for Annotating Pathological Speech",
      "published": "2024-04-29T16:44:27Z",
      "updated": "2024-04-29T16:44:27Z",
      "summary": "Rubrics are a commonly used tool for labeling voice corpora in speech quality\nassessment, although their application in the context of pathological speech\nremains relatively limited. In this study, we introduce a comprehensive rubric\nbased on various dimensions of speech quality, including phonetics, fluency,\nand prosody. The objective is to establish standardized criteria for\nidentifying errors within the speech of individuals with Down syndrome, thereby\nenabling the development of automated assessment systems. To achieve this\nobjective, we utilized the Prautocal corpus. To assess the quality of\nannotations using our rubric, two experiments were conducted, focusing on\nphonetics and fluency. For phonetic evaluation, we employed the Goodness of\nPronunciation (GoP) metric, utilizing automatic segmentation systems and\ncorrelating the results with evaluations conducted by a specialized speech\ntherapist. While the obtained correlation values were not notably high, a\npositive trend was observed. In terms of fluency assessment, deep learning\nmodels like wav2vec were used to extract audio features, and we employed an SVM\nclassifier trained on a corpus focused on identifying fluency issues to\ncategorize Prautocal corpus samples. The outcomes highlight the complexities of\nevaluating such phenomena, with variability depending on the specific type of\ndisfluency detected.",
      "authors": [
        "Mario Corrales-Astorgano",
        "David Escudero-Mancebo",
        "Lourdes Aguilar",
        "Valle Flores-Lucas",
        "Valent\u00edn Carde\u00f1oso-Payo",
        "Carlos Vivaracho-Pascual",
        "C\u00e9sar Gonz\u00e1lez-Ferreras"
      ],
      "categories": [
        "cs.CL"
      ],
      "links": [
        "http://arxiv.org/abs/2404.18851v1",
        "http://arxiv.org/pdf/2404.18851v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2404.18842v1",
      "title": "VISION: Toward a Standardized Process for Radiology Image Management at\n  the National Level",
      "published": "2024-04-29T16:30:24Z",
      "updated": "2024-04-29T16:30:24Z",
      "summary": "The compilation and analysis of radiological images poses numerous challenges\nfor researchers. The sheer volume of data as well as the computational needs of\nalgorithms capable of operating on images are extensive. Additionally, the\nassembly of these images alone is difficult, as these exams may differ widely\nin terms of clinical context, structured annotation available for model\ntraining, modality, and patient identifiers. In this paper, we describe our\nexperiences and challenges in establishing a trusted collection of radiology\nimages linked to the United States Department of Veterans Affairs (VA)\nelectronic health record database. We also discuss implications in making this\nrepository research-ready for medical investigators. Key insights include\nuncovering the specific procedures required for transferring images from a\nclinical to a research-ready environment, as well as roadblocks and bottlenecks\nin this process that may hinder future efforts at automation.",
      "authors": [
        "Kathryn Knight",
        "Ioana Danciu",
        "Olga Ovchinnikova",
        "Jacob Hinkle",
        "Mayanka Chandra Shekar",
        "Debangshu Mukherjee",
        "Eileen McAllister",
        "Caitlin Rizy",
        "Kelly Cho",
        "Amy C. Justice",
        "Joseph Erdos",
        "Peter Kuzmak",
        "Lauren Costa",
        "Yuk-Lam Ho",
        "Reddy Madipadga",
        "Suzanne Tamang",
        "Ian Goethert"
      ],
      "categories": [
        "cs.CV"
      ],
      "links": [
        "http://arxiv.org/abs/2404.18842v1",
        "http://arxiv.org/pdf/2404.18842v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2404.18977v1",
      "title": "Computational Job Market Analysis with Natural Language Processing",
      "published": "2024-04-29T14:52:38Z",
      "updated": "2024-04-29T14:52:38Z",
      "summary": "[Abridged Abstract]\n  Recent technological advances underscore labor market dynamics, yielding\nsignificant consequences for employment prospects and increasing job vacancy\ndata across platforms and languages. Aggregating such data holds potential for\nvaluable insights into labor market demands, new skills emergence, and\nfacilitating job matching for various stakeholders. However, despite prevalent\ninsights in the private sector, transparent language technology systems and\ndata for this domain are lacking. This thesis investigates Natural Language\nProcessing (NLP) technology for extracting relevant information from job\ndescriptions, identifying challenges including scarcity of training data, lack\nof standardized annotation guidelines, and shortage of effective extraction\nmethods from job ads. We frame the problem, obtaining annotated data, and\nintroducing extraction methodologies. Our contributions include job description\ndatasets, a de-identification dataset, and a novel active learning algorithm\nfor efficient model training. We propose skill extraction using weak\nsupervision, a taxonomy-aware pre-training methodology adapting multilingual\nlanguage models to the job market domain, and a retrieval-augmented model\nleveraging multiple skill extraction datasets to enhance overall performance.\nFinally, we ground extracted information within a designated taxonomy.",
      "authors": [
        "Mike Zhang"
      ],
      "categories": [
        "cs.CL"
      ],
      "links": [
        "http://arxiv.org/abs/2404.18977v1",
        "http://arxiv.org/pdf/2404.18977v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2404.18739v1",
      "title": "Towards Dog Bark Decoding: Leveraging Human Speech Processing for\n  Automated Bark Classification",
      "published": "2024-04-29T14:41:59Z",
      "updated": "2024-04-29T14:41:59Z",
      "summary": "Similar to humans, animals make extensive use of verbal and non-verbal forms\nof communication, including a large range of audio signals. In this paper, we\naddress dog vocalizations and explore the use of self-supervised speech\nrepresentation models pre-trained on human speech to address dog bark\nclassification tasks that find parallels in human-centered tasks in speech\nrecognition. We specifically address four tasks: dog recognition, breed\nidentification, gender classification, and context grounding. We show that\nusing speech embedding representations significantly improves over simpler\nclassification baselines. Further, we also find that models pre-trained on\nlarge human speech acoustics can provide additional performance boosts on\nseveral tasks.",
      "authors": [
        "Artem Abzaliev",
        "Humberto P\u00e9rez Espinosa",
        "Rada Mihalcea"
      ],
      "categories": [
        "cs.CL"
      ],
      "links": [
        "http://arxiv.org/abs/2404.18739v1",
        "http://arxiv.org/pdf/2404.18739v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2404.18973v1",
      "title": "The Convergence of AI and Synthetic Biology: The Looming Deluge",
      "published": "2024-04-29T14:17:06Z",
      "updated": "2024-04-29T14:17:06Z",
      "summary": "The convergence of artificial intelligence (AI) and synthetic biology is\nrapidly accelerating the pace of biological discovery and engineering. AI\ntechniques, such as large language models and biological design tools, are\nenabling the automated design, build, test, and learning cycles for engineered\nbiological systems. This convergence promises to democratize synthetic biology\nand unlock novel applications across domains from medicine to environmental\nsustainability. However, it also poses significant risks around reliability,\ndual use, and governance. The opacity of AI models, the deskilling of\nworkforces, and the outdated nature of current regulatory frameworks present\nchallenges in ensuring responsible development. Urgent attention is needed to\nupdate governance structures, integrate human oversight into increasingly\nautomated workflows, and foster a culture of responsibility among the growing\ncommunity of bioengineers. Only by proactively addressing these issues can we\nrealize the transformative potential of AI-driven synthetic biology while\nmitigating its risks.",
      "authors": [
        "Cindy Vindman",
        "Benjamin Trump",
        "Christopher Cummings",
        "Madison Smith",
        "Alexander J. Titus",
        "Ken Oye",
        "Valentina Prado",
        "Eyup Turmus",
        "Igor Linkov"
      ],
      "categories": [
        "q-bio.OT"
      ],
      "links": [
        "http://arxiv.org/abs/2404.18973v1",
        "http://arxiv.org/pdf/2404.18973v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2404.18709v2",
      "title": "Three-state Opinion Dynamics for Financial Markets on Complex Networks",
      "published": "2024-04-29T13:59:10Z",
      "updated": "2024-04-30T18:31:36Z",
      "summary": "This work investigates the effects of complex networks on the collective\nbehavior of a three-state opinion formation model in economic systems. Our\nmodel considers two distinct types of investors in financial markets: noise\ntraders and fundamentalists. Financial states evolve via probabilistic dynamics\nthat include economic strategies with local and global influences. The local\nmajoritarian opinion drives noise traders' market behavior, while the market\nindex influences the financial decisions of fundamentalist agents. We introduce\na level of market anxiety $q$ present in the decision-making process that\ninfluences financial action. In our investigation, nodes of a complex network\nrepresent market agents, whereas the links represent their financial\ninteractions. We investigate the stochastic dynamics of the model on three\ndistinct network topologies, including scale-free networks, small-world\nnetworks and Erd{\\\"o}s-R\\'enyi random graphs. Our model mirrors various traits\nobserved in real-world financial return series, such as heavy-tailed return\ndistributions, volatility clustering, and short-term memory correlation of\nreturns. The histograms of returns are fitted by coupled Gaussian\ndistributions, quantitatively revealing transitions from a leptokurtic to a\nmesokurtic regime under specific economic heterogeneity. We show that the\nmarket dynamics depend mainly on the average agent connectivity, anxiety level,\nand market composition rather than on specific features of network topology.",
      "authors": [
        "Bernardo J. Zubillaga",
        "Mateus F. B. Granha",
        "Andr\u00e9 L. M. Vilela",
        "Chao Wang",
        "Kenric P. Nelson",
        "H. Eugene Stanley"
      ],
      "categories": [
        "physics.soc-ph",
        "cond-mat.stat-mech",
        "econ.GN",
        "q-fin.EC",
        "89.65.Gh, 87.23.Ge, 05.10.Ln, 64.60.Cn"
      ],
      "links": [
        "http://arxiv.org/abs/2404.18709v2",
        "http://arxiv.org/pdf/2404.18709v2"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2404.18971v1",
      "title": "Credible, Unreliable or Leaked?: Evidence Verification for Enhanced\n  Automated Fact-checking",
      "published": "2024-04-29T13:47:04Z",
      "updated": "2024-04-29T13:47:04Z",
      "summary": "Automated fact-checking (AFC) is garnering increasing attention by\nresearchers aiming to help fact-checkers combat the increasing spread of\nmisinformation online. While many existing AFC methods incorporate external\ninformation from the Web to help examine the veracity of claims, they often\noverlook the importance of verifying the source and quality of collected\n\"evidence\". One overlooked challenge involves the reliance on \"leaked\nevidence\", information gathered directly from fact-checking websites and used\nto train AFC systems, resulting in an unrealistic setting for early\nmisinformation detection. Similarly, the inclusion of information from\nunreliable sources can undermine the effectiveness of AFC systems. To address\nthese challenges, we present a comprehensive approach to evidence verification\nand filtering. We create the \"CREDible, Unreliable or LEaked\" (CREDULE)\ndataset, which consists of 91,632 articles classified as Credible, Unreliable\nand Fact checked (Leaked). Additionally, we introduce the EVidence VERification\nNetwork (EVVER-Net), trained on CREDULE to detect leaked and unreliable\nevidence in both short and long texts. EVVER-Net can be used to filter evidence\ncollected from the Web, thus enhancing the robustness of end-to-end AFC\nsystems. We experiment with various language models and show that EVVER-Net can\ndemonstrate impressive performance of up to 91.5% and 94.4% accuracy, while\nleveraging domain credibility scores along with short or long texts,\nrespectively. Finally, we assess the evidence provided by widely-used\nfact-checking datasets including LIAR-PLUS, MOCHEG, FACTIFY, NewsCLIPpings+ and\nVERITE, some of which exhibit concerning rates of leaked and unreliable\nevidence.",
      "authors": [
        "Zacharias Chrysidis",
        "Stefanos-Iordanis Papadopoulos",
        "Symeon Papadopoulos",
        "Panagiotis C. Petrantonakis"
      ],
      "categories": [
        "cs.CL",
        "cs.CY",
        "cs.IR",
        "cs.SI"
      ],
      "links": [
        "http://dx.doi.org/10.1145/3643491.3660278",
        "http://arxiv.org/abs/2404.18971v1",
        "http://arxiv.org/pdf/2404.18971v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2404.18681v1",
      "title": "LLMClean: Context-Aware Tabular Data Cleaning via LLM-Generated OFDs",
      "published": "2024-04-29T13:24:23Z",
      "updated": "2024-04-29T13:24:23Z",
      "summary": "Machine learning's influence is expanding rapidly, now integral to\ndecision-making processes from corporate strategy to the advancements in\nIndustry 4.0. The efficacy of Artificial Intelligence broadly hinges on the\ncaliber of data used during its training phase; optimal performance is tied to\nexceptional data quality. Data cleaning tools, particularly those that exploit\nfunctional dependencies within ontological frameworks or context models, are\ninstrumental in augmenting data quality. Nevertheless, crafting these context\nmodels is a demanding task, both in terms of resources and expertise, often\nnecessitating specialized knowledge from domain experts.\n  In light of these challenges, this paper introduces an innovative approach,\ncalled LLMClean, for the automated generation of context models, utilizing\nLarge Language Models to analyze and understand various datasets. LLMClean\nencompasses a sequence of actions, starting with categorizing the dataset,\nextracting or mapping relevant models, and ultimately synthesizing the context\nmodel. To demonstrate its potential, we have developed and tested a prototype\nthat applies our approach to three distinct datasets from the Internet of\nThings, healthcare, and Industry 4.0 sectors. The results of our evaluation\nindicate that our automated approach can achieve data cleaning efficacy\ncomparable with that of context models crafted by human experts.",
      "authors": [
        "Fabian Biester",
        "Mohamed Abdelaal",
        "Daniel Del Gaudio"
      ],
      "categories": [
        "cs.DB"
      ],
      "links": [
        "http://arxiv.org/abs/2404.18681v1",
        "http://arxiv.org/pdf/2404.18681v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2404.18677v1",
      "title": "Towards the First Code Contribution: Processes and Information Needs",
      "published": "2024-04-29T13:19:24Z",
      "updated": "2024-04-29T13:19:24Z",
      "summary": "Newcomers to a software project must overcome many barriers before they can\nsuccessfully place their first code contribution, and they often struggle to\nfind information that is relevant to them. In this work, we argue that much of\nthe information needed by newcomers already exists, albeit scattered among many\ndifferent sources, and that many barriers can be addressed by automatically\nidentifying, extracting, generating, summarizing, and presenting documentation\nthat is specifically aimed and customized for newcomers. To gain a detailed\nunderstanding of the processes followed by newcomers and their information\nneeds before making their first code contribution, we conducted an empirical\nstudy. Based on a survey with about 100 practitioners, grounded theory\nanalysis, and validation interviews, we contribute a 16-step model for the\nprocesses followed by newcomers to a software project and we identify relevant\ninformation, along with individual and project characteristics that influence\nthe relevancy of information types and sources. Our findings form an essential\nstep towards automated tool support that provides relevant information to\nproject newcomers in each step of their contribution processes.",
      "authors": [
        "Christoph Treude",
        "Marco A. Gerosa",
        "Igor Steinmacher"
      ],
      "categories": [
        "cs.SE"
      ],
      "links": [
        "http://arxiv.org/abs/2404.18677v1",
        "http://arxiv.org/pdf/2404.18677v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2404.18663v1",
      "title": "Terrain characterisation for online adaptability of automated sonar\n  processing: Lessons learnt from operationally applying ATR to sidescan sonar\n  in MCM applications",
      "published": "2024-04-29T12:48:42Z",
      "updated": "2024-04-29T12:48:42Z",
      "summary": "The performance of Automated Recognition (ATR) algorithms on side-scan sonar\nimagery has shown to degrade rapidly when deployed on non benign environments.\nComplex seafloors and acoustic artefacts constitute distractors in the form of\nstrong textural patterns, creating false detections or preventing detections of\ntrue objects. This paper presents two online seafloor characterisation\ntechniques to improve explainability during Autonomous Underwater Vehicles\n(AUVs) missions. Importantly and as opposed to previous work in the domain,\nthese techniques are not based on a model and require limited input from human\noperators, making it suitable for real-time onboard processing. Both techniques\nrely on an unsupervised machine learning approach to extract terrain features\nwhich relate to the human understanding of terrain complexity. The first\ntechnnique provides a quantitative, application-driven terrain characterisation\nmetric based on the performance of an ATR algorithm. The second method provides\na way to incorporate subject matter expertise and enables contextualisation and\nexplainability in support for scenario-dependent subjective terrain\ncharacterisation. The terrain complexity matches the expectation of seasoned\nusers making this tool desirable and trustworthy in comparison to traditional\nunsupervised approaches. We finally detail an application of these techniques\nto repair a Mine Countermeasures (MCM) mission carried with SeeByte autonomy\nframework Neptune.",
      "authors": [
        "Thomas Guerneve",
        "Stephanos Loizou",
        "Andrea Munafo",
        "Pierre-Yves Mignotte"
      ],
      "categories": [
        "cs.CV",
        "cs.LG",
        "cs.RO",
        "cs.SE"
      ],
      "links": [
        "http://arxiv.org/abs/2404.18663v1",
        "http://arxiv.org/pdf/2404.18663v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2404.18598v2",
      "title": "Anywhere: A Multi-Agent Framework for User-Guided, Reliable, and Diverse\n  Foreground-Conditioned Image Generation",
      "published": "2024-04-29T11:13:37Z",
      "updated": "2025-02-24T15:59:18Z",
      "summary": "Recent advancements in image-conditioned image generation have demonstrated\nsubstantial progress. However, foreground-conditioned image generation remains\nunderexplored, encountering challenges such as compromised object integrity,\nforeground-background inconsistencies, limited diversity, and reduced control\nflexibility. These challenges arise from current end-to-end inpainting models,\nwhich suffer from inaccurate training masks, limited foreground semantic\nunderstanding, data distribution biases, and inherent interference between\nvisual and textual prompts. To overcome these limitations, we present Anywhere,\na multi-agent framework that departs from the traditional end-to-end approach.\nIn this framework, each agent is specialized in a distinct aspect, such as\nforeground understanding, diversity enhancement, object integrity protection,\nand textual prompt consistency. Our framework is further enhanced with the\nability to incorporate optional user textual inputs, perform automated quality\nassessments, and initiate re-generation as needed. Comprehensive experiments\ndemonstrate that this modular design effectively overcomes the limitations of\nexisting end-to-end models, resulting in higher fidelity, quality, diversity\nand controllability in foreground-conditioned image generation. Additionally,\nthe Anywhere framework is extensible, allowing it to benefit from future\nadvancements in each individual agent.",
      "authors": [
        "Tianyidan Xie",
        "Rui Ma",
        "Qian Wang",
        "Xiaoqian Ye",
        "Feixuan Liu",
        "Ying Tai",
        "Zhenyu Zhang",
        "Lanjun Wang",
        "Zili Yi"
      ],
      "categories": [
        "cs.CV",
        "cs.GR"
      ],
      "links": [
        "http://arxiv.org/abs/2404.18598v2",
        "http://arxiv.org/pdf/2404.18598v2"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2404.18580v2",
      "title": "Data-Driven Dynamics Modeling of Miniature Robotic Blimps Using Neural\n  ODEs With Parameter Auto-Tuning",
      "published": "2024-04-29T10:41:30Z",
      "updated": "2024-10-21T06:25:33Z",
      "summary": "Miniature robotic blimps, as one type of lighter-than-air aerial vehicles,\nhave attracted increasing attention in the science and engineering community\nfor their enhanced safety, extended endurance, and quieter operation compared\nto quadrotors. Accurately modeling the dynamics of these robotic blimps poses a\nsignificant challenge due to the complex aerodynamics stemming from their large\nlifting bodies. Traditional first-principle models have difficulty obtaining\naccurate aerodynamic parameters and often overlook high-order nonlinearities,\nthus coming to its limit in modeling the motion dynamics of miniature robotic\nblimps. To tackle this challenge, this letter proposes the Auto-tuning\nBlimp-oriented Neural Ordinary Differential Equation method (ABNODE), a\ndata-driven approach that integrates first-principle and neural network\nmodeling. Spiraling motion experiments of robotic blimps are conducted,\ncomparing the ABNODE with first-principle and other data-driven benchmark\nmodels, the results of which demonstrate the effectiveness of the proposed\nmethod.",
      "authors": [
        "Yongjian Zhu",
        "Hao Cheng",
        "Feitian Zhang"
      ],
      "categories": [
        "cs.RO",
        "cs.SY",
        "eess.SY"
      ],
      "links": [
        "http://dx.doi.org/10.1109/LRA.2024.3484182",
        "http://arxiv.org/abs/2404.18580v2",
        "http://arxiv.org/pdf/2404.18580v2"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2404.18577v1",
      "title": "Assessing Quality Metrics for Neural Reality Gap Input Mitigation in\n  Autonomous Driving Testing",
      "published": "2024-04-29T10:37:38Z",
      "updated": "2024-04-29T10:37:38Z",
      "summary": "Simulation-based testing of automated driving systems (ADS) is the industry\nstandard, being a controlled, safe, and cost-effective alternative to\nreal-world testing. Despite these advantages, virtual simulations often fail to\naccurately replicate real-world conditions like image fidelity, texture\nrepresentation, and environmental accuracy. This can lead to significant\ndifferences in ADS behavior between simulated and real-world domains, a\nphenomenon known as the sim2real gap. Researchers have used Image-to-Image\n(I2I) neural translation to mitigate the sim2real gap, enhancing the realism of\nsimulated environments by transforming synthetic data into more authentic\nrepresentations of real-world conditions. However, while promising, these\ntechniques may potentially introduce artifacts, distortions, or inconsistencies\nin the generated data that can affect the effectiveness of ADS testing. In our\nempirical study, we investigated how the quality of image-to-image (I2I)\ntechniques influences the mitigation of the sim2real gap, using a set of\nestablished metrics from the literature. We evaluated two popular generative\nI2I architectures, pix2pix, and CycleGAN, across two ADS perception tasks at a\nmodel level, namely vehicle detection and end-to-end lane keeping, using paired\nsimulated and real-world datasets. Our findings reveal that the effectiveness\nof I2I architectures varies across different ADS tasks, and existing evaluation\nmetrics do not consistently align with the ADS behavior. Thus, we conducted\ntask-specific fine-tuning of perception metrics, which yielded a stronger\ncorrelation. Our findings indicate that a perception metric that incorporates\nsemantic elements, tailored to each task, can facilitate selecting the most\nappropriate I2I technique for a reliable assessment of the sim2real gap\nmitigation.",
      "authors": [
        "Stefano Carlo Lambertenghi",
        "Andrea Stocco"
      ],
      "categories": [
        "cs.SE"
      ],
      "links": [
        "http://arxiv.org/abs/2404.18577v1",
        "http://arxiv.org/pdf/2404.18577v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2404.18573v2",
      "title": "Predicting Safety Misbehaviours in Autonomous Driving Systems using\n  Uncertainty Quantification",
      "published": "2024-04-29T10:28:28Z",
      "updated": "2025-02-13T11:09:19Z",
      "summary": "The automated real-time recognition of unexpected situations plays a crucial\nrole in the safety of autonomous vehicles, especially in unsupported and\nunpredictable scenarios. This paper evaluates different Bayesian uncertainty\nquantification methods from the deep learning domain for the anticipatory\ntesting of safety-critical misbehaviours during system-level simulation-based\ntesting. Specifically, we compute uncertainty scores as the vehicle executes,\nfollowing the intuition that high uncertainty scores are indicative of\nunsupported runtime conditions that can be used to distinguish safe from\nfailure-inducing driving behaviors. In our study, we conducted an evaluation of\nthe effectiveness and computational overhead associated with two Bayesian\nuncertainty quantification methods, namely MC- Dropout and Deep Ensembles, for\nmisbehaviour avoidance. Overall, for three benchmarks from the Udacity\nsimulator comprising both out-of-distribution and unsafe conditions introduced\nvia mutation testing, both methods successfully detected a high number of\nout-of-bounds episodes providing early warnings several seconds in advance,\noutperforming two state-of-the-art misbehaviour prediction methods based on\nautoencoders and attention maps in terms of effectiveness and efficiency.\nNotably, Deep Ensembles detected most misbehaviours without any false alarms\nand did so even when employing a relatively small number of models, making them\ncomputationally feasible for real-time detection. Our findings suggest that\nincorporating uncertainty quantification methods is a viable approach for\nbuilding fail-safe mechanisms in deep neural network-based autonomous vehicles.",
      "authors": [
        "Ruben Grewal",
        "Paolo Tonella",
        "Andrea Stocco"
      ],
      "categories": [
        "cs.LG",
        "cs.RO",
        "cs.SE"
      ],
      "links": [
        "http://arxiv.org/abs/2404.18573v2",
        "http://arxiv.org/pdf/2404.18573v2"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2404.18567v2",
      "title": "Double Backdoored: Converting Code Large Language Model Backdoors to\n  Traditional Malware via Adversarial Instruction Tuning Attacks",
      "published": "2024-04-29T10:14:58Z",
      "updated": "2025-03-07T00:46:35Z",
      "summary": "Instruction-tuned Large Language Models designed for coding tasks are\nincreasingly employed as AI coding assistants. However, the cybersecurity\nvulnerabilities and implications arising from the widespread integration of\nthese models are not yet fully understood due to limited research in this\ndomain. This work investigates novel techniques for transitioning backdoors\nfrom the AI/ML domain to traditional computer malware, shedding light on the\ncritical intersection of AI and cyber/software security. To explore this\nintersection, we present MalInstructCoder, a framework designed to\ncomprehensively assess the cybersecurity vulnerabilities of instruction-tuned\nCode LLMs. MalInstructCoder introduces an automated data poisoning pipeline to\ninject malicious code snippets into benign code, poisoning instruction\nfine-tuning data while maintaining functional validity. It presents two\npractical adversarial instruction tuning attacks with real-world security\nimplications: the clean prompt poisoning attack and the backdoor attack. These\nattacks aim to manipulate Code LLMs to generate code incorporating malicious or\nharmful functionality under specific attack scenarios while preserving intended\nfunctionality. We conduct a comprehensive investigation into the exploitability\nof the code-specific instruction tuning process involving three\nstate-of-the-art Code LLMs: CodeLlama, DeepSeek-Coder, and StarCoder2. Our\nfindings reveal that these models are highly vulnerable to our attacks.\nSpecifically, the clean prompt poisoning attack achieves the ASR@1 ranging from\nover 75% to 86% by poisoning only 1% (162 samples) of the instruction\nfine-tuning dataset. Similarly, the backdoor attack achieves the ASR@1 ranging\nfrom 76% to 86% with a 0.5% poisoning rate. Our study sheds light on the\ncritical cybersecurity risks posed by instruction-tuned Code LLMs and\nhighlights the urgent need for robust defense mechanisms.",
      "authors": [
        "Md Imran Hossen",
        "Sai Venkatesh Chilukoti",
        "Liqun Shan",
        "Sheng Chen",
        "Yinzhi Cao",
        "Xiali Hei"
      ],
      "categories": [
        "cs.CR"
      ],
      "links": [
        "http://arxiv.org/abs/2404.18567v2",
        "http://arxiv.org/pdf/2404.18567v2"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2404.18557v1",
      "title": "Can GPT-4 do L2 analytic assessment?",
      "published": "2024-04-29T10:00:00Z",
      "updated": "2024-04-29T10:00:00Z",
      "summary": "Automated essay scoring (AES) to evaluate second language (L2) proficiency\nhas been a firmly established technology used in educational contexts for\ndecades. Although holistic scoring has seen advancements in AES that match or\neven exceed human performance, analytic scoring still encounters issues as it\ninherits flaws and shortcomings from the human scoring process. The recent\nintroduction of large language models presents new opportunities for automating\nthe evaluation of specific aspects of L2 writing proficiency. In this paper, we\nperform a series of experiments using GPT-4 in a zero-shot fashion on a\npublicly available dataset annotated with holistic scores based on the Common\nEuropean Framework of Reference and aim to extract detailed information about\ntheir underlying analytic components. We observe significant correlations\nbetween the automatically predicted analytic scores and multiple features\nassociated with the individual proficiency components.",
      "authors": [
        "Stefano Bann\u00f2",
        "Hari Krishna Vydana",
        "Kate M. Knill",
        "Mark J. F. Gales"
      ],
      "categories": [
        "cs.CL"
      ],
      "links": [
        "http://arxiv.org/abs/2404.18557v1",
        "http://arxiv.org/pdf/2404.18557v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2404.18470v2",
      "title": "ECC Analyzer: Extract Trading Signal from Earnings Conference Calls\n  using Large Language Model for Stock Performance Prediction",
      "published": "2024-04-29T07:11:39Z",
      "updated": "2024-08-29T23:13:56Z",
      "summary": "In the realm of financial analytics, leveraging unstructured data, such as\nearnings conference calls (ECCs), to forecast stock volatility is a critical\nchallenge that has attracted both academics and investors. While previous\nstudies have used multimodal deep learning-based models to obtain a general\nview of ECCs for volatility predicting, they often fail to capture detailed,\ncomplex information. Our research introduces a novel framework: \\textbf{ECC\nAnalyzer}, which utilizes large language models (LLMs) to extract richer, more\npredictive content from ECCs to aid the model's prediction performance. We use\nthe pre-trained large models to extract textual and audio features from ECCs\nand implement a hierarchical information extraction strategy to extract more\nfine-grained information. This strategy first extracts paragraph-level general\ninformation by summarizing the text and then extracts fine-grained focus\nsentences using Retrieval-Augmented Generation (RAG). These features are then\nfused through multimodal feature fusion to perform volatility prediction.\nExperimental results demonstrate that our model outperforms traditional\nanalytical benchmarks, confirming the effectiveness of advanced LLM techniques\nin financial analysis.",
      "authors": [
        "Yupeng Cao",
        "Zhi Chen",
        "Qingyun Pei",
        "Nathan Jinseok Lee",
        "K. P. Subbalakshmi",
        "Papa Momar Ndiaye"
      ],
      "categories": [
        "cs.CE",
        "cs.AI",
        "cs.CL",
        "q-fin.RM",
        "q-fin.TR"
      ],
      "links": [
        "http://arxiv.org/abs/2404.18470v2",
        "http://arxiv.org/pdf/2404.18470v2"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2404.18963v1",
      "title": "RE-GrievanceAssist: Enhancing Customer Experience through ML-Powered\n  Complaint Management",
      "published": "2024-04-29T07:03:23Z",
      "updated": "2024-04-29T07:03:23Z",
      "summary": "In recent years, digital platform companies have faced increasing challenges\nin managing customer complaints, driven by widespread consumer adoption. This\npaper introduces an end-to-end pipeline, named RE-GrievanceAssist, designed\nspecifically for real estate customer complaint management. The pipeline\nconsists of three key components: i) response/no-response ML model using TF-IDF\nvectorization and XGBoost classifier ; ii) user type classifier using fasttext\nclassifier; iii) issue/sub-issue classifier using TF-IDF vectorization and\nXGBoost classifier. Finally, it has been deployed as a batch job in Databricks,\nresulting in a remarkable 40% reduction in overall manual effort with monthly\ncost reduction of Rs 1,50,000 since August 2023.",
      "authors": [
        "Venkatesh C",
        "Harshit Oberoi",
        "Anurag Kumar Pandey",
        "Anil Goyal",
        "Nikhil Sikka"
      ],
      "categories": [
        "cs.LG",
        "cs.CL"
      ],
      "links": [
        "http://arxiv.org/abs/2404.18963v1",
        "http://arxiv.org/pdf/2404.18963v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2404.18462v1",
      "title": "Self-supervised contrastive learning of radio data for source detection,\n  classification and peculiar object discovery",
      "published": "2024-04-29T06:46:25Z",
      "updated": "2024-04-29T06:46:25Z",
      "summary": "New advancements in radio data post-processing are underway within the SKA\nprecursor community, aiming to facilitate the extraction of scientific results\nfrom survey images through a semi-automated approach. Several of these\ndevelopments leverage deep learning (DL) methodologies for diverse tasks,\nincluding source detection, object or morphology classification, and anomaly\ndetection. Despite substantial progress, the full potential of these methods\noften remains untapped due to challenges associated with training large\nsupervised models, particularly in the presence of small and class-unbalanced\nlabelled datasets. Self-supervised learning has recently established itself as\na powerful methodology to deal with some of the aforementioned challenges, by\ndirectly learning a lower-dimensional representation from large samples of\nunlabelled data. The resulting model and data representation can then be used\nfor data inspection and various downstream tasks if a small subset of labelled\ndata is available. In this work, we explored contrastive learning methods to\nlearn suitable radio data representation from unlabelled images taken from the\nASKAP EMU and SARAO MeerKAT GPS surveys. We evaluated trained models and the\nobtained data representation over smaller labelled datasets, also taken from\ndifferent radio surveys, in selected analysis tasks: source detection and\nclassification, and search for objects with peculiar morphology. For all\nexplored downstream tasks, we reported and discussed the benefits brought by\nself-supervised foundational models built on radio data.",
      "authors": [
        "S. Riggi",
        "T. Cecconello",
        "S. Palazzo",
        "A. M. Hopkins",
        "N. Gupta",
        "C. Bordiu",
        "A. Ingallinera",
        "C. Buemi",
        "F. Bufano",
        "F. Cavallaro",
        "M. D. Filipovi\u0107",
        "P. Leto",
        "S. Loru",
        "A. C. Ruggeri",
        "C. Trigilio",
        "G. Umana",
        "F. Vitello"
      ],
      "categories": [
        "astro-ph.IM"
      ],
      "links": [
        "http://dx.doi.org/10.1017/pasa.2024.84",
        "http://arxiv.org/abs/2404.18462v1",
        "http://arxiv.org/pdf/2404.18462v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2404.18386v1",
      "title": "Network Intent Decomposition and Optimization for Energy-Aware Radio\n  Access Network",
      "published": "2024-04-29T02:46:09Z",
      "updated": "2024-04-29T02:46:09Z",
      "summary": "With recent advancements in the sixth generation (6G) communication\ntechnologies, more vertical industries have encountered diverse network\nservices. How to reduce energy consumption is critical to meet the expectation\nof the quality of diverse network services. In particular, the number of base\nstations in 6G is huge with coupled adjustable network parameters. However, the\nproblem is complex with multiple network objectives and parameters. Network\nintents are difficult to map to individual network elements and require\nenhanced automation capabilities. In this paper, we present a network intent\ndecomposition and optimization mechanism in an energy-aware radio access\nnetwork scenario. By characterizing the intent ontology with a standard\ntemplate, we present a generic network intent representation framework. Then we\npropose a novel intent modeling method using Knowledge Acquisition in automated\nSpecification language, which can model the network ontology. To clarify the\nnumber and types of network objectives and energy-saving operations, we develop\na Softgoal Interdependency Graph-based network intent decomposition model, and\nthus, a network intent decomposition algorithm is presented. Simulation results\ndemonstrate that the proposed algorithm outperforms without conflict analysis\nin intent decomposition time. Moreover, we design a deep Q-network-assisted\nintent optimization scheme to validate the performance gain.",
      "authors": [
        "Yao Wang",
        "Yijun Yu",
        "Yexing Li",
        "Dong Li",
        "Xiaoxue Zhao",
        "Chungang Yang"
      ],
      "categories": [
        "cs.NI"
      ],
      "links": [
        "http://arxiv.org/abs/2404.18386v1",
        "http://arxiv.org/pdf/2404.18386v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2404.18326v1",
      "title": "SAFE-RL: Saliency-Aware Counterfactual Explainer for Deep Reinforcement\n  Learning Policies",
      "published": "2024-04-28T21:47:34Z",
      "updated": "2024-04-28T21:47:34Z",
      "summary": "While Deep Reinforcement Learning (DRL) has emerged as a promising solution\nfor intricate control tasks, the lack of explainability of the learned policies\nimpedes its uptake in safety-critical applications, such as automated driving\nsystems (ADS). Counterfactual (CF) explanations have recently gained prominence\nfor their ability to interpret black-box Deep Learning (DL) models. CF examples\nare associated with minimal changes in the input, resulting in a complementary\noutput by the DL model. Finding such alternations, particularly for\nhigh-dimensional visual inputs, poses significant challenges. Besides, the\ntemporal dependency introduced by the reliance of the DRL agent action on a\nhistory of past state observations further complicates the generation of CF\nexamples. To address these challenges, we propose using a saliency map to\nidentify the most influential input pixels across the sequence of past observed\nstates by the agent. Then, we feed this map to a deep generative model,\nenabling the generation of plausible CFs with constrained modifications centred\non the salient regions. We evaluate the effectiveness of our framework in\ndiverse domains, including ADS, Atari Pong, Pacman and space-invaders games,\nusing traditional performance metrics such as validity, proximity and sparsity.\nExperimental results demonstrate that this framework generates more informative\nand plausible CFs than the state-of-the-art for a wide range of environments\nand DRL agents. In order to foster research in this area, we have made our\ndatasets and codes publicly available at\nhttps://github.com/Amir-Samadi/SAFE-RL.",
      "authors": [
        "Amir Samadi",
        "Konstantinos Koufos",
        "Kurt Debattista",
        "Mehrdad Dianati"
      ],
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "links": [
        "http://arxiv.org/abs/2404.18326v1",
        "http://arxiv.org/pdf/2404.18326v1"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2404.18280v2",
      "title": "Tracy, Traces, and Transducers: Computable Counterexamples and\n  Explanations for HyperLTL Model-Checking",
      "published": "2024-04-28T18:52:39Z",
      "updated": "2024-11-26T11:20:31Z",
      "summary": "HyperLTL model-checking enables the automated verification of\ninformation-flow properties for security-critical systems. However, it only\nprovides a binary answer. Here, we introduce two paradigms to compute\ncounterexamples and explanations for HyperLTL model-checking, thereby\nconsiderably increasing its usefulness. Both paradigms are based on the maxim\n``counterexamples/explanations are Skolem functions for the existentially\nquantified trace variables''.\n  Our first paradigm is complete (everything can be explained), but restricted\nto ultimately periodic system traces. The second paradigm works with (Turing\nmachine) computable Skolem functions and is therefore much more general, but\nalso shown incomplete (not everything can computably be explained). Finally, we\nprove that it is decidable whether a given finite transition system and a\nformula have computable Skolem functions witnessing that the system satisfies\nthe formula. Our algorithm also computes transducers implementing computable\nSkolem functions, if they exist.",
      "authors": [
        "Sarah Winter",
        "Martin Zimmermann"
      ],
      "categories": [
        "cs.LO",
        "cs.FL"
      ],
      "links": [
        "http://arxiv.org/abs/2404.18280v2",
        "http://arxiv.org/pdf/2404.18280v2"
      ],
      "primary_search_term": "large language models",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    }
  ]
}