{
  "query": "all:artificial intelligence AND (labor market OR employment OR jobs OR workforce OR automation)",
  "date_collected": "2025-06-01T20:31:57.050604",
  "target_period": "2025-05",
  "papers": [
    {
      "id": "http://arxiv.org/abs/2505.23764v1",
      "title": "MMSI-Bench: A Benchmark for Multi-Image Spatial Intelligence",
      "published": "2025-05-29T17:59:52Z",
      "updated": "2025-05-29T17:59:52Z",
      "summary": "Spatial intelligence is essential for multimodal large language models\n(MLLMs) operating in the complex physical world. Existing benchmarks, however,\nprobe only single-image relations and thus fail to assess the multi-image\nspatial reasoning that real-world deployments demand. We introduce MMSI-Bench,\na VQA benchmark dedicated to multi-image spatial intelligence. Six 3D-vision\nresearchers spent more than 300 hours meticulously crafting 1,000 challenging,\nunambiguous multiple-choice questions from over 120,000 images, each paired\nwith carefully designed distractors and a step-by-step reasoning process. We\nconduct extensive experiments and thoroughly evaluate 34 open-source and\nproprietary MLLMs, observing a wide gap: the strongest open-source model\nattains roughly 30% accuracy and OpenAI's o3 reasoning model reaches 40%, while\nhumans score 97%. These results underscore the challenging nature of MMSI-Bench\nand the substantial headroom for future research. Leveraging the annotated\nreasoning processes, we also provide an automated error analysis pipeline that\ndiagnoses four dominant failure modes, including (1) grounding errors, (2)\noverlap-matching and scene-reconstruction errors, (3) situation-transformation\nreasoning errors, and (4) spatial-logic errors, offering valuable insights for\nadvancing multi-image spatial intelligence. Project page:\nhttps://runsenxu.com/projects/MMSI_Bench .",
      "authors": [
        "Sihan Yang",
        "Runsen Xu",
        "Yiman Xie",
        "Sizhe Yang",
        "Mo Li",
        "Jingli Lin",
        "Chenming Zhu",
        "Xiaochen Chen",
        "Haodong Duan",
        "Xiangyu Yue",
        "Dahua Lin",
        "Tai Wang",
        "Jiangmiao Pang"
      ],
      "categories": [
        "cs.CV",
        "cs.CL"
      ],
      "links": [
        "http://arxiv.org/abs/2505.23764v1",
        "http://arxiv.org/pdf/2505.23764v1"
      ],
      "primary_search_term": "artificial intelligence",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2505.23762v1",
      "title": "ZeroGUI: Automating Online GUI Learning at Zero Human Cost",
      "published": "2025-05-29T17:59:51Z",
      "updated": "2025-05-29T17:59:51Z",
      "summary": "The rapid advancement of large Vision-Language Models (VLMs) has propelled\nthe development of pure-vision-based GUI Agents, capable of perceiving and\noperating Graphical User Interfaces (GUI) to autonomously fulfill user\ninstructions. However, existing approaches usually adopt an offline learning\nframework, which faces two core limitations: (1) heavy reliance on high-quality\nmanual annotations for element grounding and action supervision, and (2)\nlimited adaptability to dynamic and interactive environments. To address these\nlimitations, we propose ZeroGUI, a scalable, online learning framework for\nautomating GUI Agent training at Zero human cost. Specifically, ZeroGUI\nintegrates (i) VLM-based automatic task generation to produce diverse training\ngoals from the current environment state, (ii) VLM-based automatic reward\nestimation to assess task success without hand-crafted evaluation functions,\nand (iii) two-stage online reinforcement learning to continuously interact with\nand learn from GUI environments. Experiments on two advanced GUI Agents\n(UI-TARS and Aguvis) demonstrate that ZeroGUI significantly boosts performance\nacross OSWorld and AndroidLab environments. The code is available at\nhttps://github.com/OpenGVLab/ZeroGUI.",
      "authors": [
        "Chenyu Yang",
        "Shiqian Su",
        "Shi Liu",
        "Xuan Dong",
        "Yue Yu",
        "Weijie Su",
        "Xuehui Wang",
        "Zhaoyang Liu",
        "Jinguo Zhu",
        "Hao Li",
        "Wenhai Wang",
        "Yu Qiao",
        "Xizhou Zhu",
        "Jifeng Dai"
      ],
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CV"
      ],
      "links": [
        "http://arxiv.org/abs/2505.23762v1",
        "http://arxiv.org/pdf/2505.23762v1"
      ],
      "primary_search_term": "artificial intelligence",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2505.23754v1",
      "title": "DeepTheorem: Advancing LLM Reasoning for Theorem Proving Through Natural\n  Language and Reinforcement Learning",
      "published": "2025-05-29T17:59:39Z",
      "updated": "2025-05-29T17:59:39Z",
      "summary": "Theorem proving serves as a major testbed for evaluating complex reasoning\nabilities in large language models (LLMs). However, traditional automated\ntheorem proving (ATP) approaches rely heavily on formal proof systems that\npoorly align with LLMs' strength derived from informal, natural language\nknowledge acquired during pre-training. In this work, we propose DeepTheorem, a\ncomprehensive informal theorem-proving framework exploiting natural language to\nenhance LLM mathematical reasoning. DeepTheorem includes a large-scale\nbenchmark dataset consisting of 121K high-quality IMO-level informal theorems\nand proofs spanning diverse mathematical domains, rigorously annotated for\ncorrectness, difficulty, and topic categories, accompanied by systematically\nconstructed verifiable theorem variants. We devise a novel reinforcement\nlearning strategy (RL-Zero) explicitly tailored to informal theorem proving,\nleveraging the verified theorem variants to incentivize robust mathematical\ninference. Additionally, we propose comprehensive outcome and process\nevaluation metrics examining proof correctness and the quality of reasoning\nsteps. Extensive experimental analyses demonstrate DeepTheorem significantly\nimproves LLM theorem-proving performance compared to existing datasets and\nsupervised fine-tuning protocols, achieving state-of-the-art accuracy and\nreasoning quality. Our findings highlight DeepTheorem's potential to\nfundamentally advance automated informal theorem proving and mathematical\nexploration.",
      "authors": [
        "Ziyin Zhang",
        "Jiahao Xu",
        "Zhiwei He",
        "Tian Liang",
        "Qiuzhi Liu",
        "Yansi Li",
        "Linfeng Song",
        "Zhengwen Liang",
        "Zhuosheng Zhang",
        "Rui Wang",
        "Zhaopeng Tu",
        "Haitao Mi",
        "Dong Yu"
      ],
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "links": [
        "http://arxiv.org/abs/2505.23754v1",
        "http://arxiv.org/pdf/2505.23754v1"
      ],
      "primary_search_term": "artificial intelligence",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2505.23710v1",
      "title": "From Connectivity to Autonomy: The Dawn of Self-Evolving Communication\n  Systems",
      "published": "2025-05-29T17:45:02Z",
      "updated": "2025-05-29T17:45:02Z",
      "summary": "This paper envisions 6G as a self-evolving telecom ecosystem, where AI-driven\nintelligence enables dynamic adaptation beyond static connectivity. We explore\nthe key enablers of autonomous communication systems, spanning reconfigurable\ninfrastructure, adaptive middleware, and intelligent network functions,\nalongside multi-agent collaboration for distributed decision-making. We explore\nhow these methodologies align with emerging industrial IoT frameworks, ensuring\nseamless integration within digital manufacturing processes. Our findings\nemphasize the potential for improved real-time decision-making, optimizing\nefficiency, and reducing latency in networked control systems. The discussion\naddresses ethical challenges, research directions, and standardization efforts,\nconcluding with a technology stack roadmap to guide future developments. By\nleveraging state-of-the-art 6G network management techniques, this research\ncontributes to the next generation of intelligent automation solutions,\nbridging the gap between theoretical advancements and real-world industrial\napplications.",
      "authors": [
        "Zeinab Nezami",
        "Syed Danial Ali Shah",
        "Maryam Hafeez",
        "Karim Djemame",
        "Syed Ali Raza Zaidi"
      ],
      "categories": [
        "eess.SY",
        "cs.AI",
        "cs.DC",
        "cs.ET",
        "cs.SY"
      ],
      "links": [
        "http://arxiv.org/abs/2505.23710v1",
        "http://arxiv.org/pdf/2505.23710v1"
      ],
      "primary_search_term": "artificial intelligence",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2505.23695v1",
      "title": "Data-to-Dashboard: Multi-Agent LLM Framework for Insightful\n  Visualization in Enterprise Analytics",
      "published": "2025-05-29T17:32:15Z",
      "updated": "2025-05-29T17:32:15Z",
      "summary": "The rapid advancement of LLMs has led to the creation of diverse agentic\nsystems in data analysis, utilizing LLMs' capabilities to improve insight\ngeneration and visualization. In this paper, we present an agentic system that\nautomates the data-to-dashboard pipeline through modular LLM agents capable of\ndomain detection, concept extraction, multi-perspective analysis generation,\nand iterative self-reflection. Unlike existing chart QA systems, our framework\nsimulates the analytical reasoning process of business analysts by retrieving\ndomain-relevant knowledge and adapting to diverse datasets without relying on\nclosed ontologies or question templates.\n  We evaluate our system on three datasets across different domains.\nBenchmarked against GPT-4o with a single-prompt baseline, our approach shows\nimproved insightfulness, domain relevance, and analytical depth, as measured by\ntailored evaluation metrics and qualitative human assessment.\n  This work contributes a novel modular pipeline to bridge the path from raw\ndata to visualization, and opens new opportunities for human-in-the-loop\nvalidation by domain experts in business analytics. All code can be found here:\nhttps://github.com/77luvC/D2D_Data2Dashboard",
      "authors": [
        "Ran Zhang",
        "Mohannad Elhamod"
      ],
      "categories": [
        "cs.AI"
      ],
      "links": [
        "http://arxiv.org/abs/2505.23695v1",
        "http://arxiv.org/pdf/2505.23695v1"
      ],
      "primary_search_term": "artificial intelligence",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2505.23671v1",
      "title": "GSO: Challenging Software Optimization Tasks for Evaluating SWE-Agents",
      "published": "2025-05-29T17:14:55Z",
      "updated": "2025-05-29T17:14:55Z",
      "summary": "Developing high-performance software is a complex task that requires\nspecialized expertise. We introduce GSO, a benchmark for evaluating language\nmodels' capabilities in developing high-performance software. We develop an\nautomated pipeline that generates and executes performance tests to analyze\nrepository commit histories to identify 102 challenging optimization tasks\nacross 10 codebases, spanning diverse domains and programming languages. An\nagent is provided with a codebase and performance test as a precise\nspecification, and tasked to improve the runtime efficiency, which is measured\nagainst the expert developer optimization. Our quantitative evaluation reveals\nthat leading SWE-Agents struggle significantly, achieving less than 5% success\nrate, with limited improvements even with inference-time scaling. Our\nqualitative analysis identifies key failure modes, including difficulties with\nlow-level languages, practicing lazy optimization strategies, and challenges in\naccurately localizing bottlenecks. We release the code and artifacts of our\nbenchmark along with agent trajectories to enable future research.",
      "authors": [
        "Manish Shetty",
        "Naman Jain",
        "Jinjian Liu",
        "Vijay Kethanaboyina",
        "Koushik Sen",
        "Ion Stoica"
      ],
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "links": [
        "http://arxiv.org/abs/2505.23671v1",
        "http://arxiv.org/pdf/2505.23671v1"
      ],
      "primary_search_term": "artificial intelligence",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2505.23631v1",
      "title": "Human Empathy as Encoder: AI-Assisted Depression Assessment in Special\n  Education",
      "published": "2025-05-29T16:37:15Z",
      "updated": "2025-05-29T16:37:15Z",
      "summary": "Assessing student depression in sensitive environments like special education\nis challenging. Standardized questionnaires may not fully reflect students'\ntrue situations. Furthermore, automated methods often falter with rich student\nnarratives, lacking the crucial, individualized insights stemming from\nteachers' empathetic connections with students. Existing methods often fail to\naddress this ambiguity or effectively integrate educator understanding. To\naddress these limitations by fostering a synergistic human-AI collaboration,\nthis paper introduces Human Empathy as Encoder (HEAE), a novel, human-centered\nAI framework for transparent and socially responsible depression severity\nassessment. Our approach uniquely integrates student narrative text with a\nteacher-derived, 9-dimensional \"Empathy Vector\" (EV), its dimensions guided by\nthe PHQ-9 framework,to explicitly translate tacit empathetic insight into a\nstructured AI input enhancing rather than replacing human judgment. Rigorous\nexperiments optimized the multimodal fusion, text representation, and\nclassification architecture, achieving 82.74% accuracy for 7-level severity\nclassification. This work demonstrates a path toward more responsible and\nethical affective computing by structurally embedding human empathy",
      "authors": [
        "Boning Zhao"
      ],
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CL"
      ],
      "links": [
        "http://arxiv.org/abs/2505.23631v1",
        "http://arxiv.org/pdf/2505.23631v1"
      ],
      "primary_search_term": "artificial intelligence",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2505.23628v1",
      "title": "AutoSchemaKG: Autonomous Knowledge Graph Construction through Dynamic\n  Schema Induction from Web-Scale Corpora",
      "published": "2025-05-29T16:34:58Z",
      "updated": "2025-05-29T16:34:58Z",
      "summary": "We present AutoSchemaKG, a framework for fully autonomous knowledge graph\nconstruction that eliminates the need for predefined schemas. Our system\nleverages large language models to simultaneously extract knowledge triples and\ninduce comprehensive schemas directly from text, modeling both entities and\nevents while employing conceptualization to organize instances into semantic\ncategories. Processing over 50 million documents, we construct ATLAS (Automated\nTriple Linking And Schema induction), a family of knowledge graphs with 900+\nmillion nodes and 5.9 billion edges. This approach outperforms state-of-the-art\nbaselines on multi-hop QA tasks and enhances LLM factuality. Notably, our\nschema induction achieves 95\\% semantic alignment with human-crafted schemas\nwith zero manual intervention, demonstrating that billion-scale knowledge\ngraphs with dynamically induced schemas can effectively complement parametric\nknowledge in large language models.",
      "authors": [
        "Jiaxin Bai",
        "Wei Fan",
        "Qi Hu",
        "Qing Zong",
        "Chunyang Li",
        "Hong Ting Tsang",
        "Hongyu Luo",
        "Yauwai Yim",
        "Haoyu Huang",
        "Xiao Zhou",
        "Feng Qin",
        "Tianshi Zheng",
        "Xi Peng",
        "Xin Yao",
        "Huiwen Yang",
        "Leijie Wu",
        "Yi Ji",
        "Gong Zhang",
        "Renhai Chen",
        "Yangqiu Song"
      ],
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "links": [
        "http://arxiv.org/abs/2505.23628v1",
        "http://arxiv.org/pdf/2505.23628v1"
      ],
      "primary_search_term": "artificial intelligence",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2505.23559v1",
      "title": "SafeScientist: Toward Risk-Aware Scientific Discoveries by LLM Agents",
      "published": "2025-05-29T15:35:58Z",
      "updated": "2025-05-29T15:35:58Z",
      "summary": "Recent advancements in large language model (LLM) agents have significantly\naccelerated scientific discovery automation, yet concurrently raised critical\nethical and safety concerns. To systematically address these challenges, we\nintroduce \\textbf{SafeScientist}, an innovative AI scientist framework\nexplicitly designed to enhance safety and ethical responsibility in AI-driven\nscientific exploration. SafeScientist proactively refuses ethically\ninappropriate or high-risk tasks and rigorously emphasizes safety throughout\nthe research process. To achieve comprehensive safety oversight, we integrate\nmultiple defensive mechanisms, including prompt monitoring, agent-collaboration\nmonitoring, tool-use monitoring, and an ethical reviewer component.\nComplementing SafeScientist, we propose \\textbf{SciSafetyBench}, a novel\nbenchmark specifically designed to evaluate AI safety in scientific contexts,\ncomprising 240 high-risk scientific tasks across 6 domains, alongside 30\nspecially designed scientific tools and 120 tool-related risk tasks. Extensive\nexperiments demonstrate that SafeScientist significantly improves safety\nperformance by 35\\% compared to traditional AI scientist frameworks, without\ncompromising scientific output quality. Additionally, we rigorously validate\nthe robustness of our safety pipeline against diverse adversarial attack\nmethods, further confirming the effectiveness of our integrated approach. The\ncode and data will be available at https://github.com/ulab-uiuc/SafeScientist.\n\\textcolor{red}{Warning: this paper contains example data that may be offensive\nor harmful.}",
      "authors": [
        "Kunlun Zhu",
        "Jiaxun Zhang",
        "Ziheng Qi",
        "Nuoxing Shang",
        "Zijia Liu",
        "Peixuan Han",
        "Yue Su",
        "Haofei Yu",
        "Jiaxuan You"
      ],
      "categories": [
        "cs.AI"
      ],
      "links": [
        "http://arxiv.org/abs/2505.23559v1",
        "http://arxiv.org/pdf/2505.23559v1"
      ],
      "primary_search_term": "artificial intelligence",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2505.23508v1",
      "title": "A Robot-Assisted Approach to Small Talk Training for Adults with ASD",
      "published": "2025-05-29T14:51:45Z",
      "updated": "2025-05-29T14:51:45Z",
      "summary": "From dating to job interviews, making new friends or simply chatting with the\ncashier at checkout, engaging in small talk is a vital, everyday social skill.\nFor adults with Autism Spectrum Disorder (ASD), small talk can be particularly\nchallenging, yet it is essential for social integration, building\nrelationships, and accessing professional opportunities. In this study, we\npresent our development and evaluation of an in-home autonomous robot system\nthat allows users to practice small talk. Results from the week-long study show\nthat adults with ASD enjoyed the training, made notable progress in initiating\nconversations and improving eye contact, and viewed the system as a valuable\ntool for enhancing their conversational skills.",
      "authors": [
        "Rebecca Ramnauth",
        "Dra\u017een Br\u0161\u010di\u0107",
        "Brian Scassellati"
      ],
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "links": [
        "http://arxiv.org/abs/2505.23508v1",
        "http://arxiv.org/pdf/2505.23508v1"
      ],
      "primary_search_term": "artificial intelligence",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2505.23486v1",
      "title": "Autoformalization in the Era of Large Language Models: A Survey",
      "published": "2025-05-29T14:34:54Z",
      "updated": "2025-05-29T14:34:54Z",
      "summary": "Autoformalization, the process of transforming informal mathematical\npropositions into verifiable formal representations, is a foundational task in\nautomated theorem proving, offering a new perspective on the use of mathematics\nin both theoretical and applied domains. Driven by the rapid progress in\nartificial intelligence, particularly large language models (LLMs), this field\nhas witnessed substantial growth, bringing both new opportunities and unique\nchallenges. In this survey, we provide a comprehensive overview of recent\nadvances in autoformalization from both mathematical and LLM-centric\nperspectives. We examine how autoformalization is applied across various\nmathematical domains and levels of difficulty, and analyze the end-to-end\nworkflow from data preprocessing to model design and evaluation. We further\nexplore the emerging role of autoformalization in enhancing the verifiability\nof LLM-generated outputs, highlighting its potential to improve both the\ntrustworthiness and reasoning capabilities of LLMs. Finally, we summarize key\nopen-source models and datasets supporting current research, and discuss open\nchallenges and promising future directions for the field.",
      "authors": [
        "Ke Weng",
        "Lun Du",
        "Sirui Li",
        "Wangyue Lu",
        "Haozhe Sun",
        "Hengyu Liu",
        "Tiancheng Zhang"
      ],
      "categories": [
        "cs.AI"
      ],
      "links": [
        "http://arxiv.org/abs/2505.23486v1",
        "http://arxiv.org/pdf/2505.23486v1"
      ],
      "primary_search_term": "artificial intelligence",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2505.23437v1",
      "title": "Bounded-Abstention Pairwise Learning to Rank",
      "published": "2025-05-29T13:35:39Z",
      "updated": "2025-05-29T13:35:39Z",
      "summary": "Ranking systems influence decision-making in high-stakes domains like health,\neducation, and employment, where they can have substantial economic and social\nimpacts. This makes the integration of safety mechanisms essential. One such\nmechanism is $\\textit{abstention}$, which enables algorithmic decision-making\nsystem to defer uncertain or low-confidence decisions to human experts. While\nabstention have been predominantly explored in the context of classification\ntasks, its application to other machine learning paradigms remains\nunderexplored. In this paper, we introduce a novel method for abstention in\npairwise learning-to-rank tasks. Our approach is based on thresholding the\nranker's conditional risk: the system abstains from making a decision when the\nestimated risk exceeds a predefined threshold. Our contributions are threefold:\na theoretical characterization of the optimal abstention strategy, a\nmodel-agnostic, plug-in algorithm for constructing abstaining ranking models,\nand a comprehensive empirical evaluations across multiple datasets,\ndemonstrating the effectiveness of our approach.",
      "authors": [
        "Antonio Ferrara",
        "Andrea Pugnana",
        "Francesco Bonchi",
        "Salvatore Ruggieri"
      ],
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.IR"
      ],
      "links": [
        "http://arxiv.org/abs/2505.23437v1",
        "http://arxiv.org/pdf/2505.23437v1"
      ],
      "primary_search_term": "artificial intelligence",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2505.23432v1",
      "title": "A Mathematical Framework for AI-Human Integration in Work",
      "published": "2025-05-29T13:26:21Z",
      "updated": "2025-05-29T13:26:21Z",
      "summary": "The rapid rise of Generative AI (GenAI) tools has sparked debate over their\nrole in complementing or replacing human workers across job contexts. We\npresent a mathematical framework that models jobs, workers, and worker-job fit,\nintroducing a novel decomposition of skills into decision-level and\naction-level subskills to reflect the complementary strengths of humans and\nGenAI. We analyze how changes in subskill abilities affect job success,\nidentifying conditions for sharp transitions in success probability. We also\nestablish sufficient conditions under which combining workers with\ncomplementary subskills significantly outperforms relying on a single worker.\nThis explains phenomena such as productivity compression, where GenAI\nassistance yields larger gains for lower-skilled workers. We demonstrate the\nframework' s practicality using data from O*NET and Big-Bench Lite, aligning\nreal-world data with our model via subskill-division methods. Our results\nhighlight when and how GenAI complements human skills, rather than replacing\nthem.",
      "authors": [
        "Elisa Celis",
        "Lingxiao Huang",
        "Nisheeth K. Vishnoi"
      ],
      "categories": [
        "cs.AI",
        "cs.CY",
        "econ.GN",
        "q-fin.EC"
      ],
      "links": [
        "http://arxiv.org/abs/2505.23432v1",
        "http://arxiv.org/pdf/2505.23432v1"
      ],
      "primary_search_term": "artificial intelligence",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2505.23419v1",
      "title": "SWE-bench Goes Live!",
      "published": "2025-05-29T13:09:44Z",
      "updated": "2025-05-29T13:09:44Z",
      "summary": "The issue-resolving task, where a model generates patches to fix real-world\nbugs, has emerged as a critical benchmark for evaluating the capabilities of\nlarge language models (LLMs). While SWE-bench and its variants have become\nstandard in this domain, they suffer from key limitations: they have not been\nupdated since their initial releases, cover a narrow set of repositories, and\ndepend heavily on manual effort for instance construction and environment\nsetup. These factors hinder scalability and introduce risks of overfitting and\ndata contamination. In this work, we present \\textbf{SWE-bench-Live}, a\n\\textit{live-updatable} benchmark designed to overcome these challenges. Our\ninitial release consists of 1,319 tasks derived from real GitHub issues created\nsince 2024, spanning 93 repositories. Each task is accompanied by a dedicated\nDocker image to ensure reproducible execution. Central to our benchmark is\n\\method, an automated curation pipeline that streamlines the entire process\nfrom instance creation to environment setup, removing manual bottlenecks and\nenabling scalability and continuous updates. We evaluate a range of\nstate-of-the-art agent frameworks and LLMs on SWE-bench-Live, revealing a\nsubstantial performance gap compared to static benchmarks like SWE-bench, even\nunder controlled evaluation conditions. To better understand this discrepancy,\nwe perform detailed analyses across repository origin, issue recency, and task\ndifficulty. By providing a fresh, diverse, and executable benchmark grounded in\nlive repository activity, SWE-bench-Live facilitates rigorous,\ncontamination-resistant evaluation of LLMs and agents in dynamic, real-world\nsoftware development settings.",
      "authors": [
        "Linghao Zhang",
        "Shilin He",
        "Chaoyun Zhang",
        "Yu Kang",
        "Bowen Li",
        "Chengxing Xie",
        "Junhao Wang",
        "Maoquan Wang",
        "Yufan Huang",
        "Shengyu Fu",
        "Elsie Nallipogu",
        "Qingwei Lin",
        "Yingnong Dang",
        "Saravan Rajmohan",
        "Dongmei Zhang"
      ],
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CL"
      ],
      "links": [
        "http://arxiv.org/abs/2505.23419v1",
        "http://arxiv.org/pdf/2505.23419v1"
      ],
      "primary_search_term": "artificial intelligence",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2505.23397v1",
      "title": "A Unified Framework for Human AI Collaboration in Security Operations\n  Centers with Trusted Autonomy",
      "published": "2025-05-29T12:35:08Z",
      "updated": "2025-05-29T12:35:08Z",
      "summary": "This article presents a structured framework for Human-AI collaboration in\nSecurity Operations Centers (SOCs), integrating AI autonomy, trust calibration,\nand Human-in-the-loop decision making. Existing frameworks in SOCs often focus\nnarrowly on automation, lacking systematic structures to manage human\noversight, trust calibration, and scalable autonomy with AI. Many assume static\nor binary autonomy settings, failing to account for the varied complexity,\ncriticality, and risk across SOC tasks considering Humans and AI collaboration.\nTo address these limitations, we propose a novel autonomy tiered framework\ngrounded in five levels of AI autonomy from manual to fully autonomous, mapped\nto Human-in-the-Loop (HITL) roles and task-specific trust thresholds. This\nenables adaptive and explainable AI integration across core SOC functions,\nincluding monitoring, protection, threat detection, alert triage, and incident\nresponse. The proposed framework differentiates itself from previous research\nby creating formal connections between autonomy, trust, and HITL across various\nSOC levels, which allows for adaptive task distribution according to\noperational complexity and associated risks. The framework is exemplified\nthrough a simulated cyber range that features the cybersecurity AI-Avatar, a\nfine-tuned LLM-based SOC assistant. The AI-Avatar case study illustrates\nhuman-AI collaboration for SOC tasks, reducing alert fatigue, enhancing\nresponse coordination, and strategically calibrating trust. This research\nsystematically presents both the theoretical and practical aspects and\nfeasibility of designing next-generation cognitive SOCs that leverage AI not to\nreplace but to enhance human decision-making.",
      "authors": [
        "Ahmad Mohsin",
        "Helge Janicke",
        "Ahmed Ibrahim",
        "Iqbal H. Sarker",
        "Seyit Camtepe"
      ],
      "categories": [
        "cs.AI",
        "cs.CR"
      ],
      "links": [
        "http://arxiv.org/abs/2505.23397v1",
        "http://arxiv.org/pdf/2505.23397v1"
      ],
      "primary_search_term": "artificial intelligence",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2505.23383v1",
      "title": "Automated Modeling Method for Pathloss Model Discovery",
      "published": "2025-05-29T12:04:07Z",
      "updated": "2025-05-29T12:04:07Z",
      "summary": "Modeling propagation is the cornerstone for designing and optimizing\nnext-generation wireless systems, with a particular emphasis on 5G and beyond\nera. Traditional modeling methods have long relied on statistic-based\ntechniques to characterize propagation behavior across different environments.\nWith the expansion of wireless communication systems, there is a growing demand\nfor methods that guarantee the accuracy and interoperability of modeling.\nArtificial intelligence (AI)-based techniques, in particular, are increasingly\nbeing adopted to overcome this challenge, although the interpretability is not\nassured with most of these methods. Inspired by recent advancements in AI, this\npaper proposes a novel approach that accelerates the discovery of path loss\nmodels while maintaining interpretability. The proposed method automates the\nmodel formulation, evaluation, and refinement, facilitating model discovery. We\nevaluate two techniques: one based on Deep Symbolic Regression, offering full\ninterpretability, and the second based on Kolmogorov-Arnold Networks, providing\ntwo levels of interpretability. Both approaches are evaluated on two synthetic\nand two real-world datasets. Our results show that Kolmogorov-Arnold Networks\nachieve R^2 values close to 1 with minimal prediction error, while Deep\nSymbolic Regression generates compact models with moderate accuracy. Moreover,\non the selected examples, we demonstrate that automated methods outperform\ntraditional methods, achieving up to 75% reduction in prediction errors,\noffering accurate and explainable solutions with potential to increase the\nefficiency of discovering next-generation path loss models.",
      "authors": [
        "Ahmad Anaqreh",
        "Shih-Kai Chou",
        "Mihael Mohor\u010di\u010d",
        "Carolina Fortuna"
      ],
      "categories": [
        "cs.LG"
      ],
      "links": [
        "http://arxiv.org/abs/2505.23383v1",
        "http://arxiv.org/pdf/2505.23383v1"
      ],
      "primary_search_term": "artificial intelligence",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2505.23381v1",
      "title": "AutoGPS: Automated Geometry Problem Solving via Multimodal Formalization\n  and Deductive Reasoning",
      "published": "2025-05-29T12:01:20Z",
      "updated": "2025-05-29T12:01:20Z",
      "summary": "Geometry problem solving presents distinctive challenges in artificial\nintelligence, requiring exceptional multimodal comprehension and rigorous\nmathematical reasoning capabilities. Existing approaches typically fall into\ntwo categories: neural-based and symbolic-based methods, both of which exhibit\nlimitations in reliability and interpretability. To address this challenge, we\npropose AutoGPS, a neuro-symbolic collaborative framework that solves geometry\nproblems with concise, reliable, and human-interpretable reasoning processes.\nSpecifically, AutoGPS employs a Multimodal Problem Formalizer (MPF) and a\nDeductive Symbolic Reasoner (DSR). The MPF utilizes neural cross-modal\ncomprehension to translate geometry problems into structured formal language\nrepresentations, with feedback from DSR collaboratively. The DSR takes the\nformalization as input and formulates geometry problem solving as a hypergraph\nexpansion task, executing mathematically rigorous and reliable derivation to\nproduce minimal and human-readable stepwise solutions. Extensive experimental\nevaluations demonstrate that AutoGPS achieves state-of-the-art performance on\nbenchmark datasets. Furthermore, human stepwise-reasoning evaluation confirms\nAutoGPS's impressive reliability and interpretability, with 99\\% stepwise\nlogical coherence. The project homepage is at\nhttps://jayce-ping.github.io/AutoGPS-homepage.",
      "authors": [
        "Bowen Ping",
        "Minnan Luo",
        "Zhuohang Dang",
        "Chenxi Wang",
        "Chengyou Jia"
      ],
      "categories": [
        "cs.AI"
      ],
      "links": [
        "http://arxiv.org/abs/2505.23381v1",
        "http://arxiv.org/pdf/2505.23381v1"
      ],
      "primary_search_term": "artificial intelligence",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2505.23352v1",
      "title": "Understanding the Information Propagation Effects of Communication\n  Topologies in LLM-based Multi-Agent Systems",
      "published": "2025-05-29T11:21:48Z",
      "updated": "2025-05-29T11:21:48Z",
      "summary": "The communication topology in large language model-based multi-agent systems\nfundamentally governs inter-agent collaboration patterns, critically shaping\nboth the efficiency and effectiveness of collective decision-making. While\nrecent studies for communication topology automated design tend to construct\nsparse structures for efficiency, they often overlook why and when sparse and\ndense topologies help or hinder collaboration. In this paper, we present a\ncausal framework to analyze how agent outputs, whether correct or erroneous,\npropagate under topologies with varying sparsity. Our empirical studies reveal\nthat moderately sparse topologies, which effectively suppress error propagation\nwhile preserving beneficial information diffusion, typically achieve optimal\ntask performance. Guided by this insight, we propose a novel topology design\napproach, EIB-leanrner, that balances error suppression and beneficial\ninformation propagation by fusing connectivity patterns from both dense and\nsparse graphs. Extensive experiments show the superior effectiveness,\ncommunication cost, and robustness of EIB-leanrner.",
      "authors": [
        "Xu Shen",
        "Yixin Liu",
        "Yiwei Dai",
        "Yili Wang",
        "Rui Miao",
        "Yue Tan",
        "Shirui Pan",
        "Xin Wang"
      ],
      "categories": [
        "cs.MA",
        "cs.AI"
      ],
      "links": [
        "http://arxiv.org/abs/2505.23352v1",
        "http://arxiv.org/pdf/2505.23352v1"
      ],
      "primary_search_term": "artificial intelligence",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2505.23315v1",
      "title": "Enhancing Marker Scoring Accuracy through Ordinal Confidence Modelling\n  in Educational Assessments",
      "published": "2025-05-29T10:23:20Z",
      "updated": "2025-05-29T10:23:20Z",
      "summary": "A key ethical challenge in Automated Essay Scoring (AES) is ensuring that\nscores are only released when they meet high reliability standards. Confidence\nmodelling addresses this by assigning a reliability estimate measure, in the\nform of a confidence score, to each automated score. In this study, we frame\nconfidence estimation as a classification task: predicting whether an\nAES-generated score correctly places a candidate in the appropriate CEFR level.\nWhile this is a binary decision, we leverage the inherent granularity of the\nscoring domain in two ways. First, we reformulate the task as an n-ary\nclassification problem using score binning. Second, we introduce a set of novel\nKernel Weighted Ordinal Categorical Cross Entropy (KWOCCE) loss functions that\nincorporate the ordinal structure of CEFR labels. Our best-performing model\nachieves an F1 score of 0.97, and enables the system to release 47% of scores\nwith 100% CEFR agreement and 99% with at least 95% CEFR agreement -compared to\napproximately 92% (approx.) CEFR agreement from the standalone AES model where\nwe release all AM predicted scores.",
      "authors": [
        "Abhirup Chakravarty",
        "Mark Brenchley",
        "Trevor Breakspear",
        "Ian Lewin",
        "Yan Huang"
      ],
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "links": [
        "http://arxiv.org/abs/2505.23315v1",
        "http://arxiv.org/pdf/2505.23315v1"
      ],
      "primary_search_term": "artificial intelligence",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2505.23239v1",
      "title": "OSS-UAgent: An Agent-based Usability Evaluation Framework for Open\n  Source Software",
      "published": "2025-05-29T08:40:10Z",
      "updated": "2025-05-29T08:40:10Z",
      "summary": "Usability evaluation is critical to the impact and adoption of open source\nsoftware (OSS), yet traditional methods relying on human evaluators suffer from\nhigh costs and limited scalability. To address these limitations, we introduce\nOSS-UAgent, an automated, configurable, and interactive agent-based usability\nevaluation framework specifically designed for open source software. Our\nframework employs intelligent agents powered by large language models (LLMs) to\nsimulate developers performing programming tasks across various experience\nlevels (from Junior to Expert). By dynamically constructing platform-specific\nknowledge bases, OSS-UAgent ensures accurate and context-aware code generation.\nThe generated code is automatically evaluated across multiple dimensions,\nincluding compliance, correctness, and readability, providing a comprehensive\nmeasure of the software's usability. Additionally, our demonstration showcases\nOSS-UAgent's practical application in evaluating graph analytics platforms,\nhighlighting its effectiveness in automating usability evaluation.",
      "authors": [
        "Lingkai Meng",
        "Yu Shao",
        "Long Yuan",
        "Longbin Lai",
        "Peng Cheng",
        "Wenyuan Yu",
        "Wenjie Zhang",
        "Xuemin Lin",
        "Jingren Zhou"
      ],
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "links": [
        "http://arxiv.org/abs/2505.23239v1",
        "http://arxiv.org/pdf/2505.23239v1"
      ],
      "primary_search_term": "artificial intelligence",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2505.23192v1",
      "title": "Fooling the Watchers: Breaking AIGC Detectors via Semantic Prompt\n  Attacks",
      "published": "2025-05-29T07:31:17Z",
      "updated": "2025-05-29T07:31:17Z",
      "summary": "The rise of text-to-image (T2I) models has enabled the synthesis of\nphotorealistic human portraits, raising serious concerns about identity misuse\nand the robustness of AIGC detectors. In this work, we propose an automated\nadversarial prompt generation framework that leverages a grammar tree structure\nand a variant of the Monte Carlo tree search algorithm to systematically\nexplore the semantic prompt space. Our method generates diverse, controllable\nprompts that consistently evade both open-source and commercial AIGC detectors.\nExtensive experiments across multiple T2I models validate its effectiveness,\nand the approach ranked first in a real-world adversarial AIGC detection\ncompetition. Beyond attack scenarios, our method can also be used to construct\nhigh-quality adversarial datasets, providing valuable resources for training\nand evaluating more robust AIGC detection and defense systems.",
      "authors": [
        "Run Hao",
        "Peng Ying"
      ],
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CR"
      ],
      "links": [
        "http://arxiv.org/abs/2505.23192v1",
        "http://arxiv.org/pdf/2505.23192v1"
      ],
      "primary_search_term": "artificial intelligence",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2505.23191v1",
      "title": "ExpeTrans: LLMs Are Experiential Transfer Learners",
      "published": "2025-05-29T07:30:58Z",
      "updated": "2025-05-29T07:30:58Z",
      "summary": "Recent studies provide large language models (LLMs) with textual task-solving\nexperiences via prompts to improve their performance. However, previous methods\nrely on substantial human labor or time to gather such experiences for each\ntask, which is impractical given the growing variety of task types in user\nqueries to LLMs. To address this issue, we design an autonomous experience\ntransfer framework to explore whether LLMs can mimic human cognitive\nintelligence to autonomously transfer experience from existing source tasks to\nnewly encountered target tasks. This not only allows the acquisition of\nexperience without extensive costs of previous methods, but also offers a novel\npath for the generalization of LLMs. Experimental results on 13 datasets\ndemonstrate that our framework effectively improves the performance of LLMs.\nFurthermore, we provide a detailed analysis of each module in the framework.",
      "authors": [
        "Jinglong Gao",
        "Xiao Ding",
        "Lingxiao Zou",
        "Bibo Cai",
        "Bing Qin",
        "Ting Liu"
      ],
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "links": [
        "http://arxiv.org/abs/2505.23191v1",
        "http://arxiv.org/pdf/2505.23191v1"
      ],
      "primary_search_term": "artificial intelligence",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2505.23130v1",
      "title": "PhotoArtAgent: Intelligent Photo Retouching with Language Model-Based\n  Artist Agents",
      "published": "2025-05-29T06:00:51Z",
      "updated": "2025-05-29T06:00:51Z",
      "summary": "Photo retouching is integral to photographic art, extending far beyond simple\ntechnical fixes to heighten emotional expression and narrative depth. While\nartists leverage expertise to create unique visual effects through deliberate\nadjustments, non-professional users often rely on automated tools that produce\nvisually pleasing results but lack interpretative depth and interactive\ntransparency. In this paper, we introduce PhotoArtAgent, an intelligent system\nthat combines Vision-Language Models (VLMs) with advanced natural language\nreasoning to emulate the creative process of a professional artist. The agent\nperforms explicit artistic analysis, plans retouching strategies, and outputs\nprecise parameters to Lightroom through an API. It then evaluates the resulting\nimages and iteratively refines them until the desired artistic vision is\nachieved. Throughout this process, PhotoArtAgent provides transparent,\ntext-based explanations of its creative rationale, fostering meaningful\ninteraction and user control. Experimental results show that PhotoArtAgent not\nonly surpasses existing automated tools in user studies but also achieves\nresults comparable to those of professional human artists.",
      "authors": [
        "Haoyu Chen",
        "Keda Tao",
        "Yizao Wang",
        "Xinlei Wang",
        "Lei Zhu",
        "Jinjin Gu"
      ],
      "categories": [
        "cs.CV"
      ],
      "links": [
        "http://arxiv.org/abs/2505.23130v1",
        "http://arxiv.org/pdf/2505.23130v1"
      ],
      "primary_search_term": "artificial intelligence",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2505.23107v1",
      "title": "EAD: An EEG Adapter for Automated Classification",
      "published": "2025-05-29T05:21:06Z",
      "updated": "2025-05-29T05:21:06Z",
      "summary": "While electroencephalography (EEG) has been a popular modality for neural\ndecoding, it often involves task specific acquisition of the EEG data. This\nposes challenges for the development of a unified pipeline to learn embeddings\nfor various EEG signal classification, which is often involved in various\ndecoding tasks. Traditionally, EEG classification involves the step of signal\npreprocessing and the use of deep learning techniques, which are highly\ndependent on the number of EEG channels in each sample. However, the same\npipeline cannot be applied even if the EEG data is collected for the same\nexperiment but with different acquisition devices. This necessitates the\ndevelopment of a framework for learning EEG embeddings, which could be highly\nbeneficial for tasks involving multiple EEG samples for the same task but with\nvarying numbers of EEG channels. In this work, we propose EEG Adapter (EAD), a\nflexible framework compatible with any signal acquisition device. More\nspecifically, we leverage a recent EEG foundational model with significant\nadaptations to learn robust representations from the EEG data for the\nclassification task. We evaluate EAD on two publicly available datasets\nachieving state-of-the-art accuracies 99.33% and 92.31% on EEG-ImageNet and\nBrainLat respectively. This illustrates the effectiveness of the proposed\nframework across diverse EEG datasets containing two different perception\ntasks: stimulus and resting-state EEG signals. We also perform zero-shot EEG\nclassification on EEG-ImageNet task to demonstrate the generalization\ncapability of the proposed approach.",
      "authors": [
        "Pushapdeep Singh",
        "Jyoti Nigam",
        "Medicherla Vamsi Krishna",
        "Arnav Bhavsar",
        "Aditya Nigam"
      ],
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "links": [
        "http://arxiv.org/abs/2505.23107v1",
        "http://arxiv.org/pdf/2505.23107v1"
      ],
      "primary_search_term": "artificial intelligence",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2505.22990v1",
      "title": "MenTeR: A fully-automated Multi-agenT workflow for end-to-end RF/Analog\n  Circuits Netlist Design",
      "published": "2025-05-29T01:58:08Z",
      "updated": "2025-05-29T01:58:08Z",
      "summary": "RF/Analog design is essential for bridging digital technologies with\nreal-world signals, ensuring the functionality and reliability of a wide range\nof electronic systems. However, analog design procedures are often intricate,\ntime-consuming and reliant on expert intuition, and hinder the time and cost\nefficiency of circuit development. To overcome the limitations of the manual\ncircuit design, we introduce MenTeR - a multiagent workflow integrated into an\nend-to-end analog design framework. By employing multiple specialized AI agents\nthat collaboratively address different aspects of the design process, such as\nspecification understanding, circuit optimization, and test bench validation,\nMenTeR reduces the dependency on frequent trial-and-error-style intervention.\nMenTeR not only accelerates the design cycle time but also facilitates a\nbroader exploration of the design space, demonstrating robust capabilities in\nhandling real-world analog systems. We believe that MenTeR lays the groundwork\nfor future \"RF/Analog Copilots\" that can collaborate seamlessly with human\ndesigners.",
      "authors": [
        "Pin-Han Chen",
        "Yu-Sheng Lin",
        "Wei-Cheng Lee",
        "Tin-Yu Leu",
        "Po-Hsiang Hsu",
        "Anjana Dissanayake",
        "Sungjin Oh",
        "Chinq-Shiun Chiu"
      ],
      "categories": [
        "cs.AI",
        "cs.ET",
        "cs.LG"
      ],
      "links": [
        "http://arxiv.org/abs/2505.22990v1",
        "http://arxiv.org/pdf/2505.22990v1"
      ],
      "primary_search_term": "artificial intelligence",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2505.22954v1",
      "title": "Darwin Godel Machine: Open-Ended Evolution of Self-Improving Agents",
      "published": "2025-05-29T00:26:15Z",
      "updated": "2025-05-29T00:26:15Z",
      "summary": "Today's AI systems have human-designed, fixed architectures and cannot\nautonomously and continuously improve themselves. The advance of AI could\nitself be automated. If done safely, that would accelerate AI development and\nallow us to reap its benefits much sooner. Meta-learning can automate the\ndiscovery of novel algorithms, but is limited by first-order improvements and\nthe human design of a suitable search space. The G\\\"odel machine proposed a\ntheoretical alternative: a self-improving AI that repeatedly modifies itself in\na provably beneficial manner. Unfortunately, proving that most changes are net\nbeneficial is impossible in practice. We introduce the Darwin G\\\"odel Machine\n(DGM), a self-improving system that iteratively modifies its own code (thereby\nalso improving its ability to modify its own codebase) and empirically\nvalidates each change using coding benchmarks. Inspired by Darwinian evolution\nand open-endedness research, the DGM maintains an archive of generated coding\nagents. It grows the archive by sampling an agent from it and using a\nfoundation model to create a new, interesting, version of the sampled agent.\nThis open-ended exploration forms a growing tree of diverse, high-quality\nagents and allows the parallel exploration of many different paths through the\nsearch space. Empirically, the DGM automatically improves its coding\ncapabilities (e.g., better code editing tools, long-context window management,\npeer-review mechanisms), increasing performance on SWE-bench from 20.0% to\n50.0%, and on Polyglot from 14.2% to 30.7%. Furthermore, the DGM significantly\noutperforms baselines without self-improvement or open-ended exploration. All\nexperiments were done with safety precautions (e.g., sandboxing, human\noversight). The DGM is a significant step toward self-improving AI, capable of\ngathering its own stepping stones along paths that unfold into endless\ninnovation.",
      "authors": [
        "Jenny Zhang",
        "Shengran Hu",
        "Cong Lu",
        "Robert Lange",
        "Jeff Clune"
      ],
      "categories": [
        "cs.AI"
      ],
      "links": [
        "http://arxiv.org/abs/2505.22954v1",
        "http://arxiv.org/pdf/2505.22954v1"
      ],
      "primary_search_term": "artificial intelligence",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2505.22948v1",
      "title": "Foundation Molecular Grammar: Multi-Modal Foundation Models Induce\n  Interpretable Molecular Graph Languages",
      "published": "2025-05-29T00:03:09Z",
      "updated": "2025-05-29T00:03:09Z",
      "summary": "Recent data-efficient molecular generation approaches exploit graph grammars\nto introduce interpretability into the generative models. However, grammar\nlearning therein relies on expert annotation or unreliable heuristics for\nalgorithmic inference. We propose Foundation Molecular Grammar (FMG), which\nleverages multi-modal foundation models (MMFMs) to induce an interpretable\nmolecular language. By exploiting the chemical knowledge of an MMFM, FMG\nrenders molecules as images, describes them as text, and aligns information\nacross modalities using prompt learning. FMG can be used as a drop-in\nreplacement for the prior grammar learning approaches in molecular generation\nand property prediction. We show that FMG not only excels in synthesizability,\ndiversity, and data efficiency but also offers built-in chemical\ninterpretability for automated molecular discovery workflows. Code is available\nat https://github.com/shiningsunnyday/induction.",
      "authors": [
        "Michael Sun",
        "Weize Yuan",
        "Gang Liu",
        "Wojciech Matusik",
        "Jie Chen"
      ],
      "categories": [
        "cs.AI"
      ],
      "links": [
        "http://arxiv.org/abs/2505.22948v1",
        "http://arxiv.org/pdf/2505.22948v1"
      ],
      "primary_search_term": "artificial intelligence",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2505.22942v1",
      "title": "WorkForceAgent-R1: Incentivizing Reasoning Capability in LLM-based Web\n  Agents via Reinforcement Learning",
      "published": "2025-05-28T23:45:28Z",
      "updated": "2025-05-28T23:45:28Z",
      "summary": "Large language models (LLMs)-empowered web agents enables automating complex,\nreal-time web navigation tasks in enterprise environments. However, existing\nweb agents relying on supervised fine-tuning (SFT) often struggle with\ngeneralization and robustness due to insufficient reasoning capabilities when\nhandling the inherently dynamic nature of web interactions. In this study, we\nintroduce WorkForceAgent-R1, an LLM-based web agent trained using a rule-based\nR1-style reinforcement learning framework designed explicitly to enhance\nsingle-step reasoning and planning for business-oriented web navigation tasks.\nWe employ a structured reward function that evaluates both adherence to output\nformats and correctness of actions, enabling WorkForceAgent-R1 to implicitly\nlearn robust intermediate reasoning without explicit annotations or extensive\nexpert demonstrations. Extensive experiments on the WorkArena benchmark\ndemonstrate that WorkForceAgent-R1 substantially outperforms SFT baselines by\n10.26-16.59%, achieving competitive performance relative to proprietary\nLLM-based agents (gpt-4o) in workplace-oriented web navigation tasks.",
      "authors": [
        "Yuchen Zhuang",
        "Di Jin",
        "Jiaao Chen",
        "Wenqi Shi",
        "Hanrui Wang",
        "Chao Zhang"
      ],
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "links": [
        "http://arxiv.org/abs/2505.22942v1",
        "http://arxiv.org/pdf/2505.22942v1"
      ],
      "primary_search_term": "artificial intelligence",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2505.22928v1",
      "title": "Enhancing Study-Level Inference from Clinical Trial Papers via RL-based\n  Numeric Reasoning",
      "published": "2025-05-28T22:59:45Z",
      "updated": "2025-05-28T22:59:45Z",
      "summary": "Systematic reviews in medicine play a critical role in evidence-based\ndecision-making by aggregating findings from multiple studies. A central\nbottleneck in automating this process is extracting numeric evidence and\ndetermining study-level conclusions for specific outcomes and comparisons.\nPrior work has framed this problem as a textual inference task by retrieving\nrelevant content fragments and inferring conclusions from them. However, such\napproaches often rely on shallow textual cues and fail to capture the\nunderlying numeric reasoning behind expert assessments.\n  In this work, we conceptualise the problem as one of quantitative reasoning.\nRather than inferring conclusions from surface text, we extract structured\nnumerical evidence (e.g., event counts or standard deviations) and apply domain\nknowledge informed logic to derive outcome-specific conclusions. We develop a\nnumeric reasoning system composed of a numeric data extraction model and an\neffect estimate component, enabling more accurate and interpretable inference\naligned with the domain expert principles. We train the numeric data extraction\nmodel using different strategies, including supervised fine-tuning (SFT) and\nreinforcement learning (RL) with a new value reward model.\n  When evaluated on the CochraneForest benchmark, our best-performing approach\n-- using RL to train a small-scale number extraction model -- yields up to a\n21% absolute improvement in F1 score over retrieval-based systems and\noutperforms general-purpose LLMs of over 400B parameters by up to 9%. Our\nresults demonstrate the promise of reasoning-driven approaches for automating\nsystematic evidence synthesis.",
      "authors": [
        "Massimiliano Pronesti",
        "Michela Lorandi",
        "Paul Flanagan",
        "Oisin Redmon",
        "Anya Belz",
        "Yufang Hou"
      ],
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "links": [
        "http://arxiv.org/abs/2505.22928v1",
        "http://arxiv.org/pdf/2505.22928v1"
      ],
      "primary_search_term": "artificial intelligence",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2505.22878v1",
      "title": "BugWhisperer: Fine-Tuning LLMs for SoC Hardware Vulnerability Detection",
      "published": "2025-05-28T21:25:06Z",
      "updated": "2025-05-28T21:25:06Z",
      "summary": "The current landscape of system-on-chips (SoCs) security verification faces\nchallenges due to manual, labor-intensive, and inflexible methodologies. These\nissues limit the scalability and effectiveness of security protocols, making\nbug detection at the Register-Transfer Level (RTL) difficult. This paper\nproposes a new framework named BugWhisperer that utilizes a specialized,\nfine-tuned Large Language Model (LLM) to address these challenges. By enhancing\nthe LLM's hardware security knowledge and leveraging its capabilities for text\ninference and knowledge transfer, this approach automates and improves the\nadaptability and reusability of the verification process. We introduce an\nopen-source, fine-tuned LLM specifically designed for detecting security\nvulnerabilities in SoC designs. Our findings demonstrate that this tailored LLM\neffectively enhances the efficiency and flexibility of the security\nverification process. Additionally, we introduce a comprehensive hardware\nvulnerability database that supports this work and will further assist the\nresearch community in enhancing the security verification process.",
      "authors": [
        "Shams Tarek",
        "Dipayan Saha",
        "Sujan Kumar Saha",
        "Farimah Farahmandi"
      ],
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "links": [
        "http://arxiv.org/abs/2505.22878v1",
        "http://arxiv.org/pdf/2505.22878v1"
      ],
      "primary_search_term": "artificial intelligence",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2505.22831v1",
      "title": "Orca: Browsing at Scale Through User-Driven and AI-Facilitated\n  Orchestration Across Malleable Webpages",
      "published": "2025-05-28T20:13:39Z",
      "updated": "2025-05-28T20:13:39Z",
      "summary": "Web-based activities are fundamentally distributed across webpages. However,\nconventional browsers with stacks of tabs fail to support operating and\nsynthesizing large volumes of information across pages. While recent AI systems\nenable fully automated web browsing and information synthesis, they often\ndiminish user agency and hinder contextual understanding. Therefore, we explore\nhow AI could instead augment users' interactions with content across webpages\nand mitigate cognitive and manual efforts. Through literature on information\ntasks and web browsing challenges, and an iterative design process, we present\na rich set of novel interactions with our prototype web browser, Orca.\nLeveraging AI, Orca supports user-driven exploration, operation, organization,\nand synthesis of web content at scale. To enable browsing at scale, webpages\nare treated as malleable materials that humans and AI can collaboratively\nmanipulate and compose into a malleable, dynamic, and browser-level workspace.\nOur evaluation revealed an increased \"appetite\" for information foraging,\nenhanced user control, and more flexibility in sensemaking across a broader\ninformation landscape on the web.",
      "authors": [
        "Peiling Jiang",
        "Haijun Xia"
      ],
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "links": [
        "http://arxiv.org/abs/2505.22831v1",
        "http://arxiv.org/pdf/2505.22831v1"
      ],
      "primary_search_term": "artificial intelligence",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2505.22771v1",
      "title": "Automated Essay Scoring Incorporating Annotations from Automated\n  Feedback Systems",
      "published": "2025-05-28T18:39:56Z",
      "updated": "2025-05-28T18:39:56Z",
      "summary": "This study illustrates how incorporating feedback-oriented annotations into\nthe scoring pipeline can enhance the accuracy of automated essay scoring (AES).\nThis approach is demonstrated with the Persuasive Essays for Rating, Selecting,\nand Understanding Argumentative and Discourse Elements (PERSUADE) corpus. We\nintegrate two types of feedback-driven annotations: those that identify\nspelling and grammatical errors, and those that highlight argumentative\ncomponents. To illustrate how this method could be applied in real-world\nscenarios, we employ two LLMs to generate annotations -- a generative language\nmodel used for spell-correction and an encoder-based token classifier trained\nto identify and mark argumentative elements. By incorporating annotations into\nthe scoring process, we demonstrate improvements in performance using\nencoder-based large language models fine-tuned as classifiers.",
      "authors": [
        "Christopher Ormerod"
      ],
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "links": [
        "http://arxiv.org/abs/2505.22771v1",
        "http://arxiv.org/pdf/2505.22771v1"
      ],
      "primary_search_term": "artificial intelligence",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2505.22704v1",
      "title": "Training Language Models to Generate Quality Code with Program Analysis\n  Feedback",
      "published": "2025-05-28T17:57:47Z",
      "updated": "2025-05-28T17:57:47Z",
      "summary": "Code generation with large language models (LLMs), often termed vibe coding,\nis increasingly adopted in production but fails to ensure code quality,\nparticularly in security (e.g., SQL injection vulnerabilities) and\nmaintainability (e.g., missing type annotations). Existing methods, such as\nsupervised fine-tuning and rule-based post-processing, rely on labor-intensive\nannotations or brittle heuristics, limiting their scalability and\neffectiveness. We propose REAL, a reinforcement learning framework that\nincentivizes LLMs to generate production-quality code using program\nanalysis-guided feedback. Specifically, REAL integrates two automated signals:\n(1) program analysis detecting security or maintainability defects and (2) unit\ntests ensuring functional correctness. Unlike prior work, our framework is\nprompt-agnostic and reference-free, enabling scalable supervision without\nmanual intervention. Experiments across multiple datasets and model scales\ndemonstrate that REAL outperforms state-of-the-art methods in simultaneous\nassessments of functionality and code quality. Our work bridges the gap between\nrapid prototyping and production-ready code, enabling LLMs to deliver both\nspeed and quality.",
      "authors": [
        "Feng Yao",
        "Zilong Wang",
        "Liyuan Liu",
        "Junxia Cui",
        "Li Zhong",
        "Xiaohan Fu",
        "Haohui Mai",
        "Vish Krishnan",
        "Jianfeng Gao",
        "Jingbo Shang"
      ],
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "links": [
        "http://arxiv.org/abs/2505.22704v1",
        "http://arxiv.org/pdf/2505.22704v1"
      ],
      "primary_search_term": "artificial intelligence",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2505.22639v1",
      "title": "Navigating the AI-Energy Nexus with Geopolitical Insight",
      "published": "2025-05-28T17:54:30Z",
      "updated": "2025-05-28T17:54:30Z",
      "summary": "This working paper examines how geopolitical strategies and energy resource\nmanagement intersect with Artificial Intelligence (AI) development, delineating\nthe AI-energy nexus as critical to sustaining U.S. AI leadership. By analyzing\nthe centralized approaches of authoritarian regimes like China and Gulf\nnations, alongside market-driven approaches in the U.S., the paper explores\ndivergent strategies to allocate resources for AI energy needs. It underscores\nthe role of energy infrastructure, market dynamics, and state-led initiatives\nin shaping global AI competition. Recommendations include adopting\ngeopolitically informed analyses and leveraging both market and non-market\nstrengths to enhance U.S. competitiveness. This research aims to inform\npolicymakers, technologists, and researchers about the strategic implications\nof the AI-energy nexus and offers insights into advancing U.S. global\nleadership in AI amidst evolving technological paradigms.",
      "authors": [
        "Nidhi Kalra",
        "Robin Wang",
        "Ismael Arciniegas Rueda"
      ],
      "categories": [
        "cs.CY"
      ],
      "links": [
        "http://arxiv.org/abs/2505.22639v1",
        "http://arxiv.org/pdf/2505.22639v1"
      ],
      "primary_search_term": "artificial intelligence",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2505.22634v1",
      "title": "LabUtopia: High-Fidelity Simulation and Hierarchical Benchmark for\n  Scientific Embodied Agents",
      "published": "2025-05-28T17:50:53Z",
      "updated": "2025-05-28T17:50:53Z",
      "summary": "Scientific embodied agents play a crucial role in modern laboratories by\nautomating complex experimental workflows. Compared to typical household\nenvironments, laboratory settings impose significantly higher demands on\nperception of physical-chemical transformations and long-horizon planning,\nmaking them an ideal testbed for advancing embodied intelligence. However, its\ndevelopment has been long hampered by the lack of suitable simulator and\nbenchmarks. In this paper, we address this gap by introducing LabUtopia, a\ncomprehensive simulation and benchmarking suite designed to facilitate the\ndevelopment of generalizable, reasoning-capable embodied agents in laboratory\nsettings. Specifically, it integrates i) LabSim, a high-fidelity simulator\nsupporting multi-physics and chemically meaningful interactions; ii) LabScene,\na scalable procedural generator for diverse scientific scenes; and iii)\nLabBench, a hierarchical benchmark spanning five levels of complexity from\natomic actions to long-horizon mobile manipulation. LabUtopia supports 30\ndistinct tasks and includes more than 200 scene and instrument assets, enabling\nlarge-scale training and principled evaluation in high-complexity environments.\nWe demonstrate that LabUtopia offers a powerful platform for advancing the\nintegration of perception, planning, and control in scientific-purpose agents\nand provides a rigorous testbed for exploring the practical capabilities and\ngeneralization limits of embodied intelligence in future research.",
      "authors": [
        "Rui Li",
        "Zixuan Hu",
        "Wenxi Qu",
        "Jinouwen Zhang",
        "Zhenfei Yin",
        "Sha Zhang",
        "Xuantuo Huang",
        "Hanqing Wang",
        "Tai Wang",
        "Jiangmiao Pang",
        "Wanli Ouyang",
        "Lei Bai",
        "Wangmeng Zuo",
        "Ling-Yu Duan",
        "Dongzhan Zhou",
        "Shixiang Tang"
      ],
      "categories": [
        "cs.RO",
        "cs.SE"
      ],
      "links": [
        "http://arxiv.org/abs/2505.22634v1",
        "http://arxiv.org/pdf/2505.22634v1"
      ],
      "primary_search_term": "artificial intelligence",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2505.22451v1",
      "title": "AI Mathematician: Towards Fully Automated Frontier Mathematical Research",
      "published": "2025-05-28T15:10:37Z",
      "updated": "2025-05-28T15:10:37Z",
      "summary": "Large Reasoning Models (LRMs) have made significant progress in mathematical\ncapabilities in recent times. However, these successes have been primarily\nconfined to competition-level problems. In this work, we propose AI\nMathematician (AIM) framework, which harnesses the reasoning strength of LRMs\nto support frontier mathematical research. We have identified two critical\nchallenges of mathematical research compared to competition, {\\it the intrinsic\ncomplexity of research problems} and {\\it the requirement of procedural rigor}.\nTo address these challenges, AIM incorporates two core strategies: an\nexploration mechanism to foster longer solution paths, and the pessimistic\nreasonable verification method to ensure reliability.\n  This early version of AIM already exhibits strong capability in tackling\nresearch-level tasks. We conducted extensive experiments across several\nreal-world mathematical topics and obtained promising results. AIM is able to\nautonomously construct substantial portions of proofs and uncover non-trivial\ninsights within each research area. These findings highlight the potential of\nLRMs in mathematical discovery and suggest that LRM-based agent systems could\nsignificantly accelerate mathematical research in the future.",
      "authors": [
        "Yuanhang Liu",
        "Yanxing Huang",
        "Yanqiao Wang",
        "Peng Li",
        "Yang Liu"
      ],
      "categories": [
        "cs.AI"
      ],
      "links": [
        "http://arxiv.org/abs/2505.22451v1",
        "http://arxiv.org/pdf/2505.22451v1"
      ],
      "primary_search_term": "artificial intelligence",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2505.22353v1",
      "title": "VME: A Satellite Imagery Dataset and Benchmark for Detecting Vehicles in\n  the Middle East and Beyond",
      "published": "2025-05-28T13:34:05Z",
      "updated": "2025-05-28T13:34:05Z",
      "summary": "Detecting vehicles in satellite images is crucial for traffic management,\nurban planning, and disaster response. However, current models struggle with\nreal-world diversity, particularly across different regions. This challenge is\namplified by geographic bias in existing datasets, which often focus on\nspecific areas and overlook regions like the Middle East. To address this gap,\nwe present the Vehicles in the Middle East (VME) dataset, designed explicitly\nfor vehicle detection in high-resolution satellite images from Middle Eastern\ncountries. Sourced from Maxar, the VME dataset spans 54 cities across 12\ncountries, comprising over 4,000 image tiles and more than 100,000 vehicles,\nannotated using both manual and semi-automated methods. Additionally, we\nintroduce the largest benchmark dataset for Car Detection in Satellite Imagery\n(CDSI), combining images from multiple sources to enhance global car detection.\nOur experiments demonstrate that models trained on existing datasets perform\npoorly on Middle Eastern images, while the VME dataset significantly improves\ndetection accuracy in this region. Moreover, state-of-the-art models trained on\nCDSI achieve substantial improvements in global car detection.",
      "authors": [
        "Noora Al-Emadi",
        "Ingmar Weber",
        "Yin Yang",
        "Ferda Ofli"
      ],
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "links": [
        "http://dx.doi.org/10.1038/s41597-025-04567-y",
        "http://arxiv.org/abs/2505.22353v1",
        "http://arxiv.org/pdf/2505.22353v1"
      ],
      "primary_search_term": "artificial intelligence",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2505.22349v1",
      "title": "ChatPD: An LLM-driven Paper-Dataset Networking System",
      "published": "2025-05-28T13:31:08Z",
      "updated": "2025-05-28T13:31:08Z",
      "summary": "Scientific research heavily depends on suitable datasets for method\nvalidation, but existing academic platforms with dataset management like\nPapersWithCode suffer from inefficiencies in their manual workflow. To overcome\nthis bottleneck, we present a system, called ChatPD, that utilizes Large\nLanguage Models (LLMs) to automate dataset information extraction from academic\npapers and construct a structured paper-dataset network. Our system consists of\nthree key modules: \\textit{paper collection}, \\textit{dataset information\nextraction}, and \\textit{dataset entity resolution} to construct paper-dataset\nnetworks. Specifically, we propose a \\textit{Graph Completion and Inference}\nstrategy to map dataset descriptions to their corresponding entities. Through\nextensive experiments, we demonstrate that ChatPD not only outperforms the\nexisting platform PapersWithCode in dataset usage extraction but also achieves\nabout 90\\% precision and recall in entity resolution tasks. Moreover, we have\ndeployed ChatPD to continuously extract which datasets are used in papers, and\nprovide a dataset discovery service, such as task-specific dataset queries and\nsimilar dataset recommendations. We open source ChatPD and the current\npaper-dataset network on this [GitHub\nrepository]{https://github.com/ChatPD-web/ChatPD}.",
      "authors": [
        "Anjie Xu",
        "Ruiqing Ding",
        "Leye Wang"
      ],
      "categories": [
        "cs.DB",
        "cs.AI",
        "cs.IR"
      ],
      "links": [
        "http://dx.doi.org/10.1145/3711896.3737202",
        "http://arxiv.org/abs/2505.22349v1",
        "http://arxiv.org/pdf/2505.22349v1"
      ],
      "primary_search_term": "artificial intelligence",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2505.22250v2",
      "title": "YH-MINER: Multimodal Intelligent System for Natural Ecological Reef\n  Metric Extraction",
      "published": "2025-05-28T11:36:18Z",
      "updated": "2025-05-29T04:26:18Z",
      "summary": "Coral reefs, crucial for sustaining marine biodiversity and ecological\nprocesses (e.g., nutrient cycling, habitat provision), face escalating threats,\nunderscoring the need for efficient monitoring. Coral reef ecological\nmonitoring faces dual challenges of low efficiency in manual analysis and\ninsufficient segmentation accuracy in complex underwater scenarios. This study\ndevelops the YH-MINER system, establishing an intelligent framework centered on\nthe Multimodal Large Model (MLLM) for \"object detection-semantic\nsegmentation-prior input\". The system uses the object detection module\n(mAP@0.5=0.78) to generate spatial prior boxes for coral instances, driving the\nsegment module to complete pixel-level segmentation in low-light and densely\noccluded scenarios. The segmentation masks and finetuned classification\ninstructions are fed into the Qwen2-VL-based multimodal model as prior inputs,\nachieving a genus-level classification accuracy of 88% and simultaneously\nextracting core ecological metrics. Meanwhile, the system retains the\nscalability of the multimodal model through standardized interfaces, laying a\nfoundation for future integration into multimodal agent-based underwater robots\nand supporting the full-process automation of \"image acquisition-prior\ngeneration-real-time analysis\".",
      "authors": [
        "Mingzhuang Wang",
        "Yvyang Li",
        "Xiyang Zhang",
        "Fei Tan",
        "Qi Shi",
        "Guotao Zhang",
        "Siqi Chen",
        "Yufei Liu",
        "Lei Lei",
        "Ming Zhou",
        "Qiang Lin",
        "Hongqiang Yang"
      ],
      "categories": [
        "cs.CV",
        "q-bio.QM"
      ],
      "links": [
        "http://arxiv.org/abs/2505.22250v2",
        "http://arxiv.org/pdf/2505.22250v2"
      ],
      "primary_search_term": "artificial intelligence",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2505.22148v1",
      "title": "What Makes a Good Reasoning Chain? Uncovering Structural Patterns in\n  Long Chain-of-Thought Reasoning",
      "published": "2025-05-28T09:12:31Z",
      "updated": "2025-05-28T09:12:31Z",
      "summary": "Recent advances in reasoning with large language models (LLMs) have\npopularized Long Chain-of-Thought (LCoT), a strategy that encourages deliberate\nand step-by-step reasoning before producing a final answer. While LCoTs have\nenabled expert-level performance in complex tasks, how the internal structures\nof their reasoning chains drive, or even predict, the correctness of final\nanswers remains a critical yet underexplored question. In this work, we present\nLCoT2Tree, an automated framework that converts sequential LCoTs into\nhierarchical tree structures and thus enables deeper structural analysis of LLM\nreasoning. Using graph neural networks (GNNs), we reveal that structural\npatterns extracted by LCoT2Tree, including exploration, backtracking, and\nverification, serve as stronger predictors of final performance across a wide\nrange of tasks and models. Leveraging an explainability technique, we further\nidentify critical thought patterns such as over-branching that account for\nfailures. Beyond diagnostic insights, the structural patterns by LCoT2Tree\nsupport practical applications, including improving Best-of-N decoding\neffectiveness. Overall, our results underscore the critical role of internal\nstructures of reasoning chains, positioning LCoT2Tree as a powerful tool for\ndiagnosing, interpreting, and improving reasoning in LLMs.",
      "authors": [
        "Gangwei Jiang",
        "Yahui Liu",
        "Zhaoyi Li",
        "Qi Wang",
        "Fuzheng Zhang",
        "Linqi Song",
        "Ying Wei",
        "Defu Lian"
      ],
      "categories": [
        "cs.AI"
      ],
      "links": [
        "http://arxiv.org/abs/2505.22148v1",
        "http://arxiv.org/pdf/2505.22148v1"
      ],
      "primary_search_term": "artificial intelligence",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2505.22137v1",
      "title": "Limited Generalizability in Argument Mining: State-Of-The-Art Models\n  Learn Datasets, Not Arguments",
      "published": "2025-05-28T09:00:56Z",
      "updated": "2025-05-28T09:00:56Z",
      "summary": "Identifying arguments is a necessary prerequisite for various tasks in\nautomated discourse analysis, particularly within contexts such as political\ndebates, online discussions, and scientific reasoning. In addition to\ntheoretical advances in understanding the constitution of arguments, a\nsignificant body of research has emerged around practical argument mining,\nsupported by a growing number of publicly available datasets. On these\nbenchmarks, BERT-like transformers have consistently performed best,\nreinforcing the belief that such models are broadly applicable across diverse\ncontexts of debate. This study offers the first large-scale re-evaluation of\nsuch state-of-the-art models, with a specific focus on their ability to\ngeneralize in identifying arguments. We evaluate four transformers, three\nstandard and one enhanced with contrastive pre-training for better\ngeneralization, on 17 English sentence-level datasets as most relevant to the\ntask. Our findings show that, to varying degrees, these models tend to rely on\nlexical shortcuts tied to content words, suggesting that apparent progress may\noften be driven by dataset-specific cues rather than true task alignment. While\nthe models achieve strong results on familiar benchmarks, their performance\ndrops markedly when applied to unseen datasets. Nonetheless, incorporating both\ntask-specific pre-training and joint benchmark training proves effective in\nenhancing both robustness and generalization.",
      "authors": [
        "Marc Feger",
        "Katarina Boland",
        "Stefan Dietze"
      ],
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "links": [
        "http://arxiv.org/abs/2505.22137v1",
        "http://arxiv.org/pdf/2505.22137v1"
      ],
      "primary_search_term": "artificial intelligence",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2505.22126v1",
      "title": "SridBench: Benchmark of Scientific Research Illustration Drawing of\n  Image Generation Model",
      "published": "2025-05-28T08:51:01Z",
      "updated": "2025-05-28T08:51:01Z",
      "summary": "Recent years have seen rapid advances in AI-driven image generation. Early\ndiffusion models emphasized perceptual quality, while newer multimodal models\nlike GPT-4o-image integrate high-level reasoning, improving semantic\nunderstanding and structural composition. Scientific illustration generation\nexemplifies this evolution: unlike general image synthesis, it demands accurate\ninterpretation of technical content and transformation of abstract ideas into\nclear, standardized visuals. This task is significantly more\nknowledge-intensive and laborious, often requiring hours of manual work and\nspecialized tools. Automating it in a controllable, intelligent manner would\nprovide substantial practical value. Yet, no benchmark currently exists to\nevaluate AI on this front. To fill this gap, we introduce SridBench, the first\nbenchmark for scientific figure generation. It comprises 1,120 instances\ncurated from leading scientific papers across 13 natural and computer science\ndisciplines, collected via human experts and MLLMs. Each sample is evaluated\nalong six dimensions, including semantic fidelity and structural accuracy.\nExperimental results reveal that even top-tier models like GPT-4o-image lag\nbehind human performance, with common issues in text/visual clarity and\nscientific correctness. These findings highlight the need for more advanced\nreasoning-driven visual generation capabilities.",
      "authors": [
        "Yifan Chang",
        "Yukang Feng",
        "Jianwen Sun",
        "Jiaxin Ai",
        "Chuanhao Li",
        "S. Kevin Zhou",
        "Kaipeng Zhang"
      ],
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "links": [
        "http://arxiv.org/abs/2505.22126v1",
        "http://arxiv.org/pdf/2505.22126v1"
      ],
      "primary_search_term": "artificial intelligence",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2505.21988v1",
      "title": "Functional Matching of Logic Subgraphs: Beyond Structural Isomorphism",
      "published": "2025-05-28T05:31:49Z",
      "updated": "2025-05-28T05:31:49Z",
      "summary": "Subgraph matching in logic circuits is foundational for numerous Electronic\nDesign Automation (EDA) applications, including datapath optimization,\narithmetic verification, and hardware trojan detection. However, existing\ntechniques rely primarily on structural graph isomorphism and thus fail to\nidentify function-related subgraphs when synthesis transformations\nsubstantially alter circuit topology. To overcome this critical limitation, we\nintroduce the concept of functional subgraph matching, a novel approach that\nidentifies whether a given logic function is implicitly present within a larger\ncircuit, irrespective of structural variations induced by synthesis or\ntechnology mapping. Specifically, we propose a two-stage multi-modal framework:\n(1) learning robust functional embeddings across AIG and post-mapping netlists\nfor functional subgraph detection, and (2) identifying fuzzy boundaries using a\ngraph segmentation approach. Evaluations on standard benchmarks (ITC99,\nOpenABCD, ForgeEDA) demonstrate significant performance improvements over\nexisting structural methods, with average $93.8\\%$ accuracy in functional\nsubgraph detection and a dice score of $91.3\\%$ in fuzzy boundary\nidentification.",
      "authors": [
        "Ziyang Zheng",
        "Kezhi Li",
        "Zhengyuan Shi",
        "Qiang Xu"
      ],
      "categories": [
        "cs.AI"
      ],
      "links": [
        "http://arxiv.org/abs/2505.21988v1",
        "http://arxiv.org/pdf/2505.21988v1"
      ],
      "primary_search_term": "artificial intelligence",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2505.21972v1",
      "title": "Judging LLMs on a Simplex",
      "published": "2025-05-28T04:50:41Z",
      "updated": "2025-05-28T04:50:41Z",
      "summary": "Automated evaluation of free-form outputs from large language models (LLMs)\nis challenging because many distinct answers can be equally valid. A common\npractice is to use LLMs themselves as judges, but the theoretical properties of\nthis approach are not yet well understood. We show that a geometric framework\nthat represents both judges and candidates as points on a probability simplex\ncan provide helpful insight on what is or is not identifiable using LLM judges.\nOur theoretical analysis uncovers a \"phase transition\" in ranking\nidentifiability: for binary scoring systems, true rankings are identifiable\neven with weak judges under mild assumptions, while rankings become\nnon-identifiable for three or more scoring levels even with infinite data,\nabsent additional prior knowledge. This non-identifiability highlights how\nuncertainty in rankings stems from not only aleatoric uncertainty (i.e.,\ninherent stochasticity in the data) but also epistemic uncertainty regarding\nwhich assumptions hold, an aspect that has received limited attention until\nnow. To integrate both types of uncertainty, we use Bayesian inference to\nencode assumptions as priors and conduct sensitivity analysis of ranking\nestimates and credible intervals. Empirical evaluations across multiple\nbenchmarks demonstrate that Bayesian inference yields more accurate rankings\nand substantially improves coverage rates. These results underscore the\nimportance of taking a more holistic approach to uncertainty quantification\nwhen using LLMs as judges.",
      "authors": [
        "Patrick Vossler",
        "Fan Xia",
        "Yifan Mai",
        "Jean Feng"
      ],
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "links": [
        "http://arxiv.org/abs/2505.21972v1",
        "http://arxiv.org/pdf/2505.21972v1"
      ],
      "primary_search_term": "artificial intelligence",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2505.21963v1",
      "title": "LaMDAgent: An Autonomous Framework for Post-Training Pipeline\n  Optimization via LLM Agents",
      "published": "2025-05-28T04:30:51Z",
      "updated": "2025-05-28T04:30:51Z",
      "summary": "Large Language Models (LLMs) have demonstrated exceptional performance across\na wide range of tasks. To further tailor LLMs to specific domains or\napplications, post-training techniques such as Supervised Fine-Tuning (SFT),\nPreference Learning, and model merging are commonly employed. While each of\nthese methods has been extensively studied in isolation, the automated\nconstruction of complete post-training pipelines remains an underexplored area.\nExisting approaches typically rely on manual design or focus narrowly on\noptimizing individual components, such as data ordering or merging strategies.\nIn this work, we introduce LaMDAgent (short for Language Model Developing\nAgent), a novel framework that autonomously constructs and optimizes full\npost-training pipelines through the use of LLM-based agents. LaMDAgent\nsystematically explores diverse model generation techniques, datasets, and\nhyperparameter configurations, leveraging task-based feedback to discover\nhigh-performing pipelines with minimal human intervention. Our experiments show\nthat LaMDAgent improves tool-use accuracy by 9.0 points while preserving\ninstruction-following capabilities. Moreover, it uncovers effective\npost-training strategies that are often overlooked by conventional human-driven\nexploration. We further analyze the impact of data and model size scaling to\nreduce computational costs on the exploration, finding that model size scalings\nintroduces new challenges, whereas scaling data size enables cost-effective\npipeline discovery.",
      "authors": [
        "Taro Yano",
        "Yoichi Ishibashi",
        "Masafumi Oyamada"
      ],
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "links": [
        "http://arxiv.org/abs/2505.21963v1",
        "http://arxiv.org/pdf/2505.21963v1"
      ],
      "primary_search_term": "artificial intelligence",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2505.21923v1",
      "title": "FALCON: An ML Framework for Fully Automated Layout-Constrained Analog\n  Circuit Design",
      "published": "2025-05-28T03:16:08Z",
      "updated": "2025-05-28T03:16:08Z",
      "summary": "Designing analog circuits from performance specifications is a complex,\nmulti-stage process encompassing topology selection, parameter inference, and\nlayout feasibility. We introduce FALCON, a unified machine learning framework\nthat enables fully automated, specification-driven analog circuit synthesis\nthrough topology selection and layout-constrained optimization. Given a target\nperformance, FALCON first selects an appropriate circuit topology using a\nperformance-driven classifier guided by human design heuristics. Next, it\nemploys a custom, edge-centric graph neural network trained to map circuit\ntopology and parameters to performance, enabling gradient-based parameter\ninference through the learned forward model. This inference is guided by a\ndifferentiable layout cost, derived from analytical equations capturing\nparasitic and frequency-dependent effects, and constrained by design rules. We\ntrain and evaluate FALCON on a large-scale custom dataset of 1M analog mm-wave\ncircuits, generated and simulated using Cadence Spectre across 20\nexpert-designed topologies. Through this evaluation, FALCON demonstrates >99\\%\naccuracy in topology inference, <10\\% relative error in performance prediction,\nand efficient layout-aware design that completes in under 1 second per\ninstance. Together, these results position FALCON as a practical and extensible\nfoundation model for end-to-end analog circuit design automation.",
      "authors": [
        "Asal Mehradfar",
        "Xuzhe Zhao",
        "Yilun Huang",
        "Emir Ceyani",
        "Yankai Yang",
        "Shihao Han",
        "Hamidreza Aghasi",
        "Salman Avestimehr"
      ],
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.AR",
        "cs.CE"
      ],
      "links": [
        "http://arxiv.org/abs/2505.21923v1",
        "http://arxiv.org/pdf/2505.21923v1"
      ],
      "primary_search_term": "artificial intelligence",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2505.21908v1",
      "title": "Reinforcement Learning for Out-of-Distribution Reasoning in LLMs: An\n  Empirical Study on Diagnosis-Related Group Coding",
      "published": "2025-05-28T02:54:07Z",
      "updated": "2025-05-28T02:54:07Z",
      "summary": "Diagnosis-Related Group (DRG) codes are essential for hospital reimbursement\nand operations but require labor-intensive assignment. Large Language Models\n(LLMs) struggle with DRG coding due to the out-of-distribution (OOD) nature of\nthe task: pretraining corpora rarely contain private clinical or billing data.\nWe introduce DRG-Sapphire, which uses large-scale reinforcement learning (RL)\nfor automated DRG coding from clinical notes. Built on Qwen2.5-7B and trained\nwith Group Relative Policy Optimization (GRPO) using rule-based rewards,\nDRG-Sapphire introduces a series of RL enhancements to address domain-specific\nchallenges not seen in previous mathematical tasks. Our model achieves\nstate-of-the-art accuracy on the MIMIC-IV benchmark and generates\nphysician-validated reasoning for DRG assignments, significantly enhancing\nexplainability. Our study further sheds light on broader challenges of applying\nRL to knowledge-intensive, OOD tasks. We observe that RL performance scales\napproximately linearly with the logarithm of the number of supervised\nfine-tuning (SFT) examples, suggesting that RL effectiveness is fundamentally\nconstrained by the domain knowledge encoded in the base model. For OOD tasks\nlike DRG coding, strong RL performance requires sufficient knowledge infusion\nprior to RL. Consequently, scaling SFT may be more effective and\ncomputationally efficient than scaling RL alone for such tasks.",
      "authors": [
        "Hanyin Wang",
        "Zhenbang Wu",
        "Gururaj Kolar",
        "Hariprasad Korsapati",
        "Brian Bartlett",
        "Bryan Hull",
        "Jimeng Sun"
      ],
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "links": [
        "http://arxiv.org/abs/2505.21908v1",
        "http://arxiv.org/pdf/2505.21908v1"
      ],
      "primary_search_term": "artificial intelligence",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2505.21873v1",
      "title": "HelixDesign-Binder: A Scalable Production-Grade Platform for Binder\n  Design Built on HelixFold3",
      "published": "2025-05-28T01:39:31Z",
      "updated": "2025-05-28T01:39:31Z",
      "summary": "Protein binder design is central to therapeutics, diagnostics, and synthetic\nbiology, yet practical deployment remains challenging due to fragmented\nworkflows, high computational costs, and complex tool integration. We present\nHelixDesign-Binder, a production-grade, high-throughput platform built on\nHelixFold3 that automates the full binder design pipeline, from backbone\ngeneration and sequence design to structural evaluation and multi-dimensional\nscoring. By unifying these stages into a scalable and user-friendly system,\nHelixDesign-Binder enables efficient exploration of binder candidates with\nfavorable structural, energetic, and physicochemical properties. The platform\nleverages Baidu Cloud's high-performance infrastructure to support large-scale\ndesign and incorporates advanced scoring metrics, including ipTM, predicted\nbinding free energy, and interface hydrophobicity. Benchmarking across six\nprotein targets demonstrates that HelixDesign-Binder reliably produces diverse\nand high-quality binders, some of which match or exceed validated designs in\npredicted binding affinity. HelixDesign-Binder is accessible via an interactive\nweb interface in PaddleHelix platform, supporting both academic research and\nindustrial applications in antibody and protein binder development.",
      "authors": [
        "Jie Gao",
        "Jun Li",
        "Jing Hu",
        "Shanzhuo Zhang",
        "Kunrui Zhu",
        "Yueyang Huang",
        "Xiaonan Zhang",
        "Xiaomin Fang"
      ],
      "categories": [
        "q-bio.BM",
        "cs.AI",
        "q-bio.QM"
      ],
      "links": [
        "http://arxiv.org/abs/2505.21873v1",
        "http://arxiv.org/pdf/2505.21873v1"
      ],
      "primary_search_term": "artificial intelligence",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2505.21811v1",
      "title": "Revisiting Self-attention for Cross-domain Sequential Recommendation",
      "published": "2025-05-27T22:38:32Z",
      "updated": "2025-05-27T22:38:32Z",
      "summary": "Sequential recommendation is a popular paradigm in modern recommender\nsystems. In particular, one challenging problem in this space is cross-domain\nsequential recommendation (CDSR), which aims to predict future behaviors given\nuser interactions across multiple domains. Existing CDSR frameworks are mostly\nbuilt on the self-attention transformer and seek to improve by explicitly\ninjecting additional domain-specific components (e.g. domain-aware module\nblocks). While these additional components help, we argue they overlook the\ncore self-attention module already present in the transformer, a naturally\npowerful tool to learn correlations among behaviors. In this work, we aim to\nimprove the CDSR performance for simple models from a novel perspective of\nenhancing the self-attention. Specifically, we introduce a Pareto-optimal\nself-attention and formulate the cross-domain learning as a multi-objective\nproblem, where we optimize the recommendation task while dynamically minimizing\nthe cross-domain attention scores. Our approach automates knowledge transfer in\nCDSR (dubbed as AutoCDSR) -- it not only mitigates negative transfer but also\nencourages complementary knowledge exchange among auxiliary domains. Based on\nthe idea, we further introduce AutoCDSR+, a more performant variant with slight\nadditional cost. Our proposal is easy to implement and works as a plug-and-play\nmodule that can be incorporated into existing transformer-based recommenders.\nBesides flexibility, it is practical to deploy because it brings little extra\ncomputational overheads without heavy hyper-parameter tuning. AutoCDSR on\naverage improves Recall@10 for SASRec and Bert4Rec by 9.8% and 16.0% and\nNDCG@10 by 12.0% and 16.7%, respectively. Code is available at\nhttps://github.com/snap-research/AutoCDSR.",
      "authors": [
        "Clark Mingxuan Ju",
        "Leonardo Neves",
        "Bhuvesh Kumar",
        "Liam Collins",
        "Tong Zhao",
        "Yuwei Qiu",
        "Qing Dou",
        "Sohail Nizam",
        "Sen Yang",
        "Neil Shah"
      ],
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "links": [
        "http://arxiv.org/abs/2505.21811v1",
        "http://arxiv.org/pdf/2505.21811v1"
      ],
      "primary_search_term": "artificial intelligence",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2505.21744v1",
      "title": "Computocene: Notes from an Age of Observation",
      "published": "2025-05-27T20:33:37Z",
      "updated": "2025-05-27T20:33:37Z",
      "summary": "This piece plays with the idea of the Computocene: an era defined not merely\nby the ubiquity of computers, but by their deepening role in how we observe,\ninterpret, and make sense of the world. Rather than emphasizing automation,\nspeed, scale, or intelligence, computation is reframed as a mode of attention:\nfiltering information, guiding inquiry, reframing questions, and shaping the\nvery conditions under which knowledge emerges. I invite the reader to consider\ncomputers not simply as tools of calculation, but as epistemic instruments that\nparticipate in the formation of knowledge. This perspective reconfigures not\nonly scientific practice but the epistemological foundations of understanding\nitself. The Computocene thus names a shift: from computation as calculation to\ncomputation as a form of attunement to the world. It is a speculative essay,\noffered without technical formality, and intended for a general, curious\nreadership.",
      "authors": [
        "Simone Severini"
      ],
      "categories": [
        "cs.CY"
      ],
      "links": [
        "http://arxiv.org/abs/2505.21744v1",
        "http://arxiv.org/pdf/2505.21744v1"
      ],
      "primary_search_term": "artificial intelligence",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    }
  ]
}