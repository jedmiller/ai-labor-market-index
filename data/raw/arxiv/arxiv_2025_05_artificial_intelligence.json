{
  "query": "all:artificial intelligence AND (labor market OR employment OR jobs OR workforce OR automation)",
  "date_collected": "2025-06-29T03:25:23.945554",
  "target_period": "2025-05",
  "papers": [
    {
      "id": "http://arxiv.org/abs/2506.00742v1",
      "title": "ArtiScene: Language-Driven Artistic 3D Scene Generation Through Image\n  Intermediary",
      "published": "2025-05-31T23:03:54Z",
      "updated": "2025-05-31T23:03:54Z",
      "summary": "Designing 3D scenes is traditionally a challenging task that demands both\nartistic expertise and proficiency with complex software. Recent advances in\ntext-to-3D generation have greatly simplified this process by letting users\ncreate scenes based on simple text descriptions. However, as these methods\ngenerally require extra training or in-context learning, their performance is\noften hindered by the limited availability of high-quality 3D data. In\ncontrast, modern text-to-image models learned from web-scale images can\ngenerate scenes with diverse, reliable spatial layouts and consistent, visually\nappealing styles. Our key insight is that instead of learning directly from 3D\nscenes, we can leverage generated 2D images as an intermediary to guide 3D\nsynthesis. In light of this, we introduce ArtiScene, a training-free automated\npipeline for scene design that integrates the flexibility of free-form\ntext-to-image generation with the diversity and reliability of 2D intermediary\nlayouts.\n  First, we generate 2D images from a scene description, then extract the shape\nand appearance of objects to create 3D models. These models are assembled into\nthe final scene using geometry, position, and pose information derived from the\nsame intermediary image. Being generalizable to a wide range of scenes and\nstyles, ArtiScene outperforms state-of-the-art benchmarks by a large margin in\nlayout and aesthetic quality by quantitative metrics. It also averages a 74.89%\nwinning rate in extensive user studies and 95.07% in GPT-4o evaluation. Project\npage: https://artiscene-cvpr.github.io/",
      "authors": [
        "Zeqi Gu",
        "Yin Cui",
        "Zhaoshuo Li",
        "Fangyin Wei",
        "Yunhao Ge",
        "Jinwei Gu",
        "Ming-Yu Liu",
        "Abe Davis",
        "Yifan Ding"
      ],
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "links": [
        "http://arxiv.org/abs/2506.00742v1",
        "http://arxiv.org/pdf/2506.00742v1"
      ],
      "primary_search_term": "artificial intelligence",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2506.00694v2",
      "title": "Measuring Faithfulness and Abstention: An Automated Pipeline for\n  Evaluating LLM-Generated 3-ply Case-Based Legal Arguments",
      "published": "2025-05-31T19:56:40Z",
      "updated": "2025-06-03T03:22:48Z",
      "summary": "Large Language Models (LLMs) demonstrate potential in complex legal tasks\nlike argument generation, yet their reliability remains a concern. Building\nupon pilot work assessing LLM generation of 3-ply legal arguments using human\nevaluation, this paper introduces an automated pipeline to evaluate LLM\nperformance on this task, specifically focusing on faithfulness (absence of\nhallucination), factor utilization, and appropriate abstention. We define\nhallucination as the generation of factors not present in the input case\nmaterials and abstention as the model's ability to refrain from generating\narguments when instructed and no factual basis exists. Our automated method\nemploys an external LLM to extract factors from generated arguments and\ncompares them against the ground-truth factors provided in the input case\ntriples (current case and two precedent cases). We evaluated eight distinct\nLLMs on three tests of increasing difficulty: 1) generating a standard 3-ply\nargument, 2) generating an argument with swapped precedent roles, and 3)\nrecognizing the impossibility of argument generation due to lack of shared\nfactors and abstaining. Our findings indicate that while current LLMs achieve\nhigh accuracy (over 90%) in avoiding hallucination on viable argument\ngeneration tests (Tests 1 & 2), they often fail to utilize the full set of\nrelevant factors present in the cases. Critically, on the abstention test (Test\n3), most models failed to follow instructions to stop, instead generating\nspurious arguments despite the lack of common factors. This automated pipeline\nprovides a scalable method for assessing these crucial LLM behaviors,\nhighlighting the need for improvements in factor utilization and robust\nabstention capabilities before reliable deployment in legal settings. Link:\nhttps://lizhang-aiandlaw.github.io/An-Automated-Pipeline-for-Evaluating-LLM-Generated-3-ply-Case-Based-Legal-Arguments/",
      "authors": [
        "Li Zhang",
        "Morgan Gray",
        "Jaromir Savelka",
        "Kevin D. Ashley"
      ],
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG",
        "68T50"
      ],
      "links": [
        "http://arxiv.org/abs/2506.00694v2",
        "http://arxiv.org/pdf/2506.00694v2"
      ],
      "primary_search_term": "artificial intelligence",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2506.00679v1",
      "title": "CineMA: A Foundation Model for Cine Cardiac MRI",
      "published": "2025-05-31T19:12:34Z",
      "updated": "2025-05-31T19:12:34Z",
      "summary": "Cardiac magnetic resonance (CMR) is a key investigation in clinical\ncardiovascular medicine and has been used extensively in population research.\nHowever, extracting clinically important measurements such as ejection fraction\nfor diagnosing cardiovascular diseases remains time-consuming and subjective.\nWe developed CineMA, a foundation AI model automating these tasks with limited\nlabels. CineMA is a self-supervised autoencoder model trained on 74,916 cine\nCMR studies to reconstruct images from masked inputs. After fine-tuning, it was\nevaluated across eight datasets on 23 tasks from four categories: ventricle and\nmyocardium segmentation, left and right ventricle ejection fraction\ncalculation, disease detection and classification, and landmark localisation.\nCineMA is the first foundation model for cine CMR to match or outperform\nconvolutional neural networks (CNNs). CineMA demonstrated greater label\nefficiency than CNNs, achieving comparable or better performance with fewer\nannotations. This reduces the burden of clinician labelling and supports\nreplacing task-specific training with fine-tuning foundation models in future\ncardiac imaging applications. Models and code for pre-training and fine-tuning\nare available at https://github.com/mathpluscode/CineMA, democratising access\nto high-performance models that otherwise require substantial computational\nresources, promoting reproducibility and accelerating clinical translation.",
      "authors": [
        "Yunguan Fu",
        "Weixi Yi",
        "Charlotte Manisty",
        "Anish N Bhuva",
        "Thomas A Treibel",
        "James C Moon",
        "Matthew J Clarkson",
        "Rhodri Huw Davies",
        "Yipeng Hu"
      ],
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "links": [
        "http://arxiv.org/abs/2506.00679v1",
        "http://arxiv.org/pdf/2506.00679v1"
      ],
      "primary_search_term": "artificial intelligence",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2506.00664v1",
      "title": "OntoRAG: Enhancing Question-Answering through Automated Ontology\n  Derivation from Unstructured Knowledge Bases",
      "published": "2025-05-31T18:33:39Z",
      "updated": "2025-05-31T18:33:39Z",
      "summary": "Ontologies are pivotal for structuring knowledge bases to enhance question\nanswering (QA) systems powered by Large Language Models (LLMs). However,\ntraditional ontology creation relies on manual efforts by domain experts, a\nprocess that is time intensive, error prone, and impractical for large, dynamic\nknowledge domains. This paper introduces OntoRAG, an automated pipeline\ndesigned to derive ontologies from unstructured knowledge bases, with a focus\non electrical relay documents. OntoRAG integrates advanced techniques,\nincluding web scraping, PDF parsing, hybrid chunking, information extraction,\nknowledge graph construction, and ontology creation, to transform unstructured\ndata into a queryable ontology. By leveraging LLMs and graph based methods,\nOntoRAG enhances global sensemaking capabilities, outperforming conventional\nRetrieval Augmented Generation (RAG) and GraphRAG approaches in\ncomprehensiveness and diversity. Experimental results demonstrate OntoRAGs\neffectiveness, achieving a comprehensiveness win rate of 85% against vector RAG\nand 75% against GraphRAGs best configuration. This work addresses the critical\nchallenge of automating ontology creation, advancing the vision of the semantic\nweb.",
      "authors": [
        "Yash Tiwari",
        "Owais Ahmad Lone",
        "Mayukha Pal"
      ],
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "links": [
        "http://arxiv.org/abs/2506.00664v1",
        "http://arxiv.org/pdf/2506.00664v1"
      ],
      "primary_search_term": "artificial intelligence",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2506.12060v1",
      "title": "Organizational Adaptation to Generative AI in Cybersecurity: A\n  Systematic Review",
      "published": "2025-05-31T18:16:11Z",
      "updated": "2025-05-31T18:16:11Z",
      "summary": "Cybersecurity organizations are adapting to GenAI integration through\nmodified frameworks and hybrid operational processes, with success influenced\nby existing security maturity, regulatory requirements, and investments in\nhuman capital and infrastructure. This qualitative research employs systematic\ndocument analysis and comparative case study methodology to examine how\ncybersecurity organizations adapt their threat modeling frameworks and\noperational processes to address generative artificial intelligence\nintegration. Through examination of 25 studies from 2022 to 2025, the research\ndocuments substantial transformation in organizational approaches to threat\nmodeling, moving from traditional signature-based systems toward frameworks\nincorporating artificial intelligence capabilities. The research identifies\nthree primary adaptation patterns: Large Language Model integration for\nsecurity applications, GenAI frameworks for risk detection and response\nautomation, and AI/ML integration for threat hunting. Organizations with mature\nsecurity infrastructures, particularly in finance and critical infrastructure\nsectors, demonstrate higher readiness through structured governance approaches,\ndedicated AI teams, and robust incident response processes. Organizations\nachieve successful GenAI integration when they maintain appropriate human\noversight of automated systems, address data quality concerns and\nexplainability requirements, and establish governance frameworks tailored to\ntheir specific sectors. Organizations encounter ongoing difficulties with\nprivacy protection, bias reduction, personnel training, and defending against\nadversarial attacks. This work advances understanding of how organizations\nadopt innovative technologies in high-stakes environments and offers actionable\ninsights for cybersecurity professionals implementing GenAI systems.",
      "authors": [
        "Christopher Nott"
      ],
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CY",
        "K.6.5; I.2.0; K.4.1"
      ],
      "links": [
        "http://arxiv.org/abs/2506.12060v1",
        "http://arxiv.org/pdf/2506.12060v1"
      ],
      "primary_search_term": "artificial intelligence",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2506.00633v1",
      "title": "Text-to-CT Generation via 3D Latent Diffusion Model with Contrastive\n  Vision-Language Pretraining",
      "published": "2025-05-31T16:41:55Z",
      "updated": "2025-05-31T16:41:55Z",
      "summary": "Objective: While recent advances in text-conditioned generative models have\nenabled the synthesis of realistic medical images, progress has been largely\nconfined to 2D modalities such as chest X-rays. Extending text-to-image\ngeneration to volumetric Computed Tomography (CT) remains a significant\nchallenge, due to its high dimensionality, anatomical complexity, and the\nabsence of robust frameworks that align vision-language data in 3D medical\nimaging. Methods: We introduce a novel architecture for Text-to-CT generation\nthat combines a latent diffusion model with a 3D contrastive vision-language\npretraining scheme. Our approach leverages a dual-encoder CLIP-style model\ntrained on paired CT volumes and radiology reports to establish a shared\nembedding space, which serves as the conditioning input for generation. CT\nvolumes are compressed into a low-dimensional latent space via a pretrained\nvolumetric VAE, enabling efficient 3D denoising diffusion without requiring\nexternal super-resolution stages. Results: We evaluate our method on the\nCT-RATE dataset and conduct a comprehensive assessment of image fidelity,\nclinical relevance, and semantic alignment. Our model achieves competitive\nperformance across all tasks, significantly outperforming prior baselines for\ntext-to-CT generation. Moreover, we demonstrate that CT scans synthesized by\nour framework can effectively augment real data, improving downstream\ndiagnostic performance. Conclusion: Our results show that modality-specific\nvision-language alignment is a key component for high-quality 3D medical image\ngeneration. By integrating contrastive pretraining and volumetric diffusion,\nour method offers a scalable and controllable solution for synthesizing\nclinically meaningful CT volumes from text, paving the way for new applications\nin data augmentation, medical education, and automated clinical simulation.",
      "authors": [
        "Daniele Molino",
        "Camillo Maria Caruso",
        "Filippo Ruffini",
        "Paolo Soda",
        "Valerio Guarrasi"
      ],
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "links": [
        "http://arxiv.org/abs/2506.00633v1",
        "http://arxiv.org/pdf/2506.00633v1"
      ],
      "primary_search_term": "artificial intelligence",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2506.00613v1",
      "title": "Evaluating Robot Policies in a World Model",
      "published": "2025-05-31T15:51:56Z",
      "updated": "2025-05-31T15:51:56Z",
      "summary": "Robotics has broad applications from automating house chores to taking care\nof patients. However, evaluating robot control policies is challenging, as\nreal-world testing is expensive, while handcrafted simulations often fail to\naccurately reflect real-world conditions, resulting in poor correlation between\nsimulated evaluation and real-world outcomes. In this work, we investigate\nWorld-model-based Policy Evaluation (WPE). We first train an action-conditioned\nvideo generation model as a proxy to real-world environments. To enable\nefficient rollouts of hundreds of interactive steps while mitigating error\naccumulation in the world model, we propose an inference scheme which we call\nBlockwise-Autoregressive Diffusion Transformer with adjustable context and\ndecoding horizon lengths. To ensure that the world model indeed follows action\ninput, we propose metrics based on the agreement between the ground truth video\nand generated video conditioned on the same sequence of actions to evaluate the\nworld model. We then use the world model for policy evaluation by performing\nMonte Carlo rollouts in the world model while employing a vision-language model\n(VLM) as a reward function. Interestingly, we found that WPE tends to\nunderestimate the policy values for in-distribution actions and overestimate\npolicy values for out-of-distribution actions. Nevertheless, WPE preserves the\nrelative rankings of different policies. In emulating real robot executions,\nWPE achieves high fidelity in mimicing robot arm movements as in real videos,\nwhile emulating highly realistic object interaction remains challenging.\nDespite this limitation, we show that a world model can serve as a starting\npoint for evaluating robot policies before real-world deployment.",
      "authors": [
        "Julian Quevedo",
        "Percy Liang",
        "Sherry Yang"
      ],
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "links": [
        "http://arxiv.org/abs/2506.00613v1",
        "http://arxiv.org/pdf/2506.00613v1"
      ],
      "primary_search_term": "artificial intelligence",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2506.00577v1",
      "title": "Reasoning Like an Economist: Post-Training on Economic Problems Induces\n  Strategic Generalization in LLMs",
      "published": "2025-05-31T14:22:40Z",
      "updated": "2025-05-31T14:22:40Z",
      "summary": "Directly training Large Language Models (LLMs) for Multi-Agent Systems (MAS)\nremains challenging due to intricate reward modeling, dynamic agent\ninteractions, and demanding generalization requirements. This paper explores\nwhether post-training techniques, specifically Supervised Fine-Tuning (SFT) and\nReinforcement Learning with Verifiable Rewards (RLVR), can effectively\n$\\textit{generalize}$ to multi-agent scenarios. We use economic reasoning as a\ntestbed, leveraging its strong foundations in mathematics and game theory, its\ndemand for structured analytical reasoning, and its relevance to real-world\napplications such as market design, resource allocation, and policy analysis.\nWe introduce $\\textbf{Recon}$ ($\\textbf{R}$easoning like an\n$\\textbf{ECON}$omist), a 7B-parameter open-source LLM post-trained on a\nhand-curated dataset of 2,100 high-quality economic reasoning problems.\nComprehensive evaluation on economic reasoning benchmarks and multi-agent games\nreveals clear improvements in structured reasoning and economic rationality.\nThese results underscore the promise of domain-aligned post-training for\nenhancing reasoning and agent alignment, shedding light on the roles of SFT and\nRL in shaping model behavior. Code is available at\nhttps://github.com/MasterZhou1/Recon .",
      "authors": [
        "Yufa Zhou",
        "Shaobo Wang",
        "Xingyu Dong",
        "Xiangqi Jin",
        "Yifang Chen",
        "Yue Min",
        "Kexin Yang",
        "Xingzhang Ren",
        "Dayiheng Liu",
        "Linfeng Zhang"
      ],
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.GT",
        "cs.MA"
      ],
      "links": [
        "http://arxiv.org/abs/2506.00577v1",
        "http://arxiv.org/pdf/2506.00577v1"
      ],
      "primary_search_term": "artificial intelligence",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2506.06335v1",
      "title": "FinBERT2: A Specialized Bidirectional Encoder for Bridging the Gap in\n  Finance-Specific Deployment of Large Language Models",
      "published": "2025-05-31T13:59:44Z",
      "updated": "2025-05-31T13:59:44Z",
      "summary": "In natural language processing (NLP), the focus has shifted from encoder-only\ntiny language models like BERT to decoder-only large language models(LLMs) such\nas GPT-3. However, LLMs' practical application in the financial sector has\nrevealed three limitations: (1) LLMs often perform worse than fine-tuned BERT\non discriminative tasks despite costing much higher computational resources,\nsuch as market sentiment analysis in financial reports; (2) Application on\ngenerative tasks heavily relies on retrieval augmented generation (RAG) methods\nto provide current and specialized information, with general retrievers showing\nsuboptimal performance on domain-specific retrieval tasks; (3) There are\nadditional inadequacies in other feature-based scenarios, such as topic\nmodeling. We introduce FinBERT2, a specialized bidirectional encoder pretrained\non a high-quality, financial-specific corpus of 32b tokens. This represents the\nlargest known Chinese financial pretraining corpus for models of this parameter\nsize. As a better backbone, FinBERT2 can bridge the gap in the\nfinancial-specific deployment of LLMs through the following achievements: (1)\nDiscriminative fine-tuned models (Fin-Labelers) outperform other (Fin)BERT\nvariants by 0.4%-3.3% and leading LLMs by 9.7%-12.3% on average across five\nfinancial classification tasks. (2) Contrastive fine-tuned models\n(Fin-Retrievers) outperform both open-source (e.g., +6.8\\% avg improvement over\nBGE-base-zh) and proprietary (e.g., +4.2\\% avg improvement over OpenAI's\ntext-embedding-3-large) embedders across five financial retrieval tasks; (3)\nBuilding on FinBERT2 variants, we construct the Fin-TopicModel, which enables\nsuperior clustering and topic representation for financial titles. Our work\nrevisits financial BERT models through comparative analysis with contemporary\nLLMs and offers practical insights for effectively utilizing FinBERT in the\nLLMs era.",
      "authors": [
        "Xuan Xu",
        "Fufang Wen",
        "Beilin Chu",
        "Zhibing Fu",
        "Qinhong Lin",
        "Jiaqi Liu",
        "Binjie Fei",
        "Zhongliang Yang",
        "Linna Zhou",
        "Yu Li"
      ],
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CE",
        "cs.CL"
      ],
      "links": [
        "http://arxiv.org/abs/2506.06335v1",
        "http://arxiv.org/pdf/2506.06335v1"
      ],
      "primary_search_term": "artificial intelligence",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2506.00551v2",
      "title": "AnnaAgent: Dynamic Evolution Agent System with Multi-Session Memory for\n  Realistic Seeker Simulation",
      "published": "2025-05-31T13:15:51Z",
      "updated": "2025-06-10T16:35:02Z",
      "summary": "Constrained by the cost and ethical concerns of involving real seekers in\nAI-driven mental health, researchers develop LLM-based conversational agents\n(CAs) with tailored configurations, such as profiles, symptoms, and scenarios,\nto simulate seekers. While these efforts advance AI in mental health, achieving\nmore realistic seeker simulation remains hindered by two key challenges:\ndynamic evolution and multi-session memory. Seekers' mental states often\nfluctuate during counseling, which typically spans multiple sessions. To\naddress this, we propose AnnaAgent, an emotional and cognitive dynamic agent\nsystem equipped with tertiary memory. AnnaAgent incorporates an emotion\nmodulator and a complaint elicitor trained on real counseling dialogues,\nenabling dynamic control of the simulator's configurations. Additionally, its\ntertiary memory mechanism effectively integrates short-term and long-term\nmemory across sessions. Evaluation results, both automated and manual,\ndemonstrate that AnnaAgent achieves more realistic seeker simulation in\npsychological counseling compared to existing baselines. The ethically reviewed\nand screened code can be found on https://github.com/sci-m-wang/AnnaAgent.",
      "authors": [
        "Ming Wang",
        "Peidong Wang",
        "Lin Wu",
        "Xiaocui Yang",
        "Daling Wang",
        "Shi Feng",
        "Yuxin Chen",
        "Bixuan Wang",
        "Yifei Zhang"
      ],
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "links": [
        "http://arxiv.org/abs/2506.00551v2",
        "http://arxiv.org/pdf/2506.00551v2"
      ],
      "primary_search_term": "artificial intelligence",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2506.00481v1",
      "title": "PVP: An Image Dataset for Personalized Visual Persuasion with Persuasion\n  Strategies, Viewer Characteristics, and Persuasiveness Ratings",
      "published": "2025-05-31T09:21:57Z",
      "updated": "2025-05-31T09:21:57Z",
      "summary": "Visual persuasion, which uses visual elements to influence cognition and\nbehaviors, is crucial in fields such as advertising and political\ncommunication. With recent advancements in artificial intelligence, there is\ngrowing potential to develop persuasive systems that automatically generate\npersuasive images tailored to individuals. However, a significant bottleneck in\nthis area is the lack of comprehensive datasets that connect the persuasiveness\nof images with the personal information about those who evaluated the images.\nTo address this gap and facilitate technological advancements in personalized\nvisual persuasion, we release the Personalized Visual Persuasion (PVP) dataset,\ncomprising 28,454 persuasive images across 596 messages and 9 persuasion\nstrategies. Importantly, the PVP dataset provides persuasiveness scores of\nimages evaluated by 2,521 human annotators, along with their demographic and\npsychological characteristics (personality traits and values). We demonstrate\nthe utility of our dataset by developing a persuasive image generator and an\nautomated evaluator, and establish benchmark baselines. Our experiments reveal\nthat incorporating psychological characteristics enhances the generation and\nevaluation of persuasive images, providing valuable insights for personalized\nvisual persuasion.",
      "authors": [
        "Junseo Kim",
        "Jongwook Han",
        "Dongmin Choi",
        "Jongwook Yoon",
        "Eun-Ju Lee",
        "Yohan Jo"
      ],
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "links": [
        "http://arxiv.org/abs/2506.00481v1",
        "http://arxiv.org/pdf/2506.00481v1"
      ],
      "primary_search_term": "artificial intelligence",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2506.00474v1",
      "title": "A European Multi-Center Breast Cancer MRI Dataset",
      "published": "2025-05-31T08:45:02Z",
      "updated": "2025-05-31T08:45:02Z",
      "summary": "Detecting breast cancer early is of the utmost importance to effectively\ntreat the millions of women afflicted by breast cancer worldwide every year.\nAlthough mammography is the primary imaging modality for screening breast\ncancer, there is an increasing interest in adding magnetic resonance imaging\n(MRI) to screening programmes, particularly for women at high risk. Recent\nguidelines by the European Society of Breast Imaging (EUSOBI) recommended\nbreast MRI as a supplemental screening tool for women with dense breast tissue.\nHowever, acquiring and reading MRI scans requires significantly more time from\nexpert radiologists. This highlights the need to develop new automated methods\nto detect cancer accurately using MRI and Artificial Intelligence (AI), which\nhave the potential to support radiologists in breast MRI interpretation and\nclassification and help detect cancer earlier. For this reason, the ODELIA\nconsortium has made this multi-centre dataset publicly available to assist in\ndeveloping AI tools for the detection of breast cancer on MRI.",
      "authors": [
        "Gustav M\u00fcller-Franzes",
        "Lorena Escudero S\u00e1nchez",
        "Nicholas Payne",
        "Alexandra Athanasiou",
        "Michael Kalogeropoulos",
        "Aitor Lopez",
        "Alfredo Miguel Soro Busto",
        "Julia Camps Herrero",
        "Nika Rasoolzadeh",
        "Tianyu Zhang",
        "Ritse Mann",
        "Debora Jutz",
        "Maike Bode",
        "Christiane Kuhl",
        "Wouter Veldhuis",
        "Oliver Lester Saldanha",
        "JieFu Zhu",
        "Jakob Nikolas Kather",
        "Daniel Truhn",
        "Fiona J. Gilbert"
      ],
      "categories": [
        "eess.IV",
        "cs.CV"
      ],
      "links": [
        "http://arxiv.org/abs/2506.00474v1",
        "http://arxiv.org/pdf/2506.00474v1"
      ],
      "primary_search_term": "artificial intelligence",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2506.00467v1",
      "title": "SST: Self-training with Self-adaptive Thresholding for Semi-supervised\n  Learning",
      "published": "2025-05-31T08:34:04Z",
      "updated": "2025-05-31T08:34:04Z",
      "summary": "Neural networks have demonstrated exceptional performance in supervised\nlearning, benefiting from abundant high-quality annotated data. However,\nobtaining such data in real-world scenarios is costly and labor-intensive.\nSemi-supervised learning (SSL) offers a solution to this problem. Recent\nstudies, such as Semi-ViT and Noisy Student, which employ consistency\nregularization or pseudo-labeling, have demonstrated significant achievements.\nHowever, they still face challenges, particularly in accurately selecting\nsufficient high-quality pseudo-labels due to their reliance on fixed\nthresholds. Recent methods such as FlexMatch and FreeMatch have introduced\nflexible or self-adaptive thresholding techniques, greatly advancing SSL\nresearch. Nonetheless, their process of updating thresholds at each iteration\nis deemed time-consuming, computationally intensive, and potentially\nunnecessary. To address these issues, we propose Self-training with\nSelf-adaptive Thresholding (SST), a novel, effective, and efficient SSL\nframework. SST introduces an innovative Self-Adaptive Thresholding (SAT)\nmechanism that adaptively adjusts class-specific thresholds based on the\nmodel's learning progress. SAT ensures the selection of high-quality\npseudo-labeled data, mitigating the risks of inaccurate pseudo-labels and\nconfirmation bias. Extensive experiments demonstrate that SST achieves\nstate-of-the-art performance with remarkable efficiency, generalization, and\nscalability across various architectures and datasets. Semi-SST-ViT-Huge\nachieves the best results on competitive ImageNet-1K SSL benchmarks, with 80.7%\n/ 84.9% Top-1 accuracy using only 1% / 10% labeled data. Compared to the\nfully-supervised DeiT-III-ViT-Huge, which achieves 84.8% Top-1 accuracy using\n100% labeled data, our method demonstrates superior performance using only 10%\nlabeled data.",
      "authors": [
        "Shuai Zhao",
        "Heyan Huang",
        "Xinge Li",
        "Xiaokang Chen",
        "Rui Wang"
      ],
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "links": [
        "http://dx.doi.org/10.1016/j.ipm.2025.104158",
        "http://arxiv.org/abs/2506.00467v1",
        "http://arxiv.org/pdf/2506.00467v1"
      ],
      "primary_search_term": "artificial intelligence",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2506.00455v2",
      "title": "Diffusion Graph Neural Networks for Robustness in Olfaction Sensors and\n  Datasets",
      "published": "2025-05-31T08:22:09Z",
      "updated": "2025-06-15T01:39:21Z",
      "summary": "Robotic odour source localization (OSL) is a critical capability for\nautonomous systems operating in complex environments. However, current OSL\nmethods often suffer from ambiguities, particularly when robots misattribute\nodours to incorrect objects due to limitations in olfactory datasets and sensor\nresolutions. To address this challenge, we introduce a novel machine learning\nmethod using diffusion-based molecular generation to enhance odour localization\naccuracy that can be used by itself or with automated olfactory dataset\nconstruction pipelines with vision-language models (VLMs) This generative\nprocess of our diffusion model expands the chemical space beyond the\nlimitations of both current olfactory datasets and the training data of VLMs,\nenabling the identification of potential odourant molecules not previously\ndocumented. The generated molecules can then be more accurately validated using\nadvanced olfactory sensors which emulate human olfactory recognition through\nelectronic sensor arrays. By integrating visual analysis, language processing,\nand molecular generation, our framework enhances the ability of\nolfaction-vision models on robots to accurately associate odours with their\ncorrect sources, thereby improving navigation and decision-making through\nbetter sensor selection for a target compound. Our methodology represents a\nfoundational advancement in the field of artificial olfaction, offering a\nscalable solution to the challenges posed by limited olfactory data and sensor\nambiguities.",
      "authors": [
        "Kordel K. France",
        "Ovidiu Daescu"
      ],
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "links": [
        "http://arxiv.org/abs/2506.00455v2",
        "http://arxiv.org/pdf/2506.00455v2"
      ],
      "primary_search_term": "artificial intelligence",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2506.00454v1",
      "title": "Towards Temporally Explainable Dysarthric Speech Clarity Assessment",
      "published": "2025-05-31T08:16:54Z",
      "updated": "2025-05-31T08:16:54Z",
      "summary": "Dysarthria, a motor speech disorder, affects intelligibility and requires\ntargeted interventions for effective communication. In this work, we\ninvestigate automated mispronunciation feedback by collecting a dysarthric\nspeech dataset from six speakers reading two passages, annotated by a speech\ntherapist with temporal markers and mispronunciation descriptions. We design a\nthree-stage framework for explainable mispronunciation evaluation: (1) overall\nclarity scoring, (2) mispronunciation localization, and (3) mispronunciation\ntype classification. We systematically analyze pretrained Automatic Speech\nRecognition (ASR) models in each stage, assessing their effectiveness in\ndysarthric speech evaluation (Code available at:\nhttps://github.com/augmented-human-lab/interspeech25_speechtherapy,\nSupplementary webpage: https://apps.ahlab.org/interspeech25_speechtherapy/).\nOur findings offer clinically relevant insights for automating actionable\nfeedback for pronunciation assessment, which could enable independent practice\nfor patients and help therapists deliver more effective interventions.",
      "authors": [
        "Seohyun Park",
        "Chitralekha Gupta",
        "Michelle Kah Yian Kwan",
        "Xinhui Fung",
        "Alexander Wenjun Yip",
        "Suranga Nanayakkara"
      ],
      "categories": [
        "eess.AS",
        "cs.HC",
        "cs.SD"
      ],
      "links": [
        "http://arxiv.org/abs/2506.00454v1",
        "http://arxiv.org/pdf/2506.00454v1"
      ],
      "primary_search_term": "artificial intelligence",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2506.00436v2",
      "title": "Learning from Double Positive and Unlabeled Data for Potential-Customer\n  Identification",
      "published": "2025-05-31T07:33:48Z",
      "updated": "2025-06-09T09:56:53Z",
      "summary": "In this study, we propose a method for identifying potential customers in\ntargeted marketing by applying learning from positive and unlabeled data (PU\nlearning). We consider a scenario in which a company sells a product and can\nobserve only the customers who purchased it. Decision-makers seek to market\nproducts effectively based on whether people have loyalty to the company.\nIndividuals with loyalty are those who are likely to remain interested in the\ncompany even without additional advertising. Consequently, those loyal\ncustomers would likely purchase from the company if they are interested in the\nproduct. In contrast, people with lower loyalty may overlook the product or buy\nsimilar products from other companies unless they receive marketing attention.\nTherefore, by focusing marketing efforts on individuals who are interested in\nthe product but do not have strong loyalty, we can achieve more efficient\nmarketing. To achieve this goal, we consider how to learn, from limited data, a\nclassifier that identifies potential customers who (i) have interest in the\nproduct and (ii) do not have loyalty to the company. Although our algorithm\ncomprises a single-stage optimization, its objective function implicitly\ncontains two losses derived from standard PU learning settings. For this\nreason, we refer to our approach as double PU learning. We verify the validity\nof the proposed algorithm through numerical experiments, confirming that it\nfunctions appropriately for the problem at hand.",
      "authors": [
        "Masahiro Kato",
        "Yuki Ikeda",
        "Kentaro Baba",
        "Takashi Imai",
        "Ryo Inokuchi"
      ],
      "categories": [
        "cs.LG",
        "cs.AI",
        "econ.EM",
        "stat.ME",
        "stat.ML"
      ],
      "links": [
        "http://arxiv.org/abs/2506.00436v2",
        "http://arxiv.org/pdf/2506.00436v2"
      ],
      "primary_search_term": "artificial intelligence",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2506.02037v1",
      "title": "FinS-Pilot: A Benchmark for Online Financial System",
      "published": "2025-05-31T03:50:19Z",
      "updated": "2025-05-31T03:50:19Z",
      "summary": "Large language models (LLMs) have demonstrated remarkable capabilities across\nvarious professional domains, with their performance typically evaluated\nthrough standardized benchmarks. However, the development of financial RAG\nbenchmarks has been constrained by data confidentiality issues and the lack of\ndynamic data integration. To address this issue, we introduces FinS-Pilot, a\nnovel benchmark for evaluating RAG systems in online financial applications.\nConstructed from real-world financial assistant interactions, our benchmark\nincorporates both real-time API data and structured text sources, organized\nthrough an intent classification framework covering critical financial domains\nsuch as equity analysis and macroeconomic forecasting. The benchmark enables\ncomprehensive evaluation of financial assistants' capabilities in handling both\nstatic knowledge and time-sensitive market information. Through systematic\nexperiments with multiple Chinese leading LLMs, we demonstrate FinS-Pilot's\neffectiveness in identifying models suitable for financial applications while\naddressing the current gap in specialized evaluation tools for the financial\ndomain. Our work contributes both a practical evaluation framework and a\ncurated dataset to advance research in financial NLP systems. The code and\ndataset are accessible on\nGitHub\\footnote{https://github.com/PhealenWang/financial\\_rag\\_benchmark}.",
      "authors": [
        "Feng Wang",
        "Yiding Sun",
        "Jiaxin Mao",
        "Wei Xue",
        "Danqing Xu"
      ],
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "links": [
        "http://arxiv.org/abs/2506.02037v1",
        "http://arxiv.org/pdf/2506.02037v1"
      ],
      "primary_search_term": "artificial intelligence",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2506.00274v1",
      "title": "Chances and Challenges of the Model Context Protocol in Digital\n  Forensics and Incident Response",
      "published": "2025-05-30T22:15:48Z",
      "updated": "2025-05-30T22:15:48Z",
      "summary": "Large language models hold considerable promise for supporting forensic\ninvestigations, but their widespread adoption is hindered by a lack of\ntransparency, explainability, and reproducibility. This paper explores how the\nemerging Model Context Protocol can address these challenges and support the\nmeaningful use of LLMs in digital forensics. Through a theoretical analysis, we\nexamine how MCP can be integrated across various forensic scenarios - ranging\nfrom artifact analysis to the generation of interpretable reports. We also\noutline both technical and conceptual considerations for deploying an MCP\nserver in forensic environments. Our analysis reveals a wide range of use cases\nin which MCP not only strengthens existing forensic workflows but also\nfacilitates the application of LLMs to areas of forensics where their use was\npreviously limited. Furthermore, we introduce the concept of the inference\nconstraint level - a way of characterizing how specific MCP design choices can\ndeliberately constrain model behavior, thereby enhancing both auditability and\ntraceability. Our insights demonstrate that MCP has significant potential as a\nfoundational component for developing LLM-assisted forensic workflows that are\nnot only more transparent, reproducible, and legally defensible, but also\nrepresent a step toward increased automation in digital forensic analysis.\nHowever, we also highlight potential challenges that the adoption of MCP may\npose for digital forensics in the future.",
      "authors": [
        "Jan-Niclas Hilgert",
        "Carlo Jakobs",
        "Michael K\u00fclper",
        "Martin Lambertz",
        "Axel Mahr",
        "Elmar Padilla"
      ],
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "links": [
        "http://arxiv.org/abs/2506.00274v1",
        "http://arxiv.org/pdf/2506.00274v1"
      ],
      "primary_search_term": "artificial intelligence",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2506.00249v1",
      "title": "MIR: Methodology Inspiration Retrieval for Scientific Research Problems",
      "published": "2025-05-30T21:33:03Z",
      "updated": "2025-05-30T21:33:03Z",
      "summary": "There has been a surge of interest in harnessing the reasoning capabilities\nof Large Language Models (LLMs) to accelerate scientific discovery. While\nexisting approaches rely on grounding the discovery process within the relevant\nliterature, effectiveness varies significantly with the quality and nature of\nthe retrieved literature. We address the challenge of retrieving prior work\nwhose concepts can inspire solutions for a given research problem, a task we\ndefine as Methodology Inspiration Retrieval (MIR). We construct a novel dataset\ntailored for training and evaluating retrievers on MIR, and establish\nbaselines. To address MIR, we build the Methodology Adjacency Graph (MAG);\ncapturing methodological lineage through citation relationships. We leverage\nMAG to embed an \"intuitive prior\" into dense retrievers for identifying\npatterns of methodological inspiration beyond superficial semantic similarity.\nThis achieves significant gains of +5.4 in Recall@3 and +7.8 in Mean Average\nPrecision (mAP) over strong baselines. Further, we adapt LLM-based re-ranking\nstrategies to MIR, yielding additional improvements of +4.5 in Recall@3 and\n+4.8 in mAP. Through extensive ablation studies and qualitative analyses, we\nexhibit the promise of MIR in enhancing automated scientific discovery and\noutline avenues for advancing inspiration-driven retrieval.",
      "authors": [
        "Aniketh Garikaparthi",
        "Manasi Patwardhan",
        "Aditya Sanjiv Kanade",
        "Aman Hassan",
        "Lovekesh Vig",
        "Arman Cohan"
      ],
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "links": [
        "http://arxiv.org/abs/2506.00249v1",
        "http://arxiv.org/pdf/2506.00249v1"
      ],
      "primary_search_term": "artificial intelligence",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2506.00241v1",
      "title": "Designing AI Tools for Clinical Care Teams to Support Serious Illness\n  Conversations with Older Adults in the Emergency Department",
      "published": "2025-05-30T21:15:57Z",
      "updated": "2025-05-30T21:15:57Z",
      "summary": "Serious illness conversations (SICs), discussions between clinical care teams\nand patients with serious, life-limiting illnesses about their values, goals,\nand care preferences, are critical for patient-centered care. Without these\nconversations, patients often receive aggressive interventions that may not\nalign with their goals. Clinical care teams face significant barriers when\nconducting serious illness conversations with older adult patients in Emergency\nDepartment (ED) settings, where most older adult patients lack documented\ntreatment goals. To understand current practices and identify AI support\nopportunities, we conducted interviews with two domain experts and nine ED\nclinical care team members. Through thematic analysis, we characterized a\nfour-phase serious illness conversation workflow (identification, preparation,\nconduction, documentation) and identified key needs and challenges at each\nstage. Clinical care teams struggle with fragmented EHR data access, time\nconstraints, emotional preparation demands, and documentation burdens. While\nparticipants expressed interest in AI tools for information synthesis,\nconversational support, and automated documentation, they emphasized preserving\nhuman connection and clinical autonomy. We present design guidelines for AI\ntools supporting SIC workflows that fit within existing clinical practices.\nThis work contributes empirical understanding of ED-based serious illness\nconversations and provides design considerations for AI in high-stakes clinical\nenvironments.",
      "authors": [
        "Menglin Zhao",
        "Zhuorui Yong",
        "Ruijia Guan",
        "Kai-Wei Chang",
        "Adrian Haimovich",
        "Kei Ouchi",
        "Timothy Bickmore",
        "Bingsheng Yao",
        "Dakuo Wang",
        "Smit Desai"
      ],
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "links": [
        "http://arxiv.org/abs/2506.00241v1",
        "http://arxiv.org/pdf/2506.00241v1"
      ],
      "primary_search_term": "artificial intelligence",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2506.00233v1",
      "title": "Ethical AI: Towards Defining a Collective Evaluation Framework",
      "published": "2025-05-30T21:10:47Z",
      "updated": "2025-05-30T21:10:47Z",
      "summary": "Artificial Intelligence (AI) is transforming sectors such as healthcare,\nfinance, and autonomous systems, offering powerful tools for innovation. Yet\nits rapid integration raises urgent ethical concerns related to data ownership,\nprivacy, and systemic bias. Issues like opaque decision-making, misleading\noutputs, and unfair treatment in high-stakes domains underscore the need for\ntransparent and accountable AI systems. This article addresses these challenges\nby proposing a modular ethical assessment framework built on ontological blocks\nof meaning-discrete, interpretable units that encode ethical principles such as\nfairness, accountability, and ownership. By integrating these blocks with FAIR\n(Findable, Accessible, Interoperable, Reusable) principles, the framework\nsupports scalable, transparent, and legally aligned ethical evaluations,\nincluding compliance with the EU AI Act. Using a real-world use case in\nAI-powered investor profiling, the paper demonstrates how the framework enables\ndynamic, behavior-informed risk classification. The findings suggest that\nontological blocks offer a promising path toward explainable and auditable AI\nethics, though challenges remain in automation and probabilistic reasoning.",
      "authors": [
        "Aasish Kumar Sharma",
        "Dimitar Kyosev",
        "Julian Kunkel"
      ],
      "categories": [
        "cs.AI"
      ],
      "links": [
        "http://arxiv.org/abs/2506.00233v1",
        "http://arxiv.org/pdf/2506.00233v1"
      ],
      "primary_search_term": "artificial intelligence",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2506.03188v1",
      "title": "Multi-Analyte, Swab-based Automated Wound Monitor with AI",
      "published": "2025-05-30T20:42:37Z",
      "updated": "2025-05-30T20:42:37Z",
      "summary": "Diabetic foot ulcers (DFUs), a class of chronic wounds, affect ~750,000\nindividuals every year in the US alone and identifying non-healing DFUs that\ndevelop to chronic wounds early can drastically reduce treatment costs and\nminimize risks of amputation. There is therefore a pressing need for diagnostic\ntools that can detect non-healing DFUs early. We develop a low cost,\nmulti-analyte 3D printed assays seamlessly integrated on swabs that can\nidentify non-healing DFUs and a Wound Sensor iOS App - an innovative mobile\napplication developed for the controlled acquisition and automated analysis of\nwound sensor data. By comparing both the original base image (before exposure\nto the wound) and the wound-exposed image, we developed automated computer\nvision techniques to compare density changes between the two assay images,\nwhich allow us to automatically determine the severity of the wound. The iOS\napp ensures accurate data collection and presents actionable insights, despite\nchallenges such as variations in camera configurations and ambient conditions.\nThe proposed integrated sensor and iOS app will allow healthcare professionals\nto monitor wound conditions real-time, track healing progress, and assess\ncritical parameters related to wound care.",
      "authors": [
        "Madhu Babu Sikha",
        "Lalith Appari",
        "Gurudatt Nanjanagudu Ganesh",
        "Amay Bandodkar",
        "Imon Banerjee"
      ],
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV",
        "cs.HC"
      ],
      "links": [
        "http://arxiv.org/abs/2506.03188v1",
        "http://arxiv.org/pdf/2506.03188v1"
      ],
      "primary_search_term": "artificial intelligence",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2506.00202v3",
      "title": "What do professional software developers need to know to succeed in an\n  age of Artificial Intelligence?",
      "published": "2025-05-30T20:14:03Z",
      "updated": "2025-06-23T08:27:54Z",
      "summary": "Generative AI is showing early evidence of productivity gains for software\ndevelopers, but concerns persist regarding workforce disruption and deskilling.\nWe describe our research with 21 developers at the cutting edge of using AI,\nsummarizing 12 of their work goals we uncovered, together with 75 associated\ntasks and the skills & knowledge for each, illustrating how developers use AI\nat work. From all of these, we distilled our findings in the form of 5\ninsights. We found that the skills & knowledge to be a successful AI-enhanced\ndeveloper are organized into four domains (using Generative AI effectively,\ncore software engineering, adjacent engineering, and adjacent non-engineering)\ndeployed at critical junctures throughout a 6-step task workflow. In order to\n\"future proof\" developers for this age of AI, on-the-job learning initiatives\nand computer science degree programs will need to target both \"soft\" skills and\nthe technical skills & knowledge in all four domains to reskill, upskill and\nsafeguard against deskilling.",
      "authors": [
        "Matthew Kam",
        "Cody Miller",
        "Miaoxin Wang",
        "Abey Tidwell",
        "Irene A. Lee",
        "Joyce Malyn-Smith",
        "Beatriz Perez",
        "Vikram Tiwari",
        "Joshua Kenitzer",
        "Andrew Macvean",
        "Erin Barrar"
      ],
      "categories": [
        "cs.AI"
      ],
      "links": [
        "http://dx.doi.org/10.1145/3696630.3727251",
        "http://arxiv.org/abs/2506.00202v3",
        "http://arxiv.org/pdf/2506.00202v3"
      ],
      "primary_search_term": "artificial intelligence",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2506.00178v1",
      "title": "Tournament of Prompts: Evolving LLM Instructions Through Structured\n  Debates and Elo Ratings",
      "published": "2025-05-30T19:33:41Z",
      "updated": "2025-05-30T19:33:41Z",
      "summary": "Prompt engineering represents a critical bottleneck to harness the full\npotential of Large Language Models (LLMs) for solving complex tasks, as it\nrequires specialized expertise, significant trial-and-error, and manual\nintervention. This challenge is particularly pronounced for tasks involving\nsubjective quality assessment, where defining explicit optimization objectives\nbecomes fundamentally problematic. Existing automated prompt optimization\nmethods falter in these scenarios, as they typically require well-defined\ntask-specific numerical fitness functions or rely on generic templates that\ncannot capture the nuanced requirements of complex use cases. We introduce\nDEEVO (DEbate-driven EVOlutionary prompt optimization), a novel framework that\nguides prompt evolution through a debate-driven evaluation with an Elo-based\nselection. Contrary to prior work, DEEVOs approach enables exploration of the\ndiscrete prompt space while preserving semantic coherence through intelligent\ncrossover and strategic mutation operations that incorporate debate-based\nfeedback, combining elements from both successful and unsuccessful prompts\nbased on identified strengths rather than arbitrary splicing. Using Elo ratings\nas a fitness proxy, DEEVO simultaneously drives improvement and preserves\nvaluable diversity in the prompt population. Experimental results demonstrate\nthat DEEVO significantly outperforms both manual prompt engineering and\nalternative state-of-the-art optimization approaches on open-ended tasks and\nclose-ended tasks despite using no ground truth feedback. By connecting LLMs\nreasoning capabilities with adaptive optimization, DEEVO represents a\nsignificant advancement in prompt optimization research by eliminating the need\nof predetermined metrics to continuously improve AI systems.",
      "authors": [
        "Anirudh Nair",
        "Adi Banerjee",
        "Laurent Mombaerts",
        "Matthew Hagen",
        "Tarik Borogovac"
      ],
      "categories": [
        "cs.AI",
        "cs.NE"
      ],
      "links": [
        "http://arxiv.org/abs/2506.00178v1",
        "http://arxiv.org/pdf/2506.00178v1"
      ],
      "primary_search_term": "artificial intelligence",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2506.00169v1",
      "title": "Utilizing AI for Aviation Post-Accident Analysis Classification",
      "published": "2025-05-30T19:15:04Z",
      "updated": "2025-05-30T19:15:04Z",
      "summary": "The volume of textual data available in aviation safety reports presents a\nchallenge for timely and accurate analysis. This paper examines how Artificial\nIntelligence (AI) and, specifically, Natural Language Processing (NLP) can\nautomate the process of extracting valuable insights from this data, ultimately\nenhancing aviation safety. The paper reviews ongoing efforts focused on the\napplication of NLP and deep learning to aviation safety reports, with the goal\nof classifying the level of damage to an aircraft and identifying the phase of\nflight during which safety occurrences happen. Additionally, the paper explores\nthe use of Topic Modeling (TM) to uncover latent thematic structures within\naviation incident reports, aiming to identify recurring patterns and potential\nareas for safety improvement. The paper compares and contrasts the performance\nof various deep learning models and TM techniques applied to datasets from the\nNational Transportation Safety Board (NTSB) and the Australian Transport Safety\nBureau (ATSB), as well as the Aviation Safety Network (ASN), discussing the\nimpact of dataset size and source on the accuracy of the analysis. The findings\ndemonstrate that both NLP and deep learning, as well as TM, can significantly\nimprove the efficiency and accuracy of aviation safety analysis, paving the way\nfor more proactive safety management and risk mitigation strategies.",
      "authors": [
        "Aziida Nanyonga",
        "Graham Wild"
      ],
      "categories": [
        "cs.AI"
      ],
      "links": [
        "http://arxiv.org/abs/2506.00169v1",
        "http://arxiv.org/pdf/2506.00169v1"
      ],
      "primary_search_term": "artificial intelligence",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2506.00150v1",
      "title": "Supporting architecture evaluation for ATAM scenarios with LLMs",
      "published": "2025-05-30T18:42:12Z",
      "updated": "2025-05-30T18:42:12Z",
      "summary": "Architecture evaluation methods have long been used to evaluate software\ndesigns. Several evaluation methods have been proposed and used to analyze\ntradeoffs between different quality attributes. Having competing qualities\nleads to conflicts for selecting which quality-attribute scenarios are the most\nsuitable ones that an architecture should tackle and for prioritizing the\nscenarios required by the stakeholders. In this context, architecture\nevaluation is carried out manually, often involving long brainstorming sessions\nto decide which are the most adequate quality scenarios. To reduce this effort\nand make the assessment and selection of scenarios more efficient, we suggest\nthe usage of LLMs to partially automate evaluation activities. As a first step\nto validate this hypothesis, this work studies MS Copilot as an LLM tool to\nanalyze quality scenarios suggested by students in a software architecture\ncourse and compares the students' results with the assessment provided by the\nLLM. Our initial study reveals that the LLM produces in most cases better and\nmore accurate results regarding the risks, sensitivity points and tradeoff\nanalysis of the quality scenarios. Overall, the use of generative AI has the\npotential to partially automate and support the architecture evaluation tasks,\nimproving the human decision-making process.",
      "authors": [
        "Rafael Capilla",
        "J. Andr\u00e9s D\u00edaz-Pace",
        "Yamid Ram\u00edrez",
        "Jennifer P\u00e9rez",
        "Vanessa Rodr\u00edguez-Horcajo"
      ],
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "links": [
        "http://arxiv.org/abs/2506.00150v1",
        "http://arxiv.org/pdf/2506.00150v1"
      ],
      "primary_search_term": "artificial intelligence",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2506.00140v2",
      "title": "Balancing Profit and Fairness in Risk-Based Pricing Markets",
      "published": "2025-05-30T18:24:08Z",
      "updated": "2025-06-04T16:06:36Z",
      "summary": "Dynamic, risk-based pricing can systematically exclude vulnerable consumer\ngroups from essential resources such as health insurance and consumer credit.\nWe show that a regulator can realign private incentives with social objectives\nthrough a learned, interpretable tax schedule. First, we provide a formal\nproposition that bounding each firm's \\emph{local} demographic gap implicitly\nbounds the \\emph{global} opt-out disparity, motivating firm-level penalties.\nBuilding on this insight we introduce \\texttt{MarketSim} -- an open-source,\nscalable simulator of heterogeneous consumers and profit-maximizing firms --\nand train a reinforcement learning (RL) social planner (SP) that selects a\nbracketed fairness-tax while remaining close to a simple linear prior via an\n$\\mathcal{L}_1$ regularizer. The learned policy is thus both transparent and\neasily interpretable. In two empirically calibrated markets, i.e., U.S.\nhealth-insurance and consumer-credit, our planner simultaneously raises\ndemand-fairness by up to $16\\%$ relative to unregulated Free Market while\noutperforming a fixed linear schedule in terms of social welfare without\nexplicit coordination. These results illustrate how AI-assisted regulation can\nconvert a competitive social dilemma into a win-win equilibrium, providing a\nprincipled and practical framework for fairness-aware market oversight.",
      "authors": [
        "Jesse Thibodeau",
        "Hadi Nekoei",
        "Afaf Ta\u00efk",
        "Janarthanan Rajendran",
        "Golnoosh Farnadi"
      ],
      "categories": [
        "cs.AI",
        "cs.LG",
        "econ.GN",
        "q-fin.EC"
      ],
      "links": [
        "http://arxiv.org/abs/2506.00140v2",
        "http://arxiv.org/pdf/2506.00140v2"
      ],
      "primary_search_term": "artificial intelligence",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2505.24878v1",
      "title": "Open CaptchaWorld: A Comprehensive Web-based Platform for Testing and\n  Benchmarking Multimodal LLM Agents",
      "published": "2025-05-30T17:59:55Z",
      "updated": "2025-05-30T17:59:55Z",
      "summary": "CAPTCHAs have been a critical bottleneck for deploying web agents in\nreal-world applications, often blocking them from completing end-to-end\nautomation tasks. While modern multimodal LLM agents have demonstrated\nimpressive performance in static perception tasks, their ability to handle\ninteractive, multi-step reasoning challenges like CAPTCHAs is largely untested.\nTo address this gap, we introduce Open CaptchaWorld, the first web-based\nbenchmark and platform specifically designed to evaluate the visual reasoning\nand interaction capabilities of MLLM-powered agents through diverse and dynamic\nCAPTCHA puzzles. Our benchmark spans 20 modern CAPTCHA types, totaling 225\nCAPTCHAs, annotated with a new metric we propose: CAPTCHA Reasoning Depth,\nwhich quantifies the number of cognitive and motor steps required to solve each\npuzzle. Experimental results show that humans consistently achieve near-perfect\nscores, state-of-the-art MLLM agents struggle significantly, with success rates\nat most 40.0% by Browser-Use Openai-o3, far below human-level performance,\n93.3%. This highlights Open CaptchaWorld as a vital benchmark for diagnosing\nthe limits of current multimodal agents and guiding the development of more\nrobust multimodal reasoning systems. Code and Data are available at this https\nURL.",
      "authors": [
        "Yaxin Luo",
        "Zhaoyi Li",
        "Jiacheng Liu",
        "Jiacheng Cui",
        "Xiaohan Zhao",
        "Zhiqiang Shen"
      ],
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CV",
        "cs.LG"
      ],
      "links": [
        "http://arxiv.org/abs/2505.24878v1",
        "http://arxiv.org/pdf/2505.24878v1"
      ],
      "primary_search_term": "artificial intelligence",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2506.02032v1",
      "title": "Towards Secure MLOps: Surveying Attacks, Mitigation Strategies, and\n  Research Challenges",
      "published": "2025-05-30T17:45:31Z",
      "updated": "2025-05-30T17:45:31Z",
      "summary": "The rapid adoption of machine learning (ML) technologies has driven\norganizations across diverse sectors to seek efficient and reliable methods to\naccelerate model development-to-deployment. Machine Learning Operations (MLOps)\nhas emerged as an integrative approach addressing these requirements by\nunifying relevant roles and streamlining ML workflows. As the MLOps market\ncontinues to grow, securing these pipelines has become increasingly critical.\nHowever, the unified nature of MLOps ecosystem introduces vulnerabilities,\nmaking them susceptible to adversarial attacks where a single misconfiguration\ncan lead to compromised credentials, severe financial losses, damaged public\ntrust, and the poisoning of training data. Our paper presents a systematic\napplication of the MITRE ATLAS (Adversarial Threat Landscape for\nArtificial-Intelligence Systems) framework, a comprehensive and continuously\nupdated catalog of AI-focused attacks, to systematically assess attacks across\ndifferent phases of the MLOps ecosystem. We begin by examining the preparatory\nphases during which adversaries acquire the essential intelligence required to\ninitiate their attacks. We then present a structured taxonomy of attack\ntechniques explicitly mapped to corresponding phases of the MLOps ecosystem,\nsupported by examples drawn from red-teaming exercises and real-world\nincidents. This is followed by a taxonomy of mitigation strategies aligned with\nthese attack categories, offering actionable early-stage defenses to strengthen\nthe security of MLOps ecosystem. Given the rapid evolution and adoption of\nMLOps, we further highlight key research gaps that require immediate attention.\nOur work emphasizes the importance of implementing robust security protocols\nfrom the outset, empowering practitioners to safeguard MLOps ecosystem against\nevolving cyber attacks.",
      "authors": [
        "Raj Patel",
        "Himanshu Tripathi",
        "Jasper Stone",
        "Noorbakhsh Amiri Golilarz",
        "Sudip Mittal",
        "Shahram Rahimi",
        "Vini Chaudhary"
      ],
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "links": [
        "http://arxiv.org/abs/2506.02032v1",
        "http://arxiv.org/pdf/2506.02032v1"
      ],
      "primary_search_term": "artificial intelligence",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2505.24838v1",
      "title": "VideoCAD: A Large-Scale Video Dataset for Learning UI Interactions and\n  3D Reasoning from CAD Software",
      "published": "2025-05-30T17:39:52Z",
      "updated": "2025-05-30T17:39:52Z",
      "summary": "Computer-Aided Design (CAD) is a time-consuming and complex process,\nrequiring precise, long-horizon user interactions with intricate 3D interfaces.\nWhile recent advances in AI-driven user interface (UI) agents show promise,\nmost existing datasets and methods focus on short, low-complexity tasks in\nmobile or web applications, failing to capture the demands of professional\nengineering tools. In this work, we introduce VideoCAD, the first attempt at\nengineering UI interaction learning for precision tasks. Specifically, VideoCAD\nis a large-scale synthetic dataset consisting of over 41K annotated video\nrecordings of CAD operations, generated using an automated framework for\ncollecting high-fidelity UI action data from human-made CAD designs. Compared\nto existing datasets, VideoCAD offers an order of magnitude higher complexity\nin UI interaction learning for real-world engineering tasks, having up to a 20x\nlonger time horizon than other datasets. We show two important downstream\napplications of VideoCAD: learning UI interactions from professional precision\n3D CAD tools and a visual question-answering (VQA) benchmark designed to\nevaluate multimodal large language models' (LLM) spatial reasoning and video\nunderstanding abilities. To learn the UI interactions, we propose\nVideoCADFormer - a state-of-the-art model in learning CAD interactions directly\nfrom video, which outperforms multiple behavior cloning baselines. Both\nVideoCADFormer and the VQA benchmark derived from VideoCAD reveal key\nchallenges in the current state of video-based UI understanding, including the\nneed for precise action grounding, multi-modal and spatial reasoning, and\nlong-horizon dependencies.",
      "authors": [
        "Brandon Man",
        "Ghadi Nehme",
        "Md Ferdous Alam",
        "Faez Ahmed"
      ],
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "links": [
        "http://arxiv.org/abs/2505.24838v1",
        "http://arxiv.org/pdf/2505.24838v1"
      ],
      "primary_search_term": "artificial intelligence",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2505.24830v1",
      "title": "Improving Reliability and Explainability of Medical Question Answering\n  through Atomic Fact Checking in Retrieval-Augmented LLMs",
      "published": "2025-05-30T17:33:07Z",
      "updated": "2025-05-30T17:33:07Z",
      "summary": "Large language models (LLMs) exhibit extensive medical knowledge but are\nprone to hallucinations and inaccurate citations, which pose a challenge to\ntheir clinical adoption and regulatory compliance. Current methods, such as\nRetrieval Augmented Generation, partially address these issues by grounding\nanswers in source documents, but hallucinations and low fact-level\nexplainability persist. In this work, we introduce a novel atomic fact-checking\nframework designed to enhance the reliability and explainability of LLMs used\nin medical long-form question answering. This method decomposes LLM-generated\nresponses into discrete, verifiable units called atomic facts, each of which is\nindependently verified against an authoritative knowledge base of medical\nguidelines. This approach enables targeted correction of errors and direct\ntracing to source literature, thereby improving the factual accuracy and\nexplainability of medical Q&A. Extensive evaluation using multi-reader\nassessments by medical experts and an automated open Q&A benchmark demonstrated\nsignificant improvements in factual accuracy and explainability. Our framework\nachieved up to a 40% overall answer improvement and a 50% hallucination\ndetection rate. The ability to trace each atomic fact back to the most relevant\nchunks from the database provides a granular, transparent explanation of the\ngenerated responses, addressing a major gap in current medical AI applications.\nThis work represents a crucial step towards more trustworthy and reliable\nclinical applications of LLMs, addressing key prerequisites for clinical\napplication and fostering greater confidence in AI-assisted healthcare.",
      "authors": [
        "Juraj Vladika",
        "Annika Domres",
        "Mai Nguyen",
        "Rebecca Moser",
        "Jana Nano",
        "Felix Busch",
        "Lisa C. Adams",
        "Keno K. Bressem",
        "Denise Bernhardt",
        "Stephanie E. Combs",
        "Kai J. Borm",
        "Florian Matthes",
        "Jan C. Peeken"
      ],
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "links": [
        "http://arxiv.org/abs/2505.24830v1",
        "http://arxiv.org/pdf/2505.24830v1"
      ],
      "primary_search_term": "artificial intelligence",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2505.24785v2",
      "title": "EXP-Bench: Can AI Conduct AI Research Experiments?",
      "published": "2025-05-30T16:46:29Z",
      "updated": "2025-06-02T01:59:50Z",
      "summary": "Automating AI research holds immense potential for accelerating scientific\nprogress, yet current AI agents struggle with the complexities of rigorous,\nend-to-end experimentation. We introduce EXP-Bench, a novel benchmark designed\nto systematically evaluate AI agents on complete research experiments sourced\nfrom influential AI publications. Given a research question and incomplete\nstarter code, EXP-Bench challenges AI agents to formulate hypotheses, design\nand implement experimental procedures, execute them, and analyze results. To\nenable the creation of such intricate and authentic tasks with high-fidelity,\nwe design a semi-autonomous pipeline to extract and structure crucial\nexperimental details from these research papers and their associated\nopen-source code. With the pipeline, EXP-Bench curated 461 AI research tasks\nfrom 51 top-tier AI research papers. Evaluations of leading LLM-based agents,\nsuch as OpenHands and IterativeAgent on EXP-Bench demonstrate partial\ncapabilities: while scores on individual experimental aspects such as design or\nimplementation correctness occasionally reach 20-35%, the success rate for\ncomplete, executable experiments was a mere 0.5%. By identifying these\nbottlenecks and providing realistic step-by-step experiment procedures,\nEXP-Bench serves as a vital tool for future AI agents to improve their ability\nto conduct AI research experiments. EXP-Bench is open-sourced at\nhttps://github.com/Just-Curieous/Curie/tree/main/benchmark/exp_bench.",
      "authors": [
        "Patrick Tser Jern Kon",
        "Jiachen Liu",
        "Xinyi Zhu",
        "Qiuyi Ding",
        "Jingjia Peng",
        "Jiarong Xing",
        "Yibo Huang",
        "Yiming Qiu",
        "Jayanth Srinivasa",
        "Myungjin Lee",
        "Mosharaf Chowdhury",
        "Matei Zaharia",
        "Ang Chen"
      ],
      "categories": [
        "cs.AI"
      ],
      "links": [
        "http://arxiv.org/abs/2505.24785v2",
        "http://arxiv.org/pdf/2505.24785v2"
      ],
      "primary_search_term": "artificial intelligence",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2505.24716v1",
      "title": "Towards Scalable Schema Mapping using Large Language Models",
      "published": "2025-05-30T15:36:56Z",
      "updated": "2025-05-30T15:36:56Z",
      "summary": "The growing need to integrate information from a large number of diverse\nsources poses significant scalability challenges for data integration systems.\nThese systems often rely on manually written schema mappings, which are\ncomplex, source-specific, and costly to maintain as sources evolve. While\nrecent advances suggest that large language models (LLMs) can assist in\nautomating schema matching by leveraging both structural and natural language\ncues, key challenges remain. In this paper, we identify three core issues with\nusing LLMs for schema mapping: (1) inconsistent outputs due to sensitivity to\ninput phrasing and structure, which we propose methods to address through\nsampling and aggregation techniques; (2) the need for more expressive mappings\n(e.g., GLaV), which strain the limited context windows of LLMs; and (3) the\ncomputational cost of repeated LLM calls, which we propose to mitigate through\nstrategies like data type prefiltering.",
      "authors": [
        "Christopher Buss",
        "Mahdis Safari",
        "Arash Termehchy",
        "Stefan Lee",
        "David Maier"
      ],
      "categories": [
        "cs.DB",
        "cs.AI"
      ],
      "links": [
        "http://arxiv.org/abs/2505.24716v1",
        "http://arxiv.org/pdf/2505.24716v1"
      ],
      "primary_search_term": "artificial intelligence",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2505.24681v1",
      "title": "Generative Knowledge Production Pipeline Driven by Academic Influencers",
      "published": "2025-05-30T15:07:01Z",
      "updated": "2025-05-30T15:07:01Z",
      "summary": "Generative AI transforms knowledge production, validation, and dissemination,\nraising academic integrity and credibility concerns. This study examines 53\nacademic influencer videos that reached 5.3 million viewers to identify an\nemerging, structured, implementation-ready pipeline balancing originality,\nethical compliance, and human-AI collaboration despite the disruptive impacts.\nFindings highlight generative AI's potential to automate publication workflows\nand democratize participation in knowledge production while challenging\ntraditional scientific norms. Academic influencers emerge as key intermediaries\nin this paradigm shift, connecting bottom-up practices with institutional\npolicies to improve adaptability. Accordingly, the study proposes a generative\npublication production pipeline and a policy framework for co-intelligence\nadaptation and reinforcing credibility-centered standards in AI-powered\nresearch. These insights support scholars, educators, and policymakers in\nunderstanding AI's transformative impact by advocating responsible and\ninnovation-driven knowledge production. Additionally, they reveal pathways for\nautomating best practices, optimizing scholarly workflows, and fostering\ncreativity in academic research and publication.",
      "authors": [
        "Katalin Feher",
        "Marton Demeter"
      ],
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.HC",
        "cs.SI",
        "1.2, J.4, K.4"
      ],
      "links": [
        "http://arxiv.org/abs/2505.24681v1",
        "http://arxiv.org/pdf/2505.24681v1"
      ],
      "primary_search_term": "artificial intelligence",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2506.08026v2",
      "title": "TIP-Search: Time-Predictable Inference Scheduling for Market Prediction\n  under Uncertain Load",
      "published": "2025-05-30T14:52:01Z",
      "updated": "2025-06-16T19:58:59Z",
      "summary": "This paper proposes TIP-Search, a time-predictable inference scheduling\nframework for real-time market prediction under uncertain workloads. Motivated\nby the strict latency demands in high-frequency financial systems, TIP-Search\ndynamically selects a deep learning model from a heterogeneous pool, aiming to\nmaximize predictive accuracy while satisfying per-task deadline constraints.\nOur approach profiles latency and generalization performance offline, then\nperforms online task-aware selection without relying on explicit input domain\nlabels. We evaluate TIP-Search on three real-world limit order book datasets\n(FI-2010, Binance BTC/USDT, LOBSTER AAPL) and demonstrate that it outperforms\nstatic baselines with up to 8.5% improvement in accuracy and 100% deadline\nsatisfaction. Our results highlight the effectiveness of TIP-Search in robust\nlow-latency financial inference under uncertainty.",
      "authors": [
        "Xibai Wang"
      ],
      "categories": [
        "cs.AI",
        "cs.LG",
        "cs.SY",
        "eess.SY",
        "q-fin.CP"
      ],
      "links": [
        "http://arxiv.org/abs/2506.08026v2",
        "http://arxiv.org/pdf/2506.08026v2"
      ],
      "primary_search_term": "artificial intelligence",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2505.24640v1",
      "title": "Efficient Text Encoders for Labor Market Analysis",
      "published": "2025-05-30T14:27:25Z",
      "updated": "2025-05-30T14:27:25Z",
      "summary": "Labor market analysis relies on extracting insights from job advertisements,\nwhich provide valuable yet unstructured information on job titles and\ncorresponding skill requirements. While state-of-the-art methods for skill\nextraction achieve strong performance, they depend on large language models\n(LLMs), which are computationally expensive and slow. In this paper, we propose\n\\textbf{ConTeXT-match}, a novel contrastive learning approach with token-level\nattention that is well-suited for the extreme multi-label classification task\nof skill classification. \\textbf{ConTeXT-match} significantly improves skill\nextraction efficiency and performance, achieving state-of-the-art results with\na lightweight bi-encoder model. To support robust evaluation, we introduce\n\\textbf{Skill-XL}, a new benchmark with exhaustive, sentence-level skill\nannotations that explicitly address the redundancy in the large label space.\nFinally, we present \\textbf{JobBERT V2}, an improved job title normalization\nmodel that leverages extracted skills to produce high-quality job title\nrepresentations. Experiments demonstrate that our models are efficient,\naccurate, and scalable, making them ideal for large-scale, real-time labor\nmarket analysis.",
      "authors": [
        "Jens-Joris Decorte",
        "Jeroen Van Hautte",
        "Chris Develder",
        "Thomas Demeester"
      ],
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "links": [
        "http://arxiv.org/abs/2505.24640v1",
        "http://arxiv.org/pdf/2505.24640v1"
      ],
      "primary_search_term": "artificial intelligence",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2505.24630v1",
      "title": "The Hallucination Dilemma: Factuality-Aware Reinforcement Learning for\n  Large Reasoning Models",
      "published": "2025-05-30T14:23:32Z",
      "updated": "2025-05-30T14:23:32Z",
      "summary": "Large language models (LLMs) have significantly advanced in reasoning tasks\nthrough reinforcement learning (RL) optimization, achieving impressive\ncapabilities across various challenging benchmarks. However, our empirical\nanalysis reveals a critical drawback: reasoning-oriented RL fine-tuning\nsignificantly increases the prevalence of hallucinations. We theoretically\nanalyze the RL training dynamics, identifying high-variance gradient,\nentropy-induced randomness, and susceptibility to spurious local optima as key\nfactors leading to hallucinations. To address this drawback, we propose\nFactuality-aware Step-wise Policy Optimization (FSPO), an innovative RL\nfine-tuning algorithm incorporating explicit factuality verification at each\nreasoning step. FSPO leverages automated verification against given evidence to\ndynamically adjust token-level advantage values, incentivizing factual\ncorrectness throughout the reasoning process. Experiments across mathematical\nreasoning and hallucination benchmarks using Qwen2.5 and Llama models\ndemonstrate that FSPO effectively reduces hallucinations while enhancing\nreasoning accuracy, substantially improving both reliability and performance.",
      "authors": [
        "Junyi Li",
        "Hwee Tou Ng"
      ],
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "links": [
        "http://arxiv.org/abs/2505.24630v1",
        "http://arxiv.org/pdf/2505.24630v1"
      ],
      "primary_search_term": "artificial intelligence",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2505.24584v2",
      "title": "AutoChemSchematic AI: A Closed-Loop, Physics-Aware Agentic Framework for\n  Auto-Generating Chemical Process and Instrumentation Diagrams",
      "published": "2025-05-30T13:32:00Z",
      "updated": "2025-06-02T01:08:24Z",
      "summary": "Recent advancements in generative AI have accelerated the discovery of novel\nchemicals and materials; however, transitioning these discoveries to\nindustrial-scale production remains a critical bottleneck, as it requires the\ndevelopment of entirely new chemical manufacturing processes. Current AI\nmethods cannot auto-generate PFDs or PIDs, despite their critical role in\nscaling chemical processes, while adhering to engineering constraints. We\npresent a closed loop, physics aware framework for the automated generation of\nindustrially viable PFDs and PIDs. The framework integrates domain specialized\nsmall scale language models (SLMs) (trained for chemical process QA tasks) with\nfirst principles simulation, leveraging three key components: (1) a\nhierarchical knowledge graph of process flow and instrumentation descriptions\nfor 1,020+ chemicals, (2) a multi-stage training pipeline that fine tunes\ndomain specialized SLMs on synthetic datasets via Supervised Fine-Tuning (SFT),\nDirect Preference Optimization (DPO), and Retrieval-Augmented Instruction\nTuning (RAIT), and (3) DWSIM based simulator in the loop validation to ensure\nfeasibility. To improve both runtime efficiency and model compactness, the\nframework incorporates advanced inference time optimizations including\nFlashAttention, Lookahead Decoding, PagedAttention with KV-cache quantization,\nand Test Time Inference Scaling and independently applies structural pruning\ntechniques (width and depth) guided by importance heuristics to reduce model\nsize with minimal accuracy loss. Experiments demonstrate that the framework\ngenerates simulator-validated process descriptions with high fidelity,\noutperforms baseline methods in correctness, and generalizes to unseen\nchemicals. By bridging AI-driven design with industrial-scale feasibility, this\nwork significantly reduces R&D timelines from lab discovery to plant\ndeployment.",
      "authors": [
        "Sakhinana Sagar Srinivas",
        "Shivam Gupta",
        "Venkataramana Runkana"
      ],
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.IR"
      ],
      "links": [
        "http://arxiv.org/abs/2505.24584v2",
        "http://arxiv.org/pdf/2505.24584v2"
      ],
      "primary_search_term": "artificial intelligence",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2505.24554v2",
      "title": "Bench4KE: Benchmarking Automated Competency Question Generation",
      "published": "2025-05-30T13:03:42Z",
      "updated": "2025-06-04T09:08:05Z",
      "summary": "The availability of Large Language Models (LLMs) presents a unique\nopportunity to reinvigorate research on Knowledge Engineering (KE) automation,\na trend already evident in recent efforts developing LLM-based methods and\ntools for the automatic generation of Competency Questions (CQs). However, the\nevaluation of these tools lacks standardisation. This undermines the\nmethodological rigour and hinders the replication and comparison of results. To\naddress this gap, we introduce Bench4KE, an extensible API-based benchmarking\nsystem for KE automation. Its first release focuses on evaluating tools that\ngenerate CQs automatically. CQs are natural language questions used by ontology\nengineers to define the functional requirements of an ontology. Bench4KE\nprovides a curated gold standard consisting of CQ datasets from four real-world\nontology projects. It uses a suite of similarity metrics to assess the quality\nof the CQs generated. We present a comparative analysis of four recent CQ\ngeneration systems, which are based on LLMs, establishing a baseline for future\nresearch. Bench4KE is also designed to accommodate additional KE automation\ntasks, such as SPARQL query generation, ontology testing and drafting. Code and\ndatasets are publicly available under the Apache 2.0 license.",
      "authors": [
        "Anna Sofia Lippolis",
        "Minh Davide Ragagni",
        "Paolo Ciancarini",
        "Andrea Giovanni Nuzzolese",
        "Valentina Presutti"
      ],
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "links": [
        "http://arxiv.org/abs/2505.24554v2",
        "http://arxiv.org/pdf/2505.24554v2"
      ],
      "primary_search_term": "artificial intelligence",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2506.00099v2",
      "title": "Finance as Extended Biology: Reciprocity as the Cognitive Substrate of\n  Financial Behavior",
      "published": "2025-05-30T12:37:52Z",
      "updated": "2025-06-06T11:37:02Z",
      "summary": "A central challenge in economics and artificial intelligence is explaining\nhow financial behaviors-such as credit, insurance, and trade-emerge without\nformal institutions. We argue that these functions are not products of\ninstitutional design, but structured extensions of a single behavioral\nsubstrate: reciprocity. Far from being a derived strategy, reciprocity served\nas the foundational logic of early human societies-governing the circulation of\ngoods, regulation of obligation, and maintenance of long-term cooperation well\nbefore markets, money, or formal rules. Trade, commonly regarded as the origin\nof financial systems, is reframed here as the canonical form of reciprocity:\nsimultaneous, symmetric, and partner-contingent. Building on this logic, we\nreconstruct four core financial functions-credit, insurance, token exchange,\nand investment-as expressions of the same underlying principle under varying\nconditions. By grounding financial behavior in minimal, simulateable dynamics\nof reciprocal interaction, this framework shifts the focus from institutional\nengineering to behavioral computation-offering a new foundation for modeling\ndecentralized financial behavior in both human and artificial agents.",
      "authors": [
        "Egil Diau"
      ],
      "categories": [
        "physics.soc-ph"
      ],
      "links": [
        "http://arxiv.org/abs/2506.00099v2",
        "http://arxiv.org/pdf/2506.00099v2"
      ],
      "primary_search_term": "artificial intelligence",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2506.03185v1",
      "title": "DLiPath: A Benchmark for the Comprehensive Assessment of Donor Liver\n  Based on Histopathological Image Dataset",
      "published": "2025-05-30T12:13:00Z",
      "updated": "2025-05-30T12:13:00Z",
      "summary": "Pathologists comprehensive evaluation of donor liver biopsies provides\ncrucial information for accepting or discarding potential grafts. However,\nrapidly and accurately obtaining these assessments intraoperatively poses a\nsignificant challenge for pathologists. Features in donor liver biopsies, such\nas portal tract fibrosis, total steatosis, macrovesicular steatosis, and\nhepatocellular ballooning are correlated with transplant outcomes, yet\nquantifying these indicators suffers from substantial inter- and intra-observer\nvariability. To address this, we introduce DLiPath, the first benchmark for\ncomprehensive donor liver assessment based on a histopathology image dataset.\nWe collected and publicly released 636 whole slide images from 304 donor liver\npatients at the Department of Pathology, the Third Xiangya Hospital, with\nexpert annotations for key pathological features (including cholestasis, portal\ntract fibrosis, portal inflammation, total steatosis, macrovesicular steatosis,\nand hepatocellular ballooning). We selected nine state-of-the-art\nmultiple-instance learning (MIL) models based on the DLiPath dataset as\nbaselines for extensive comparative analysis. The experimental results\ndemonstrate that several MIL models achieve high accuracy across donor liver\nassessment indicators on DLiPath, charting a clear course for future automated\nand intelligent donor liver assessment research. Data and code are available at\nhttps://github.com/panliangrui/ACM_MM_2025.",
      "authors": [
        "Liangrui Pan",
        "Xingchen Li",
        "Zhongyi Chen",
        "Ling Chu",
        "Shaoliang Peng"
      ],
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV",
        "q-bio.QM"
      ],
      "links": [
        "http://arxiv.org/abs/2506.03185v1",
        "http://arxiv.org/pdf/2506.03185v1"
      ],
      "primary_search_term": "artificial intelligence",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2505.24493v1",
      "title": "MELT: Towards Automated Multimodal Emotion Data Annotation by Leveraging\n  LLM Embedded Knowledge",
      "published": "2025-05-30T11:45:36Z",
      "updated": "2025-05-30T11:45:36Z",
      "summary": "Although speech emotion recognition (SER) has advanced significantly with\ndeep learning, annotation remains a major hurdle. Human annotation is not only\ncostly but also subject to inconsistencies annotators often have different\npreferences and may lack the necessary contextual knowledge, which can lead to\nvaried and inaccurate labels. Meanwhile, Large Language Models (LLMs) have\nemerged as a scalable alternative for annotating text data. However, the\npotential of LLMs to perform emotional speech data annotation without human\nsupervision has yet to be thoroughly investigated. To address these problems,\nwe apply GPT-4o to annotate a multimodal dataset collected from the sitcom\nFriends, using only textual cues as inputs. By crafting structured text\nprompts, our methodology capitalizes on the knowledge GPT-4o has accumulated\nduring its training, showcasing that it can generate accurate and contextually\nrelevant annotations without direct access to multimodal inputs. Therefore, we\npropose MELT, a multimodal emotion dataset fully annotated by GPT-4o. We\ndemonstrate the effectiveness of MELT by fine-tuning four self-supervised\nlearning (SSL) backbones and assessing speech emotion recognition performance\nacross emotion datasets. Additionally, our subjective experiments\\' results\ndemonstrate a consistence performance improvement on SER.",
      "authors": [
        "Xin Jing",
        "Jiadong Wang",
        "Iosif Tsangko",
        "Andreas Triantafyllopoulos",
        "Bj\u00f6rn W. Schuller"
      ],
      "categories": [
        "cs.AI",
        "cs.SD",
        "eess.AS"
      ],
      "links": [
        "http://arxiv.org/abs/2505.24493v1",
        "http://arxiv.org/pdf/2505.24493v1"
      ],
      "primary_search_term": "artificial intelligence",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2505.24415v1",
      "title": "Boosting Automatic Exercise Evaluation Through Musculoskeletal\n  Simulation-Based IMU Data Augmentation",
      "published": "2025-05-30T09:53:37Z",
      "updated": "2025-05-30T09:53:37Z",
      "summary": "Automated evaluation of movement quality holds significant potential for\nenhancing physiotherapeutic treatments and sports training by providing\nobjective, real-time feedback. However, the effectiveness of deep learning\nmodels in assessing movements captured by inertial measurement units (IMUs) is\noften hampered by limited data availability, class imbalance, and label\nambiguity. In this work, we present a novel data augmentation method that\ngenerates realistic IMU data using musculoskeletal simulations integrated with\nsystematic modifications of movement trajectories. Crucially, our approach\nensures biomechanical plausibility and allows for automatic, reliable labeling\nby combining inverse kinematic parameters with a knowledge-based evaluation\nstrategy. Extensive evaluations demonstrate that augmented variants closely\nresembles real-world data, significantly improving the classification accuracy\nand generalization capability of neural network models. Additionally, we\nhighlight the benefits of augmented data for patient-specific fine-tuning\nscenarios, particularly when only limited subject-specific training examples\nare available. Our findings underline the practicality and efficacy of this\naugmentation method in overcoming common challenges faced by deep learning\napplications in physiotherapeutic exercise evaluation.",
      "authors": [
        "Andreas Spilz",
        "Heiko Oppel",
        "Michael Munz"
      ],
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "links": [
        "http://arxiv.org/abs/2505.24415v1",
        "http://arxiv.org/pdf/2505.24415v1"
      ],
      "primary_search_term": "artificial intelligence",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2505.24369v1",
      "title": "Adversarial Preference Learning for Robust LLM Alignment",
      "published": "2025-05-30T09:02:07Z",
      "updated": "2025-05-30T09:02:07Z",
      "summary": "Modern language models often rely on Reinforcement Learning from Human\nFeedback (RLHF) to encourage safe behaviors. However, they remain vulnerable to\nadversarial attacks due to three key limitations: (1) the inefficiency and high\ncost of human annotation, (2) the vast diversity of potential adversarial\nattacks, and (3) the risk of feedback bias and reward hacking. To address these\nchallenges, we introduce Adversarial Preference Learning (APL), an iterative\nadversarial training method incorporating three key innovations. First, a\ndirect harmfulness metric based on the model's intrinsic preference\nprobabilities, eliminating reliance on external assessment. Second, a\nconditional generative attacker that synthesizes input-specific adversarial\nvariations. Third, an iterative framework with automated closed-loop feedback,\nenabling continuous adaptation through vulnerability discovery and mitigation.\nExperiments on Mistral-7B-Instruct-v0.3 demonstrate that APL significantly\nenhances robustness, achieving 83.33% harmlessness win rate over the base model\n(evaluated by GPT-4o), reducing harmful outputs from 5.88% to 0.43% (measured\nby LLaMA-Guard), and lowering attack success rate by up to 65% according to\nHarmBench. Notably, APL maintains competitive utility, with an MT-Bench score\nof 6.59 (comparable to the baseline 6.78) and an LC-WinRate of 46.52% against\nthe base model.",
      "authors": [
        "Yuanfu Wang",
        "Pengyu Wang",
        "Chenyang Xi",
        "Bo Tang",
        "Junyi Zhu",
        "Wenqiang Wei",
        "Chen Chen",
        "Chao Yang",
        "Jingfeng Zhang",
        "Chaochao Lu",
        "Yijun Niu",
        "Keming Mao",
        "Zhiyu Li",
        "Feiyu Xiong",
        "Jie Hu",
        "Mingchuan Yang"
      ],
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "links": [
        "http://arxiv.org/abs/2505.24369v1",
        "http://arxiv.org/pdf/2505.24369v1"
      ],
      "primary_search_term": "artificial intelligence",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2505.24258v1",
      "title": "FABLE: A Novel Data-Flow Analysis Benchmark on Procedural Text for Large\n  Language Model Evaluation",
      "published": "2025-05-30T06:32:34Z",
      "updated": "2025-05-30T06:32:34Z",
      "summary": "Understanding how data moves, transforms, and persists, known as data flow,\nis fundamental to reasoning in procedural tasks. Despite their fluency in\nnatural and programming languages, large language models (LLMs), although\nincreasingly being applied to decisions with procedural tasks, have not been\nsystematically evaluated for their ability to perform data-flow reasoning. We\nintroduce FABLE, an extensible benchmark designed to assess LLMs' understanding\nof data flow using structured, procedural text. FABLE adapts eight classical\ndata-flow analyses from software engineering: reaching definitions, very busy\nexpressions, available expressions, live variable analysis, interval analysis,\ntype-state analysis, taint analysis, and concurrency analysis. These analyses\nare instantiated across three real-world domains: cooking recipes, travel\nroutes, and automated plans. The benchmark includes 2,400 question-answer\npairs, with 100 examples for each domain-analysis combination. We evaluate\nthree types of LLMs: a reasoning-focused model (DeepSeek-R1 8B), a\ngeneral-purpose model (LLaMA 3.1 8B), and a code-specific model (Granite Code\n8B). Each model is tested using majority voting over five sampled completions\nper prompt. Results show that the reasoning model achieves higher accuracy, but\nat the cost of over 20 times slower inference compared to the other models. In\ncontrast, the general-purpose and code-specific models perform close to random\nchance. FABLE provides the first diagnostic benchmark to systematically\nevaluate data-flow reasoning and offers insights for developing models with\nstronger procedural understanding.",
      "authors": [
        "Vishal Pallagani",
        "Nitin Gupta",
        "John Aydin",
        "Biplav Srivastava"
      ],
      "categories": [
        "cs.AI"
      ],
      "links": [
        "http://arxiv.org/abs/2505.24258v1",
        "http://arxiv.org/pdf/2505.24258v1"
      ],
      "primary_search_term": "artificial intelligence",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2505.24252v1",
      "title": "A Reward-driven Automated Webshell Malicious-code Generator for\n  Red-teaming",
      "published": "2025-05-30T06:16:42Z",
      "updated": "2025-05-30T06:16:42Z",
      "summary": "Frequent cyber-attacks have elevated WebShell exploitation and defense to a\ncritical research focus within network security. However, there remains a\nsignificant shortage of publicly available, well-categorized malicious-code\ndatasets organized by obfuscation method. Existing malicious-code generation\nmethods, which primarily rely on prompt engineering, often suffer from limited\ndiversity and high redundancy in the payloads they produce. To address these\nlimitations, we propose \\textbf{RAWG}, a \\textbf{R}eward-driven\n\\textbf{A}utomated \\textbf{W}ebshell Malicious-code \\textbf{G}enerator designed\nfor red-teaming applications. Our approach begins by categorizing webshell\nsamples from common datasets into seven distinct types of obfuscation. We then\nemploy a large language model (LLM) to extract and normalize key tokens from\neach sample, creating a standardized, high-quality corpus. Using this curated\ndataset, we perform supervised fine-tuning (SFT) on an open-source large model\nto enable the generation of diverse, highly obfuscated webshell malicious\npayloads. To further enhance generation quality, we apply Proximal Policy\nOptimization (PPO), treating malicious-code samples as \"chosen\" data and benign\ncode as \"rejected\" data during reinforcement learning. Extensive experiments\ndemonstrate that RAWG significantly outperforms current state-of-the-art\nmethods in both payload diversity and escape effectiveness.",
      "authors": [
        "Yizhong Ding"
      ],
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "links": [
        "http://arxiv.org/abs/2505.24252v1",
        "http://arxiv.org/pdf/2505.24252v1"
      ],
      "primary_search_term": "artificial intelligence",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2505.24230v1",
      "title": "ProofNet++: A Neuro-Symbolic System for Formal Proof Verification with\n  Self-Correction",
      "published": "2025-05-30T05:44:34Z",
      "updated": "2025-05-30T05:44:34Z",
      "summary": "We propose ProofNet++, a neuro-symbolic framework that enhances automated\ntheorem proving by combining large language models (LLMs) with formal proof\nverification and self-correction mechanisms. Current LLM-based systems suffer\nfrom hallucinated logical steps and unverifiable reasoning. ProofNet++\nmitigates these limitations by integrating symbolic proof tree supervision, a\nreinforcement learning loop using verifiers as reward functions, and an\niterative self-correction module. Our experiments on miniF2F, Lean's mathlib,\nand HOL Light show that ProofNet++ significantly improves proof accuracy,\ncorrectness, and formal verifiability over prior models. We provide theoretical\nanalysis of the convergence and stability of the verifier-guided RL framework\nand release our datasets and codebase for future research.",
      "authors": [
        "Murari Ambati"
      ],
      "categories": [
        "cs.AI"
      ],
      "links": [
        "http://arxiv.org/abs/2505.24230v1",
        "http://arxiv.org/pdf/2505.24230v1"
      ],
      "primary_search_term": "artificial intelligence",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2506.00085v1",
      "title": "COSMIC: Generalized Refusal Direction Identification in LLM Activations",
      "published": "2025-05-30T04:54:18Z",
      "updated": "2025-05-30T04:54:18Z",
      "summary": "Large Language Models (LLMs) encode behaviors such as refusal within their\nactivation space, yet identifying these behaviors remains a significant\nchallenge. Existing methods often rely on predefined refusal templates\ndetectable in output tokens or require manual analysis. We introduce\n\\textbf{COSMIC} (Cosine Similarity Metrics for Inversion of Concepts), an\nautomated framework for direction selection that identifies viable steering\ndirections and target layers using cosine similarity - entirely independent of\nmodel outputs. COSMIC achieves steering performance comparable to prior methods\nwithout requiring assumptions about a model's refusal behavior, such as the\npresence of specific refusal tokens. It reliably identifies refusal directions\nin adversarial settings and weakly aligned models, and is capable of steering\nsuch models toward safer behavior with minimal increase in false refusals,\ndemonstrating robustness across a wide range of alignment conditions.",
      "authors": [
        "Vincent Siu",
        "Nicholas Crispino",
        "Zihao Yu",
        "Sam Pan",
        "Zhun Wang",
        "Yang Liu",
        "Dawn Song",
        "Chenguang Wang"
      ],
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "links": [
        "http://arxiv.org/abs/2506.00085v1",
        "http://arxiv.org/pdf/2506.00085v1"
      ],
      "primary_search_term": "artificial intelligence",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2505.24138v1",
      "title": "AMSbench: A Comprehensive Benchmark for Evaluating MLLM Capabilities in\n  AMS Circuits",
      "published": "2025-05-30T02:17:45Z",
      "updated": "2025-05-30T02:17:45Z",
      "summary": "Analog/Mixed-Signal (AMS) circuits play a critical role in the integrated\ncircuit (IC) industry. However, automating Analog/Mixed-Signal (AMS) circuit\ndesign has remained a longstanding challenge due to its difficulty and\ncomplexity. Recent advances in Multi-modal Large Language Models (MLLMs) offer\npromising potential for supporting AMS circuit analysis and design. However,\ncurrent research typically evaluates MLLMs on isolated tasks within the domain,\nlacking a comprehensive benchmark that systematically assesses model\ncapabilities across diverse AMS-related challenges. To address this gap, we\nintroduce AMSbench, a benchmark suite designed to evaluate MLLM performance\nacross critical tasks including circuit schematic perception, circuit analysis,\nand circuit design. AMSbench comprises approximately 8000 test questions\nspanning multiple difficulty levels and assesses eight prominent models,\nencompassing both open-source and proprietary solutions such as Qwen 2.5-VL and\nGemini 2.5 Pro. Our evaluation highlights significant limitations in current\nMLLMs, particularly in complex multi-modal reasoning and sophisticated circuit\ndesign tasks. These results underscore the necessity of advancing MLLMs'\nunderstanding and effective application of circuit-specific knowledge, thereby\nnarrowing the existing performance gap relative to human expertise and moving\ntoward fully automated AMS circuit design workflows. Our data is released at\nhttps://huggingface.co/datasets/wwhhyy/AMSBench",
      "authors": [
        "Yichen Shi",
        "Ze Zhang",
        "Hongyang Wang",
        "Zhuofu Tao",
        "Zhongyi Li",
        "Bingyu Chen",
        "Yaxin Wang",
        "Zhiping Yu",
        "Ting-Jung Lin",
        "Lei He"
      ],
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "links": [
        "http://arxiv.org/abs/2505.24138v1",
        "http://arxiv.org/pdf/2505.24138v1"
      ],
      "primary_search_term": "artificial intelligence",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2505.24090v1",
      "title": "Searching Clinical Data Using Generative AI",
      "published": "2025-05-30T00:33:51Z",
      "updated": "2025-05-30T00:33:51Z",
      "summary": "Artificial Intelligence (AI) is making a major impact on healthcare,\nparticularly through its application in natural language processing (NLP) and\npredictive analytics. The healthcare sector has increasingly adopted AI for\ntasks such as clinical data analysis and medical code assignment. However,\nsearching for clinical information in large and often unorganized datasets\nremains a manual and error-prone process. Assisting this process with\nautomations can help physicians improve their operational productivity\nsignificantly.\n  In this paper, we present a generative AI approach, coined SearchAI, to\nenhance the accuracy and efficiency of searching clinical data. Unlike\ntraditional code assignment, which is a one-to-one problem, clinical data\nsearch is a one-to-many problem, i.e., a given search query can map to a family\nof codes. Healthcare professionals typically search for groups of related\ndiseases, drugs, or conditions that map to many codes, and therefore, they need\nsearch tools that can handle keyword synonyms, semantic variants, and broad\nopen-ended queries. SearchAI employs a hierarchical model that respects the\ncoding hierarchy and improves the traversal of relationships from parent to\nchild nodes. SearchAI navigates these hierarchies predictively and ensures that\nall paths are reachable without losing any relevant nodes.\n  To evaluate the effectiveness of SearchAI, we conducted a series of\nexperiments using both public and production datasets. Our results show that\nSearchAI outperforms default hierarchical traversals across several metrics,\nincluding accuracy, robustness, performance, and scalability. SearchAI can help\nmake clinical data more accessible, leading to streamlined workflows, reduced\nadministrative burden, and enhanced coding and diagnostic accuracy.",
      "authors": [
        "Karan Hanswadkar",
        "Anika Kanchi",
        "Shivani Tripathi",
        "Shi Qiao",
        "Rony Chatterjee",
        "Alekh Jindal"
      ],
      "categories": [
        "cs.DB",
        "cs.AI"
      ],
      "links": [
        "http://arxiv.org/abs/2505.24090v1",
        "http://arxiv.org/pdf/2505.24090v1"
      ],
      "primary_search_term": "artificial intelligence",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    }
  ]
}