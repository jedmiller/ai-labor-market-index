{
  "query": "all:machine learning AND (labor market OR employment OR jobs OR workforce OR automation)",
  "date_collected": "2025-06-08T03:20:52.356078",
  "target_period": "2025-05",
  "papers": [
    {
      "id": "http://arxiv.org/abs/2506.00742v1",
      "title": "ArtiScene: Language-Driven Artistic 3D Scene Generation Through Image\n  Intermediary",
      "published": "2025-05-31T23:03:54Z",
      "updated": "2025-05-31T23:03:54Z",
      "summary": "Designing 3D scenes is traditionally a challenging task that demands both\nartistic expertise and proficiency with complex software. Recent advances in\ntext-to-3D generation have greatly simplified this process by letting users\ncreate scenes based on simple text descriptions. However, as these methods\ngenerally require extra training or in-context learning, their performance is\noften hindered by the limited availability of high-quality 3D data. In\ncontrast, modern text-to-image models learned from web-scale images can\ngenerate scenes with diverse, reliable spatial layouts and consistent, visually\nappealing styles. Our key insight is that instead of learning directly from 3D\nscenes, we can leverage generated 2D images as an intermediary to guide 3D\nsynthesis. In light of this, we introduce ArtiScene, a training-free automated\npipeline for scene design that integrates the flexibility of free-form\ntext-to-image generation with the diversity and reliability of 2D intermediary\nlayouts.\n  First, we generate 2D images from a scene description, then extract the shape\nand appearance of objects to create 3D models. These models are assembled into\nthe final scene using geometry, position, and pose information derived from the\nsame intermediary image. Being generalizable to a wide range of scenes and\nstyles, ArtiScene outperforms state-of-the-art benchmarks by a large margin in\nlayout and aesthetic quality by quantitative metrics. It also averages a 74.89%\nwinning rate in extensive user studies and 95.07% in GPT-4o evaluation. Project\npage: https://artiscene-cvpr.github.io/",
      "authors": [
        "Zeqi Gu",
        "Yin Cui",
        "Zhaoshuo Li",
        "Fangyin Wei",
        "Yunhao Ge",
        "Jinwei Gu",
        "Ming-Yu Liu",
        "Abe Davis",
        "Yifan Ding"
      ],
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "links": [
        "http://arxiv.org/abs/2506.00742v1",
        "http://arxiv.org/pdf/2506.00742v1"
      ],
      "primary_search_term": "machine learning",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2506.00694v2",
      "title": "Measuring Faithfulness and Abstention: An Automated Pipeline for\n  Evaluating LLM-Generated 3-ply Case-Based Legal Arguments",
      "published": "2025-05-31T19:56:40Z",
      "updated": "2025-06-03T03:22:48Z",
      "summary": "Large Language Models (LLMs) demonstrate potential in complex legal tasks\nlike argument generation, yet their reliability remains a concern. Building\nupon pilot work assessing LLM generation of 3-ply legal arguments using human\nevaluation, this paper introduces an automated pipeline to evaluate LLM\nperformance on this task, specifically focusing on faithfulness (absence of\nhallucination), factor utilization, and appropriate abstention. We define\nhallucination as the generation of factors not present in the input case\nmaterials and abstention as the model's ability to refrain from generating\narguments when instructed and no factual basis exists. Our automated method\nemploys an external LLM to extract factors from generated arguments and\ncompares them against the ground-truth factors provided in the input case\ntriples (current case and two precedent cases). We evaluated eight distinct\nLLMs on three tests of increasing difficulty: 1) generating a standard 3-ply\nargument, 2) generating an argument with swapped precedent roles, and 3)\nrecognizing the impossibility of argument generation due to lack of shared\nfactors and abstaining. Our findings indicate that while current LLMs achieve\nhigh accuracy (over 90%) in avoiding hallucination on viable argument\ngeneration tests (Tests 1 & 2), they often fail to utilize the full set of\nrelevant factors present in the cases. Critically, on the abstention test (Test\n3), most models failed to follow instructions to stop, instead generating\nspurious arguments despite the lack of common factors. This automated pipeline\nprovides a scalable method for assessing these crucial LLM behaviors,\nhighlighting the need for improvements in factor utilization and robust\nabstention capabilities before reliable deployment in legal settings. Link:\nhttps://lizhang-aiandlaw.github.io/An-Automated-Pipeline-for-Evaluating-LLM-Generated-3-ply-Case-Based-Legal-Arguments/",
      "authors": [
        "Li Zhang",
        "Morgan Gray",
        "Jaromir Savelka",
        "Kevin D. Ashley"
      ],
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG",
        "68T50"
      ],
      "links": [
        "http://arxiv.org/abs/2506.00694v2",
        "http://arxiv.org/pdf/2506.00694v2"
      ],
      "primary_search_term": "machine learning",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2506.00680v1",
      "title": "Understanding the European energy crisis through structural causal\n  models",
      "published": "2025-05-31T19:14:46Z",
      "updated": "2025-05-31T19:14:46Z",
      "summary": "Natural gas supplies in Europe were disrupted and energy prices soared in the\ncontext of Russia's invasion of Ukraine. Electricity prices in France\nexperienced the largest relative increase among European countries, even though\nnatural gas plays a negligible role in the French electricity system. In this\narticle, we demonstrate the importance of causal statistical methods and\npropose causal graphs to investigate the French electricity market and pinpoint\nkey influencing factors on electricity prices and net exports. We demonstrate\nthat a causal approach resolves paradoxical results of simple correlation\nstudies and enables a quantitative analysis of indirect causal effects. We\nintroduce a linear structural causal model as well as non-linear tree-based\nmachine learning combined with Shapley flows. The models elucidate the\ninterplay of gas prices and the unavailability of nuclear power plants during\nthe energy crisis: The high unavailability made France dependent on imports and\nlinked prices to neighbouring countries.",
      "authors": [
        "Anton Tausendfreund",
        "Sarah Schreyer",
        "Florian Immig",
        "Ulrich Oberhofer",
        "Julius Trebbien",
        "Aaron Praktiknjo",
        "Benjamin Sch\u00e4fer",
        "Dirk Witthaut"
      ],
      "categories": [
        "stat.AP"
      ],
      "links": [
        "http://arxiv.org/abs/2506.00680v1",
        "http://arxiv.org/pdf/2506.00680v1"
      ],
      "primary_search_term": "machine learning",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2506.00664v1",
      "title": "OntoRAG: Enhancing Question-Answering through Automated Ontology\n  Derivation from Unstructured Knowledge Bases",
      "published": "2025-05-31T18:33:39Z",
      "updated": "2025-05-31T18:33:39Z",
      "summary": "Ontologies are pivotal for structuring knowledge bases to enhance question\nanswering (QA) systems powered by Large Language Models (LLMs). However,\ntraditional ontology creation relies on manual efforts by domain experts, a\nprocess that is time intensive, error prone, and impractical for large, dynamic\nknowledge domains. This paper introduces OntoRAG, an automated pipeline\ndesigned to derive ontologies from unstructured knowledge bases, with a focus\non electrical relay documents. OntoRAG integrates advanced techniques,\nincluding web scraping, PDF parsing, hybrid chunking, information extraction,\nknowledge graph construction, and ontology creation, to transform unstructured\ndata into a queryable ontology. By leveraging LLMs and graph based methods,\nOntoRAG enhances global sensemaking capabilities, outperforming conventional\nRetrieval Augmented Generation (RAG) and GraphRAG approaches in\ncomprehensiveness and diversity. Experimental results demonstrate OntoRAGs\neffectiveness, achieving a comprehensiveness win rate of 85% against vector RAG\nand 75% against GraphRAGs best configuration. This work addresses the critical\nchallenge of automating ontology creation, advancing the vision of the semantic\nweb.",
      "authors": [
        "Yash Tiwari",
        "Owais Ahmad Lone",
        "Mayukha Pal"
      ],
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "links": [
        "http://arxiv.org/abs/2506.00664v1",
        "http://arxiv.org/pdf/2506.00664v1"
      ],
      "primary_search_term": "machine learning",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2506.03193v1",
      "title": "Human Fall Detection using Transfer Learning-based 3D CNN",
      "published": "2025-05-31T16:58:12Z",
      "updated": "2025-05-31T16:58:12Z",
      "summary": "Unintentional or accidental falls are one of the significant health issues in\nsenior persons. The population of senior persons is increasing steadily. So,\nthere is a need for an automated fall detection monitoring system. This paper\nintroduces a vision-based fall detection system using a pre-trained 3D CNN.\nUnlike 2D CNN, 3D CNN extracts not only spatial but also temporal features. The\nproposed model leverages the original learned weights of a 3D CNN model\npre-trained on the Sports1M dataset to extract the spatio-temporal features.\nOnly the SVM classifier was trained, which saves the time required to train the\n3D CNN. Stratified shuffle five split cross-validation has been used to split\nthe dataset into training and testing data. Extracted features from the\nproposed 3D CNN model were fed to an SVM classifier to classify the activity as\nfall or ADL. Two datasets, GMDCSA and CAUCAFall, were utilized to conduct the\nexperiment. The source code for this work can be accessed via the following\nlink: https://github.com/ekramalam/HFD_3DCNN.",
      "authors": [
        "Ekram Alam",
        "Abu Sufian",
        "Paramartha Dutta",
        "Marco Leo"
      ],
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "links": [
        "http://dx.doi.org/10.1007/978-3-031-81935-3_9",
        "http://arxiv.org/abs/2506.03193v1",
        "http://arxiv.org/pdf/2506.03193v1"
      ],
      "primary_search_term": "machine learning",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2506.00577v1",
      "title": "Reasoning Like an Economist: Post-Training on Economic Problems Induces\n  Strategic Generalization in LLMs",
      "published": "2025-05-31T14:22:40Z",
      "updated": "2025-05-31T14:22:40Z",
      "summary": "Directly training Large Language Models (LLMs) for Multi-Agent Systems (MAS)\nremains challenging due to intricate reward modeling, dynamic agent\ninteractions, and demanding generalization requirements. This paper explores\nwhether post-training techniques, specifically Supervised Fine-Tuning (SFT) and\nReinforcement Learning with Verifiable Rewards (RLVR), can effectively\n$\\textit{generalize}$ to multi-agent scenarios. We use economic reasoning as a\ntestbed, leveraging its strong foundations in mathematics and game theory, its\ndemand for structured analytical reasoning, and its relevance to real-world\napplications such as market design, resource allocation, and policy analysis.\nWe introduce $\\textbf{Recon}$ ($\\textbf{R}$easoning like an\n$\\textbf{ECON}$omist), a 7B-parameter open-source LLM post-trained on a\nhand-curated dataset of 2,100 high-quality economic reasoning problems.\nComprehensive evaluation on economic reasoning benchmarks and multi-agent games\nreveals clear improvements in structured reasoning and economic rationality.\nThese results underscore the promise of domain-aligned post-training for\nenhancing reasoning and agent alignment, shedding light on the roles of SFT and\nRL in shaping model behavior. Code is available at\nhttps://github.com/MasterZhou1/Recon .",
      "authors": [
        "Yufa Zhou",
        "Shaobo Wang",
        "Xingyu Dong",
        "Xiangqi Jin",
        "Yifang Chen",
        "Yue Min",
        "Kexin Yang",
        "Xingzhang Ren",
        "Dayiheng Liu",
        "Linfeng Zhang"
      ],
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.GT",
        "cs.MA"
      ],
      "links": [
        "http://arxiv.org/abs/2506.00577v1",
        "http://arxiv.org/pdf/2506.00577v1"
      ],
      "primary_search_term": "machine learning",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2506.00572v1",
      "title": "Machine-learning Growth at Risk",
      "published": "2025-05-31T14:06:53Z",
      "updated": "2025-05-31T14:06:53Z",
      "summary": "We analyse growth vulnerabilities in the US using quantile partial\ncorrelation regression, a selection-based machine-learning method that achieves\nmodel selection consistency under time series. We find that downside risk is\nprimarily driven by financial, labour-market, and housing variables, with their\nimportance changing over time. Decomposing downside risk into its individual\ncomponents, we construct sector-specific indices that predict it, while\ncontrolling for information from other sectors, thereby isolating the downside\nrisks emanating from each sector.",
      "authors": [
        "Tobias Adrian",
        "Hongqi Chen",
        "Max-Sebastian Dov\u00ec",
        "Ji Hyung Lee"
      ],
      "categories": [
        "econ.GN",
        "q-fin.EC"
      ],
      "links": [
        "http://arxiv.org/abs/2506.00572v1",
        "http://arxiv.org/pdf/2506.00572v1"
      ],
      "primary_search_term": "machine learning",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2506.00505v1",
      "title": "From Rules to Rewards: Reinforcement Learning for Interest Rate\n  Adjustment in DeFi Lending",
      "published": "2025-05-31T10:56:07Z",
      "updated": "2025-05-31T10:56:07Z",
      "summary": "Decentralized Finance (DeFi) lending enables permissionless borrowing via\nsmart contracts. However, it faces challenges in optimizing interest rates,\nmitigating bad debt, and improving capital efficiency. Rule-based interest-rate\nmodels struggle to adapt to dynamic market conditions, leading to\ninefficiencies. This work applies Offline Reinforcement Learning (RL) to\noptimize interest rate adjustments in DeFi lending protocols. Using historical\ndata from Aave protocol, we evaluate three RL approaches: Conservative\nQ-Learning (CQL), Behavior Cloning (BC), and TD3 with Behavior Cloning\n(TD3-BC). TD3-BC demonstrates superior performance in balancing utilization,\ncapital stability, and risk, outperforming existing models. It adapts\neffectively to historical stress events like the May 2021 crash and the March\n2023 USDC depeg, showcasing potential for automated, real-time governance.",
      "authors": [
        "Hanxiao Qu",
        "Krzysztof Gogol",
        "Florian Groetschla",
        "Claudio Tessone"
      ],
      "categories": [
        "cs.LG"
      ],
      "links": [
        "http://arxiv.org/abs/2506.00505v1",
        "http://arxiv.org/pdf/2506.00505v1"
      ],
      "primary_search_term": "machine learning",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2506.00467v1",
      "title": "SST: Self-training with Self-adaptive Thresholding for Semi-supervised\n  Learning",
      "published": "2025-05-31T08:34:04Z",
      "updated": "2025-05-31T08:34:04Z",
      "summary": "Neural networks have demonstrated exceptional performance in supervised\nlearning, benefiting from abundant high-quality annotated data. However,\nobtaining such data in real-world scenarios is costly and labor-intensive.\nSemi-supervised learning (SSL) offers a solution to this problem. Recent\nstudies, such as Semi-ViT and Noisy Student, which employ consistency\nregularization or pseudo-labeling, have demonstrated significant achievements.\nHowever, they still face challenges, particularly in accurately selecting\nsufficient high-quality pseudo-labels due to their reliance on fixed\nthresholds. Recent methods such as FlexMatch and FreeMatch have introduced\nflexible or self-adaptive thresholding techniques, greatly advancing SSL\nresearch. Nonetheless, their process of updating thresholds at each iteration\nis deemed time-consuming, computationally intensive, and potentially\nunnecessary. To address these issues, we propose Self-training with\nSelf-adaptive Thresholding (SST), a novel, effective, and efficient SSL\nframework. SST introduces an innovative Self-Adaptive Thresholding (SAT)\nmechanism that adaptively adjusts class-specific thresholds based on the\nmodel's learning progress. SAT ensures the selection of high-quality\npseudo-labeled data, mitigating the risks of inaccurate pseudo-labels and\nconfirmation bias. Extensive experiments demonstrate that SST achieves\nstate-of-the-art performance with remarkable efficiency, generalization, and\nscalability across various architectures and datasets. Semi-SST-ViT-Huge\nachieves the best results on competitive ImageNet-1K SSL benchmarks, with 80.7%\n/ 84.9% Top-1 accuracy using only 1% / 10% labeled data. Compared to the\nfully-supervised DeiT-III-ViT-Huge, which achieves 84.8% Top-1 accuracy using\n100% labeled data, our method demonstrates superior performance using only 10%\nlabeled data.",
      "authors": [
        "Shuai Zhao",
        "Heyan Huang",
        "Xinge Li",
        "Xiaokang Chen",
        "Rui Wang"
      ],
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "links": [
        "http://dx.doi.org/10.1016/j.ipm.2025.104158",
        "http://arxiv.org/abs/2506.00467v1",
        "http://arxiv.org/pdf/2506.00467v1"
      ],
      "primary_search_term": "machine learning",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2506.00455v1",
      "title": "Diffusion Models for Increasing Accuracy in Olfaction Sensors and\n  Datasets",
      "published": "2025-05-31T08:22:09Z",
      "updated": "2025-05-31T08:22:09Z",
      "summary": "Robotic odour source localization (OSL) is a critical capability for\nautonomous systems operating in complex environments. However, current OSL\nmethods often suffer from ambiguities, particularly when robots misattribute\nodours to incorrect objects due to limitations in olfactory datasets and sensor\nresolutions. To address this challenge, we introduce a novel machine learning\nmethod using diffusion-based molecular generation to enhance odour localization\naccuracy that can be used by itself or with automated olfactory dataset\nconstruction pipelines with vision-language models (VLMs) This generative\nprocess of our diffusion model expands the chemical space beyond the\nlimitations of both current olfactory datasets and the training data of VLMs,\nenabling the identification of potential odourant molecules not previously\ndocumented. The generated molecules can then be more accurately validated using\nadvanced olfactory sensors which emulate human olfactory recognition through\nelectronic sensor arrays. By integrating visual analysis, language processing,\nand molecular generation, our framework enhances the ability of\nolfaction-vision models on robots to accurately associate odours with their\ncorrect sources, thereby improving navigation and decision-making in\nenvironments where olfactory cues are essential. Our methodology represents a\nfoundational advancement in the field of robotic olfaction, offering a scalable\nsolution to the challenges posed by limited olfactory data and sensor\nambiguities.",
      "authors": [
        "Kordel K. France",
        "Ovidiu Daescu"
      ],
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "links": [
        "http://arxiv.org/abs/2506.00455v1",
        "http://arxiv.org/pdf/2506.00455v1"
      ],
      "primary_search_term": "machine learning",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2506.00436v1",
      "title": "Learning from Double Positive and Unlabeled Data for Potential-Customer\n  Identification",
      "published": "2025-05-31T07:33:48Z",
      "updated": "2025-05-31T07:33:48Z",
      "summary": "In this study, we propose a method for identifying potential customers in\ntargeted marketing by applying learning from positive and unlabeled data (PU\nlearning). We consider a scenario in which a company sells a product and can\nobserve only the customers who purchased it. Decision-makers seek to market\nproducts effectively based on whether people have loyalty to the company.\nIndividuals with loyalty are those who are likely to remain interested in the\ncompany even without additional advertising. Consequently, those loyal\ncustomers would likely purchase from the company if they are interested in the\nproduct. In contrast, people with lower loyalty may overlook the product or buy\nsimilar products from other companies unless they receive marketing attention.\nTherefore, by focusing marketing efforts on individuals who are interested in\nthe product but do not have strong loyalty, we can achieve more efficient\nmarketing. To achieve this goal, we consider how to learn, from limited data, a\nclassifier that identifies potential customers who (i) have interest in the\nproduct and (ii) do not have loyalty to the company. Although our algorithm\ncomprises a single-stage optimization, its objective function implicitly\ncontains two losses derived from standard PU learning settings. For this\nreason, we refer to our approach as double PU learning. We verify the validity\nof the proposed algorithm through numerical experiments, confirming that it\nfunctions appropriately for the problem at hand.",
      "authors": [
        "Masahiro Kato",
        "Yuki Ikeda abd Kentaro Baba",
        "Takashi Imai",
        "Ryo Inokuchi"
      ],
      "categories": [
        "cs.LG",
        "cs.AI",
        "econ.EM",
        "stat.ME",
        "stat.ML"
      ],
      "links": [
        "http://arxiv.org/abs/2506.00436v1",
        "http://arxiv.org/pdf/2506.00436v1"
      ],
      "primary_search_term": "machine learning",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2506.00386v1",
      "title": "Adaptive-VP: A Framework for LLM-Based Virtual Patients that Adapts to\n  Trainees' Dialogue to Facilitate Nurse Communication Training",
      "published": "2025-05-31T04:34:55Z",
      "updated": "2025-05-31T04:34:55Z",
      "summary": "Effective communication training is essential to preparing nurses for\nhigh-quality patient care. While standardized patient (SP) simulations provide\nvaluable experiential learning, they are often costly and inflexible. Virtual\npatient (VP) systems offer a scalable alternative, but most fail to adapt to\nthe varying communication skills of trainees. In particular, when trainees\nrespond ineffectively, VPs should escalate in hostility or become\nuncooperative--yet this level of adaptive interaction remains largely\nunsupported. To address this gap, we introduce Adaptive-VP, a VP dialogue\ngeneration framework that leverages large language models (LLMs) to dynamically\nadapt VP behavior based on trainee input. The framework features a pipeline for\nconstructing clinically grounded yet flexible VP scenarios and a modular system\nfor assessing trainee communication and adjusting VP responses in real time,\nwhile ensuring learner safety. We validated Adaptive-VP by simulating\nchallenging patient conversations. Automated evaluation using a corpus from\npracticing nurses showed that our communication skill evaluation mechanism\nreflected real-world proficiency levels. Expert nurses further confirmed that\nAdaptive-VP produced more natural and realistic interactions than existing\napproaches, demonstrating its potential as a scalable and effective tool for\nnursing communication training.",
      "authors": [
        "Keyeun Lee",
        "Seolhee Lee",
        "Esther Hehsun Kim",
        "Yena Ko",
        "Jinsu Eun",
        "Dahee Kim",
        "Hyewon Cho",
        "Haiyi Zhu",
        "Robert E. Kraut",
        "Eunyoung Suh",
        "Eun-mee Kim",
        "Hajin Lim"
      ],
      "categories": [
        "cs.CL",
        "cs.HC"
      ],
      "links": [
        "http://arxiv.org/abs/2506.00386v1",
        "http://arxiv.org/pdf/2506.00386v1"
      ],
      "primary_search_term": "machine learning",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2506.00305v1",
      "title": "Learning Aerodynamics for the Control of Flying Humanoid Robots",
      "published": "2025-05-30T23:27:44Z",
      "updated": "2025-05-30T23:27:44Z",
      "summary": "Robots with multi-modal locomotion are an active research field due to their\nversatility in diverse environments. In this context, additional actuation can\nprovide humanoid robots with aerial capabilities. Flying humanoid robots face\nchallenges in modeling and control, particularly with aerodynamic forces. This\npaper addresses these challenges from a technological and scientific\nstandpoint. The technological contribution includes the mechanical design of\niRonCub-Mk1, a jet-powered humanoid robot, optimized for jet engine\nintegration, and hardware modifications for wind tunnel experiments on humanoid\nrobots for precise aerodynamic forces and surface pressure measurements. The\nscientific contribution offers a comprehensive approach to model and control\naerodynamic forces using classical and learning techniques. Computational Fluid\nDynamics (CFD) simulations calculate aerodynamic forces, validated through wind\ntunnel experiments on iRonCub-Mk1. An automated CFD framework expands the\naerodynamic dataset, enabling the training of a Deep Neural Network and a\nlinear regression model. These models are integrated into a simulator for\ndesigning aerodynamic-aware controllers, validated through flight simulations\nand balancing experiments on the iRonCub-Mk1 physical prototype.",
      "authors": [
        "Antonello Paolino",
        "Gabriele Nava",
        "Fabio Di Natale",
        "Fabio Bergonti",
        "Punith Reddy Vanteddu",
        "Donato Grassi",
        "Luca Riccobene",
        "Alex Zanotti",
        "Renato Tognaccini",
        "Gianluca Iaccarino",
        "Daniele Pucci"
      ],
      "categories": [
        "cs.RO",
        "cs.LG"
      ],
      "links": [
        "http://arxiv.org/abs/2506.00305v1",
        "http://arxiv.org/pdf/2506.00305v1"
      ],
      "primary_search_term": "machine learning",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2506.00233v1",
      "title": "Ethical AI: Towards Defining a Collective Evaluation Framework",
      "published": "2025-05-30T21:10:47Z",
      "updated": "2025-05-30T21:10:47Z",
      "summary": "Artificial Intelligence (AI) is transforming sectors such as healthcare,\nfinance, and autonomous systems, offering powerful tools for innovation. Yet\nits rapid integration raises urgent ethical concerns related to data ownership,\nprivacy, and systemic bias. Issues like opaque decision-making, misleading\noutputs, and unfair treatment in high-stakes domains underscore the need for\ntransparent and accountable AI systems. This article addresses these challenges\nby proposing a modular ethical assessment framework built on ontological blocks\nof meaning-discrete, interpretable units that encode ethical principles such as\nfairness, accountability, and ownership. By integrating these blocks with FAIR\n(Findable, Accessible, Interoperable, Reusable) principles, the framework\nsupports scalable, transparent, and legally aligned ethical evaluations,\nincluding compliance with the EU AI Act. Using a real-world use case in\nAI-powered investor profiling, the paper demonstrates how the framework enables\ndynamic, behavior-informed risk classification. The findings suggest that\nontological blocks offer a promising path toward explainable and auditable AI\nethics, though challenges remain in automation and probabilistic reasoning.",
      "authors": [
        "Aasish Kumar Sharma",
        "Dimitar Kyosev",
        "Julian Kunkel"
      ],
      "categories": [
        "cs.AI"
      ],
      "links": [
        "http://arxiv.org/abs/2506.00233v1",
        "http://arxiv.org/pdf/2506.00233v1"
      ],
      "primary_search_term": "machine learning",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2506.02034v1",
      "title": "High-throughput viscometry via machine-learning from videos of inverted\n  vials",
      "published": "2025-05-30T20:45:05Z",
      "updated": "2025-05-30T20:45:05Z",
      "summary": "Although the inverted vial test has been widely used as a qualitative method\nfor estimating fluid viscosity, quantitative rheological characterization has\nremained limited due to its complex, uncontrolled flow - driven by gravity,\nsurface tension, inertia, and initial conditions. Here, we present a computer\nvision (CV) viscometer that automates the inverted vial test and enables\nquantitative viscosity inference across nearly five orders of magnitude\n(0.01-1000 Pas), without requiring direct velocity field measurements. The\nsystem simultaneously inverts multiple vials and records videos of the evolving\nfluid, which are fed into a neural network that approximates the inverse\nfunction from visual features and known fluid density. Despite the complex,\nmulti-regime flow within the vial, our approach achieves relative errors below\n25%, improving to 15% for viscosities above 0.1 Pas. When tested on\nnon-Newtonian polymer solutions, the method reliably estimates zero-shear\nviscosity as long as viscoelastic or shear-thinning behaviors remain negligible\nwithin the flow regime. Moreover, high standard deviations in the inferred\nvalues may serve as a proxy for identifying fluids with strong non-Newtonian\nbehavior. The CV viscometer requires only one camera and one motor, is\ncontactless and low-cost, and can be easily integrated into high-throughput\nexperimental automated and manual workflows. Transcending traditional\ncharacterization paradigms, our method leverages uncontrolled flows and visual\nfeatures to achieve simplicity and scalability, enabling high-throughput\nviscosity inference that can meet the growing demand of data-driven material\nmodels while remaining accessible to lower resource environments.",
      "authors": [
        "Ignacio Arretche",
        "Mohammad Tanver Hossain",
        "Ramdas Tiwari",
        "Abbie Kim",
        "Mya G. Mills",
        "Connor D. Armstrong",
        "Jacob J. Lessard",
        "Sameh H. Tawfick",
        "Randy H. Ewoldt"
      ],
      "categories": [
        "cs.GR"
      ],
      "links": [
        "http://arxiv.org/abs/2506.02034v1",
        "http://arxiv.org/pdf/2506.02034v1"
      ],
      "primary_search_term": "machine learning",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2506.00202v2",
      "title": "What do professional software developers need to know to succeed in an\n  age of Artificial Intelligence?",
      "published": "2025-05-30T20:14:03Z",
      "updated": "2025-06-04T17:32:53Z",
      "summary": "Generative AI is showing early evidence of productivity gains for software\ndevelopers, but concerns persist regarding workforce disruption and deskilling.\nWe describe our research with 21 developers at the cutting edge of using AI,\nsummarizing 12 of their work goals we uncovered, together with 75 associated\ntasks and the skills & knowledge for each, illustrating how developers use AI\nat work. From all of these, we distilled our findings in the form of 5\ninsights. We found that the skills & knowledge to be a successful AI-enhanced\ndeveloper are organized into four domains (using Generative AI effectively,\ncore software engineering, adjacent engineering, and adjacent non-engineering)\ndeployed at critical junctures throughout a 6-step task workflow. In order to\n\"future proof\" developers for this age of AI, on-the-job learning initiatives\nand computer science degree programs will need to target both \"soft\" skills and\nthe technical skills & knowledge in all four domains to reskill, upskill and\nsafeguard against deskilling.",
      "authors": [
        "Matthew Kam",
        "Cody Miller",
        "Miaoxin Wang",
        "Abey Tidwell",
        "Irene A. Lee",
        "Joyce Malyn-Smith",
        "Beatriz Perez",
        "Vikram Tiwari",
        "Joshua Kenitzer",
        "Andrew Macvean",
        "Erin Barrar"
      ],
      "categories": [
        "cs.AI"
      ],
      "links": [
        "http://dx.doi.org/10.1145/3696630.3727251",
        "http://arxiv.org/abs/2506.00202v2",
        "http://arxiv.org/pdf/2506.00202v2"
      ],
      "primary_search_term": "machine learning",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2506.00169v1",
      "title": "Utilizing AI for Aviation Post-Accident Analysis Classification",
      "published": "2025-05-30T19:15:04Z",
      "updated": "2025-05-30T19:15:04Z",
      "summary": "The volume of textual data available in aviation safety reports presents a\nchallenge for timely and accurate analysis. This paper examines how Artificial\nIntelligence (AI) and, specifically, Natural Language Processing (NLP) can\nautomate the process of extracting valuable insights from this data, ultimately\nenhancing aviation safety. The paper reviews ongoing efforts focused on the\napplication of NLP and deep learning to aviation safety reports, with the goal\nof classifying the level of damage to an aircraft and identifying the phase of\nflight during which safety occurrences happen. Additionally, the paper explores\nthe use of Topic Modeling (TM) to uncover latent thematic structures within\naviation incident reports, aiming to identify recurring patterns and potential\nareas for safety improvement. The paper compares and contrasts the performance\nof various deep learning models and TM techniques applied to datasets from the\nNational Transportation Safety Board (NTSB) and the Australian Transport Safety\nBureau (ATSB), as well as the Aviation Safety Network (ASN), discussing the\nimpact of dataset size and source on the accuracy of the analysis. The findings\ndemonstrate that both NLP and deep learning, as well as TM, can significantly\nimprove the efficiency and accuracy of aviation safety analysis, paving the way\nfor more proactive safety management and risk mitigation strategies.",
      "authors": [
        "Aziida Nanyonga",
        "Graham Wild"
      ],
      "categories": [
        "cs.AI"
      ],
      "links": [
        "http://arxiv.org/abs/2506.00169v1",
        "http://arxiv.org/pdf/2506.00169v1"
      ],
      "primary_search_term": "machine learning",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2506.00164v1",
      "title": "Efficient Endangered Deer Species Monitoring with UAV Aerial Imagery and\n  Deep Learning",
      "published": "2025-05-30T19:09:48Z",
      "updated": "2025-05-30T19:09:48Z",
      "summary": "This paper examines the use of Unmanned Aerial Vehicles (UAVs) and deep\nlearning for detecting endangered deer species in their natural habitats. As\ntraditional identification processes require trained manual labor that can be\ncostly in resources and time, there is a need for more efficient solutions.\nLeveraging high-resolution aerial imagery, advanced computer vision techniques\nare applied to automate the identification process of deer across two distinct\nprojects in Buenos Aires, Argentina. The first project, Pantano Project,\ninvolves the marsh deer in the Paran\\'a Delta, while the second, WiMoBo,\nfocuses on the Pampas deer in Campos del Tuy\\'u National Park. A tailored\nalgorithm was developed using the YOLO framework, trained on extensive datasets\ncompiled from UAV-captured images. The findings demonstrate that the algorithm\neffectively identifies marsh deer with a high degree of accuracy and provides\ninitial insights into its applicability to Pampas deer, albeit with noted\nlimitations. This study not only supports ongoing conservation efforts but also\nhighlights the potential of integrating AI with UAV technology to enhance\nwildlife monitoring and management practices.",
      "authors": [
        "Agust\u00edn Roca",
        "Gabriel Torre",
        "Juan I. Giribet",
        "Gast\u00f3n Castro",
        "Leonardo Colombo",
        "Ignacio Mas",
        "Javier Pereira"
      ],
      "categories": [
        "cs.CV"
      ],
      "links": [
        "http://dx.doi.org/10.1109/ARGENCON62399.2024.10735858",
        "http://arxiv.org/abs/2506.00164v1",
        "http://arxiv.org/pdf/2506.00164v1"
      ],
      "primary_search_term": "machine learning",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2506.00140v2",
      "title": "Balancing Profit and Fairness in Risk-Based Pricing Markets",
      "published": "2025-05-30T18:24:08Z",
      "updated": "2025-06-04T16:06:36Z",
      "summary": "Dynamic, risk-based pricing can systematically exclude vulnerable consumer\ngroups from essential resources such as health insurance and consumer credit.\nWe show that a regulator can realign private incentives with social objectives\nthrough a learned, interpretable tax schedule. First, we provide a formal\nproposition that bounding each firm's \\emph{local} demographic gap implicitly\nbounds the \\emph{global} opt-out disparity, motivating firm-level penalties.\nBuilding on this insight we introduce \\texttt{MarketSim} -- an open-source,\nscalable simulator of heterogeneous consumers and profit-maximizing firms --\nand train a reinforcement learning (RL) social planner (SP) that selects a\nbracketed fairness-tax while remaining close to a simple linear prior via an\n$\\mathcal{L}_1$ regularizer. The learned policy is thus both transparent and\neasily interpretable. In two empirically calibrated markets, i.e., U.S.\nhealth-insurance and consumer-credit, our planner simultaneously raises\ndemand-fairness by up to $16\\%$ relative to unregulated Free Market while\noutperforming a fixed linear schedule in terms of social welfare without\nexplicit coordination. These results illustrate how AI-assisted regulation can\nconvert a competitive social dilemma into a win-win equilibrium, providing a\nprincipled and practical framework for fairness-aware market oversight.",
      "authors": [
        "Jesse Thibodeau",
        "Hadi Nekoei",
        "Afaf Ta\u00efk",
        "Janarthanan Rajendran",
        "Golnoosh Farnadi"
      ],
      "categories": [
        "cs.AI",
        "cs.LG",
        "econ.GN",
        "q-fin.EC"
      ],
      "links": [
        "http://arxiv.org/abs/2506.00140v2",
        "http://arxiv.org/pdf/2506.00140v2"
      ],
      "primary_search_term": "machine learning",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2506.00128v1",
      "title": "Applying Large Language Models to Issue Classification: Revisiting with\n  Extended Data and New Models",
      "published": "2025-05-30T18:02:55Z",
      "updated": "2025-05-30T18:02:55Z",
      "summary": "Effective prioritization of issue reports in software engineering helps to\noptimize resource allocation and information recovery. However, manual issue\nclassification is laborious and lacks scalability. As an alternative, many open\nsource software (OSS) projects employ automated processes for this task, yet\nthis method often relies on large datasets for adequate training.\nTraditionally, machine learning techniques have been used for issue\nclassification. More recently, large language models (LLMs) have emerged as\npowerful tools for addressing a range of software engineering challenges,\nincluding code and test generation, mapping new requirements to legacy software\nendpoints, and conducting code reviews. The following research investigates an\nautomated approach to issue classification based on LLMs. By leveraging the\ncapabilities of such models, we aim to develop a robust system for prioritizing\nissue reports, mitigating the necessity for extensive training data while also\nmaintaining reliability in classification. In our research, we developed an\nLLM-based approach for accurately labeling issues by selecting two of the most\nprominent large language models. We then compared their performance across\nmultiple datasets. Our findings show that GPT-4o achieved the best results in\nclassifying issues from the NLBSE 2024 competition. Moreover, GPT-4o\noutperformed DeepSeek R1, achieving an F1 score 20% higher when both models\nwere trained on the same dataset from the NLBSE 2023 competition, which was ten\ntimes larger than the NLBSE 2024 dataset. The fine-tuned GPT-4o model attained\nan average F1 score of 80.7%, while the fine-tuned DeepSeek R1 model achieved\n59.33%. Increasing the dataset size did not improve the F1 score, reducing the\ndependence on massive datasets for building an efficient solution to issue\nclassification.",
      "authors": [
        "Gabriel Aracena",
        "Kyle Luster",
        "Fabio Santos",
        "Igor Steinmacher",
        "Marco A. Gerosa"
      ],
      "categories": [
        "cs.SE",
        "cs.LG"
      ],
      "links": [
        "http://arxiv.org/abs/2506.00128v1",
        "http://arxiv.org/pdf/2506.00128v1"
      ],
      "primary_search_term": "machine learning",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2505.24878v1",
      "title": "Open CaptchaWorld: A Comprehensive Web-based Platform for Testing and\n  Benchmarking Multimodal LLM Agents",
      "published": "2025-05-30T17:59:55Z",
      "updated": "2025-05-30T17:59:55Z",
      "summary": "CAPTCHAs have been a critical bottleneck for deploying web agents in\nreal-world applications, often blocking them from completing end-to-end\nautomation tasks. While modern multimodal LLM agents have demonstrated\nimpressive performance in static perception tasks, their ability to handle\ninteractive, multi-step reasoning challenges like CAPTCHAs is largely untested.\nTo address this gap, we introduce Open CaptchaWorld, the first web-based\nbenchmark and platform specifically designed to evaluate the visual reasoning\nand interaction capabilities of MLLM-powered agents through diverse and dynamic\nCAPTCHA puzzles. Our benchmark spans 20 modern CAPTCHA types, totaling 225\nCAPTCHAs, annotated with a new metric we propose: CAPTCHA Reasoning Depth,\nwhich quantifies the number of cognitive and motor steps required to solve each\npuzzle. Experimental results show that humans consistently achieve near-perfect\nscores, state-of-the-art MLLM agents struggle significantly, with success rates\nat most 40.0% by Browser-Use Openai-o3, far below human-level performance,\n93.3%. This highlights Open CaptchaWorld as a vital benchmark for diagnosing\nthe limits of current multimodal agents and guiding the development of more\nrobust multimodal reasoning systems. Code and Data are available at this https\nURL.",
      "authors": [
        "Yaxin Luo",
        "Zhaoyi Li",
        "Jiacheng Liu",
        "Jiacheng Cui",
        "Xiaohan Zhao",
        "Zhiqiang Shen"
      ],
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CV",
        "cs.LG"
      ],
      "links": [
        "http://arxiv.org/abs/2505.24878v1",
        "http://arxiv.org/pdf/2505.24878v1"
      ],
      "primary_search_term": "machine learning",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2506.02032v1",
      "title": "Towards Secure MLOps: Surveying Attacks, Mitigation Strategies, and\n  Research Challenges",
      "published": "2025-05-30T17:45:31Z",
      "updated": "2025-05-30T17:45:31Z",
      "summary": "The rapid adoption of machine learning (ML) technologies has driven\norganizations across diverse sectors to seek efficient and reliable methods to\naccelerate model development-to-deployment. Machine Learning Operations (MLOps)\nhas emerged as an integrative approach addressing these requirements by\nunifying relevant roles and streamlining ML workflows. As the MLOps market\ncontinues to grow, securing these pipelines has become increasingly critical.\nHowever, the unified nature of MLOps ecosystem introduces vulnerabilities,\nmaking them susceptible to adversarial attacks where a single misconfiguration\ncan lead to compromised credentials, severe financial losses, damaged public\ntrust, and the poisoning of training data. Our paper presents a systematic\napplication of the MITRE ATLAS (Adversarial Threat Landscape for\nArtificial-Intelligence Systems) framework, a comprehensive and continuously\nupdated catalog of AI-focused attacks, to systematically assess attacks across\ndifferent phases of the MLOps ecosystem. We begin by examining the preparatory\nphases during which adversaries acquire the essential intelligence required to\ninitiate their attacks. We then present a structured taxonomy of attack\ntechniques explicitly mapped to corresponding phases of the MLOps ecosystem,\nsupported by examples drawn from red-teaming exercises and real-world\nincidents. This is followed by a taxonomy of mitigation strategies aligned with\nthese attack categories, offering actionable early-stage defenses to strengthen\nthe security of MLOps ecosystem. Given the rapid evolution and adoption of\nMLOps, we further highlight key research gaps that require immediate attention.\nOur work emphasizes the importance of implementing robust security protocols\nfrom the outset, empowering practitioners to safeguard MLOps ecosystem against\nevolving cyber attacks.",
      "authors": [
        "Raj Patel",
        "Himanshu Tripathi",
        "Jasper Stone",
        "Noorbakhsh Amiri Golilarz",
        "Sudip Mittal",
        "Shahram Rahimi",
        "Vini Chaudhary"
      ],
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "links": [
        "http://arxiv.org/abs/2506.02032v1",
        "http://arxiv.org/pdf/2506.02032v1"
      ],
      "primary_search_term": "machine learning",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2505.24838v1",
      "title": "VideoCAD: A Large-Scale Video Dataset for Learning UI Interactions and\n  3D Reasoning from CAD Software",
      "published": "2025-05-30T17:39:52Z",
      "updated": "2025-05-30T17:39:52Z",
      "summary": "Computer-Aided Design (CAD) is a time-consuming and complex process,\nrequiring precise, long-horizon user interactions with intricate 3D interfaces.\nWhile recent advances in AI-driven user interface (UI) agents show promise,\nmost existing datasets and methods focus on short, low-complexity tasks in\nmobile or web applications, failing to capture the demands of professional\nengineering tools. In this work, we introduce VideoCAD, the first attempt at\nengineering UI interaction learning for precision tasks. Specifically, VideoCAD\nis a large-scale synthetic dataset consisting of over 41K annotated video\nrecordings of CAD operations, generated using an automated framework for\ncollecting high-fidelity UI action data from human-made CAD designs. Compared\nto existing datasets, VideoCAD offers an order of magnitude higher complexity\nin UI interaction learning for real-world engineering tasks, having up to a 20x\nlonger time horizon than other datasets. We show two important downstream\napplications of VideoCAD: learning UI interactions from professional precision\n3D CAD tools and a visual question-answering (VQA) benchmark designed to\nevaluate multimodal large language models' (LLM) spatial reasoning and video\nunderstanding abilities. To learn the UI interactions, we propose\nVideoCADFormer - a state-of-the-art model in learning CAD interactions directly\nfrom video, which outperforms multiple behavior cloning baselines. Both\nVideoCADFormer and the VQA benchmark derived from VideoCAD reveal key\nchallenges in the current state of video-based UI understanding, including the\nneed for precise action grounding, multi-modal and spatial reasoning, and\nlong-horizon dependencies.",
      "authors": [
        "Brandon Man",
        "Ghadi Nehme",
        "Md Ferdous Alam",
        "Faez Ahmed"
      ],
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "links": [
        "http://arxiv.org/abs/2505.24838v1",
        "http://arxiv.org/pdf/2505.24838v1"
      ],
      "primary_search_term": "machine learning",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2505.24708v1",
      "title": "Efficient Bayesian multi-fidelity inverse analysis for expensive and\n  non-differentiable physics-based simulations in high stochastic dimensions",
      "published": "2025-05-30T15:29:36Z",
      "updated": "2025-05-30T15:29:36Z",
      "summary": "High-dimensional Bayesian inverse analysis (dim >> 100) is mostly unfeasible\nfor computationally demanding, nonlinear physics-based high-fidelity (HF)\nmodels. Usually, the use of more efficient gradient-based inference schemes is\nimpeded if the multi-physics models are provided by complex legacy codes.\nAdjoint-based derivatives are either exceedingly cumbersome to derive or\nnon-existent for practically relevant large-scale nonlinear and coupled\nmulti-physics problems. Similarly, holistic automated differentiation w.r.t.\nprimary variables of multi-physics codes is usually not yet an option and\nrequires extensive code restructuring if not considered from the outset in the\nsoftware design. This absence of differentiability further exacerbates the\nalready present computational challenges. To overcome the existing limitations,\nwe propose a novel inference approach called Bayesian multi-fidelity inverse\nanalysis (BMFIA), which leverages simpler and computationally cheaper\nlower-fidelity (LF) models that are designed to provide model derivatives.\nBMFIA learns a simple, probabilistic dependence of the LF and HF models, which\nis then employed in an altered likelihood formulation to statistically correct\nthe inaccurate LF response. From a Bayesian viewpoint, this dependence\nrepresents a multi-fidelity conditional density (discriminative model). We\ndemonstrate how this multi-fidelity conditional density can be learned robustly\nin the small data regime from only a few HF and LF simulations (50 to 300),\nwhich would not be sufficient for naive surrogate approaches. The formulation\nis fully differentiable and allows the flexible design of a wide range of LF\nmodels. We demonstrate that BMFIA solves Bayesian inverse problems for\nscenarios that used to be prohibitive, such as finely-resolved spatial\nreconstruction problems for nonlinear and transient coupled poro-elastic media\nphysics.",
      "authors": [
        "Jonas Nitzler",
        "Bugrahan Z. Tem\u00fcr",
        "Phaedon-Stelios Koutsourelakis",
        "Wolfgang A. Wall"
      ],
      "categories": [
        "cs.CE"
      ],
      "links": [
        "http://arxiv.org/abs/2505.24708v1",
        "http://arxiv.org/pdf/2505.24708v1"
      ],
      "primary_search_term": "machine learning",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2505.24679v1",
      "title": "Beyond FACS: Data-driven Facial Expression Dictionaries, with\n  Application to Predicting Autism",
      "published": "2025-05-30T15:06:01Z",
      "updated": "2025-05-30T15:06:01Z",
      "summary": "The Facial Action Coding System (FACS) has been used by numerous studies to\ninvestigate the links between facial behavior and mental health. The laborious\nand costly process of FACS coding has motivated the development of machine\nlearning frameworks for Action Unit (AU) detection. Despite intense efforts\nspanning three decades, the detection accuracy for many AUs is considered to be\nbelow the threshold needed for behavioral research. Also, many AUs are excluded\naltogether, making it impossible to fulfill the ultimate goal of FACS-the\nrepresentation of any facial expression in its entirety. This paper considers\nan alternative approach. Instead of creating automated tools that mimic FACS\nexperts, we propose to use a new coding system that mimics the key properties\nof FACS. Specifically, we construct a data-driven coding system called the\nFacial Basis, which contains units that correspond to localized and\ninterpretable 3D facial movements, and overcomes three structural limitations\nof automated FACS coding. First, the proposed method is completely\nunsupervised, bypassing costly, laborious and variable manual annotation.\nSecond, Facial Basis reconstructs all observable movement, rather than relying\non a limited repertoire of recognizable movements (as in automated FACS).\nFinally, the Facial Basis units are additive, whereas AUs may fail detection\nwhen they appear in a non-additive combination. The proposed method outperforms\nthe most frequently used AU detector in predicting autism diagnosis from\nin-person and remote conversations, highlighting the importance of encoding\nfacial behavior comprehensively. To our knowledge, Facial Basis is the first\nalternative to FACS for deconstructing facial expressions in videos into\nlocalized movements. We provide an open source implementation of the method at\ngithub.com/sariyanidi/FacialBasis.",
      "authors": [
        "Evangelos Sariyanidi",
        "Lisa Yankowitz",
        "Robert T. Schultz",
        "John D. Herrington",
        "Birkan Tunc",
        "Jeffrey Cohn"
      ],
      "categories": [
        "cs.CV"
      ],
      "links": [
        "http://arxiv.org/abs/2505.24679v1",
        "http://arxiv.org/pdf/2505.24679v1"
      ],
      "primary_search_term": "machine learning",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2505.24667v1",
      "title": "Decoupled Competitive Framework for Semi-supervised Medical Image\n  Segmentation",
      "published": "2025-05-30T14:56:00Z",
      "updated": "2025-05-30T14:56:00Z",
      "summary": "Confronting the critical challenge of insufficiently annotated samples in\nmedical domain, semi-supervised medical image segmentation (SSMIS) emerges as a\npromising solution. Specifically, most methodologies following the Mean Teacher\n(MT) or Dual Students (DS) architecture have achieved commendable results.\nHowever, to date, these approaches face a performance bottleneck due to two\ninherent limitations, \\textit{e.g.}, the over-coupling problem within MT\nstructure owing to the employment of exponential moving average (EMA)\nmechanism, as well as the severe cognitive bias between two students of DS\nstructure, both of which potentially lead to reduced efficacy, or even model\ncollapse eventually. To mitigate these issues, a Decoupled Competitive\nFramework (DCF) is elaborated in this work, which utilizes a straightforward\ncompetition mechanism for the update of EMA, effectively decoupling students\nand teachers in a dynamical manner. In addition, the seamless exchange of\ninvaluable and precise insights is facilitated among students, guaranteeing a\nbetter learning paradigm. The DCF introduced undergoes rigorous validation on\nthree publicly accessible datasets, which encompass both 2D and 3D datasets.\nThe results demonstrate the superiority of our method over previous\ncutting-edge competitors. Code will be available at\nhttps://github.com/JiaheChen2002/DCF.",
      "authors": [
        "Jiahe Chen",
        "Jiahe Ying",
        "Shen Wang",
        "Jianwei Zheng"
      ],
      "categories": [
        "cs.CV"
      ],
      "links": [
        "http://arxiv.org/abs/2505.24667v1",
        "http://arxiv.org/pdf/2505.24667v1"
      ],
      "primary_search_term": "machine learning",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2505.24640v1",
      "title": "Efficient Text Encoders for Labor Market Analysis",
      "published": "2025-05-30T14:27:25Z",
      "updated": "2025-05-30T14:27:25Z",
      "summary": "Labor market analysis relies on extracting insights from job advertisements,\nwhich provide valuable yet unstructured information on job titles and\ncorresponding skill requirements. While state-of-the-art methods for skill\nextraction achieve strong performance, they depend on large language models\n(LLMs), which are computationally expensive and slow. In this paper, we propose\n\\textbf{ConTeXT-match}, a novel contrastive learning approach with token-level\nattention that is well-suited for the extreme multi-label classification task\nof skill classification. \\textbf{ConTeXT-match} significantly improves skill\nextraction efficiency and performance, achieving state-of-the-art results with\na lightweight bi-encoder model. To support robust evaluation, we introduce\n\\textbf{Skill-XL}, a new benchmark with exhaustive, sentence-level skill\nannotations that explicitly address the redundancy in the large label space.\nFinally, we present \\textbf{JobBERT V2}, an improved job title normalization\nmodel that leverages extracted skills to produce high-quality job title\nrepresentations. Experiments demonstrate that our models are efficient,\naccurate, and scalable, making them ideal for large-scale, real-time labor\nmarket analysis.",
      "authors": [
        "Jens-Joris Decorte",
        "Jeroen Van Hautte",
        "Chris Develder",
        "Thomas Demeester"
      ],
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "links": [
        "http://arxiv.org/abs/2505.24640v1",
        "http://arxiv.org/pdf/2505.24640v1"
      ],
      "primary_search_term": "machine learning",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2505.24630v1",
      "title": "The Hallucination Dilemma: Factuality-Aware Reinforcement Learning for\n  Large Reasoning Models",
      "published": "2025-05-30T14:23:32Z",
      "updated": "2025-05-30T14:23:32Z",
      "summary": "Large language models (LLMs) have significantly advanced in reasoning tasks\nthrough reinforcement learning (RL) optimization, achieving impressive\ncapabilities across various challenging benchmarks. However, our empirical\nanalysis reveals a critical drawback: reasoning-oriented RL fine-tuning\nsignificantly increases the prevalence of hallucinations. We theoretically\nanalyze the RL training dynamics, identifying high-variance gradient,\nentropy-induced randomness, and susceptibility to spurious local optima as key\nfactors leading to hallucinations. To address this drawback, we propose\nFactuality-aware Step-wise Policy Optimization (FSPO), an innovative RL\nfine-tuning algorithm incorporating explicit factuality verification at each\nreasoning step. FSPO leverages automated verification against given evidence to\ndynamically adjust token-level advantage values, incentivizing factual\ncorrectness throughout the reasoning process. Experiments across mathematical\nreasoning and hallucination benchmarks using Qwen2.5 and Llama models\ndemonstrate that FSPO effectively reduces hallucinations while enhancing\nreasoning accuracy, substantially improving both reliability and performance.",
      "authors": [
        "Junyi Li",
        "Hwee Tou Ng"
      ],
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "links": [
        "http://arxiv.org/abs/2505.24630v1",
        "http://arxiv.org/pdf/2505.24630v1"
      ],
      "primary_search_term": "machine learning",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2505.24584v2",
      "title": "AutoChemSchematic AI: A Closed-Loop, Physics-Aware Agentic Framework for\n  Auto-Generating Chemical Process and Instrumentation Diagrams",
      "published": "2025-05-30T13:32:00Z",
      "updated": "2025-06-02T01:08:24Z",
      "summary": "Recent advancements in generative AI have accelerated the discovery of novel\nchemicals and materials; however, transitioning these discoveries to\nindustrial-scale production remains a critical bottleneck, as it requires the\ndevelopment of entirely new chemical manufacturing processes. Current AI\nmethods cannot auto-generate PFDs or PIDs, despite their critical role in\nscaling chemical processes, while adhering to engineering constraints. We\npresent a closed loop, physics aware framework for the automated generation of\nindustrially viable PFDs and PIDs. The framework integrates domain specialized\nsmall scale language models (SLMs) (trained for chemical process QA tasks) with\nfirst principles simulation, leveraging three key components: (1) a\nhierarchical knowledge graph of process flow and instrumentation descriptions\nfor 1,020+ chemicals, (2) a multi-stage training pipeline that fine tunes\ndomain specialized SLMs on synthetic datasets via Supervised Fine-Tuning (SFT),\nDirect Preference Optimization (DPO), and Retrieval-Augmented Instruction\nTuning (RAIT), and (3) DWSIM based simulator in the loop validation to ensure\nfeasibility. To improve both runtime efficiency and model compactness, the\nframework incorporates advanced inference time optimizations including\nFlashAttention, Lookahead Decoding, PagedAttention with KV-cache quantization,\nand Test Time Inference Scaling and independently applies structural pruning\ntechniques (width and depth) guided by importance heuristics to reduce model\nsize with minimal accuracy loss. Experiments demonstrate that the framework\ngenerates simulator-validated process descriptions with high fidelity,\noutperforms baseline methods in correctness, and generalizes to unseen\nchemicals. By bridging AI-driven design with industrial-scale feasibility, this\nwork significantly reduces R&D timelines from lab discovery to plant\ndeployment.",
      "authors": [
        "Sakhinana Sagar Srinivas",
        "Shivam Gupta",
        "Venkataramana Runkana"
      ],
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.IR"
      ],
      "links": [
        "http://arxiv.org/abs/2505.24584v2",
        "http://arxiv.org/pdf/2505.24584v2"
      ],
      "primary_search_term": "machine learning",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2506.03185v1",
      "title": "DLiPath: A Benchmark for the Comprehensive Assessment of Donor Liver\n  Based on Histopathological Image Dataset",
      "published": "2025-05-30T12:13:00Z",
      "updated": "2025-05-30T12:13:00Z",
      "summary": "Pathologists comprehensive evaluation of donor liver biopsies provides\ncrucial information for accepting or discarding potential grafts. However,\nrapidly and accurately obtaining these assessments intraoperatively poses a\nsignificant challenge for pathologists. Features in donor liver biopsies, such\nas portal tract fibrosis, total steatosis, macrovesicular steatosis, and\nhepatocellular ballooning are correlated with transplant outcomes, yet\nquantifying these indicators suffers from substantial inter- and intra-observer\nvariability. To address this, we introduce DLiPath, the first benchmark for\ncomprehensive donor liver assessment based on a histopathology image dataset.\nWe collected and publicly released 636 whole slide images from 304 donor liver\npatients at the Department of Pathology, the Third Xiangya Hospital, with\nexpert annotations for key pathological features (including cholestasis, portal\ntract fibrosis, portal inflammation, total steatosis, macrovesicular steatosis,\nand hepatocellular ballooning). We selected nine state-of-the-art\nmultiple-instance learning (MIL) models based on the DLiPath dataset as\nbaselines for extensive comparative analysis. The experimental results\ndemonstrate that several MIL models achieve high accuracy across donor liver\nassessment indicators on DLiPath, charting a clear course for future automated\nand intelligent donor liver assessment research. Data and code are available at\nhttps://github.com/panliangrui/ACM_MM_2025.",
      "authors": [
        "Liangrui Pan",
        "Xingchen Li",
        "Zhongyi Chen",
        "Ling Chu",
        "Shaoliang Peng"
      ],
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV",
        "q-bio.QM"
      ],
      "links": [
        "http://arxiv.org/abs/2506.03185v1",
        "http://arxiv.org/pdf/2506.03185v1"
      ],
      "primary_search_term": "machine learning",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2505.24493v1",
      "title": "MELT: Towards Automated Multimodal Emotion Data Annotation by Leveraging\n  LLM Embedded Knowledge",
      "published": "2025-05-30T11:45:36Z",
      "updated": "2025-05-30T11:45:36Z",
      "summary": "Although speech emotion recognition (SER) has advanced significantly with\ndeep learning, annotation remains a major hurdle. Human annotation is not only\ncostly but also subject to inconsistencies annotators often have different\npreferences and may lack the necessary contextual knowledge, which can lead to\nvaried and inaccurate labels. Meanwhile, Large Language Models (LLMs) have\nemerged as a scalable alternative for annotating text data. However, the\npotential of LLMs to perform emotional speech data annotation without human\nsupervision has yet to be thoroughly investigated. To address these problems,\nwe apply GPT-4o to annotate a multimodal dataset collected from the sitcom\nFriends, using only textual cues as inputs. By crafting structured text\nprompts, our methodology capitalizes on the knowledge GPT-4o has accumulated\nduring its training, showcasing that it can generate accurate and contextually\nrelevant annotations without direct access to multimodal inputs. Therefore, we\npropose MELT, a multimodal emotion dataset fully annotated by GPT-4o. We\ndemonstrate the effectiveness of MELT by fine-tuning four self-supervised\nlearning (SSL) backbones and assessing speech emotion recognition performance\nacross emotion datasets. Additionally, our subjective experiments\\' results\ndemonstrate a consistence performance improvement on SER.",
      "authors": [
        "Xin Jing",
        "Jiadong Wang",
        "Iosif Tsangko",
        "Andreas Triantafyllopoulos",
        "Bj\u00f6rn W. Schuller"
      ],
      "categories": [
        "cs.AI",
        "cs.SD",
        "eess.AS"
      ],
      "links": [
        "http://arxiv.org/abs/2505.24493v1",
        "http://arxiv.org/pdf/2505.24493v1"
      ],
      "primary_search_term": "machine learning",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2505.24415v1",
      "title": "Boosting Automatic Exercise Evaluation Through Musculoskeletal\n  Simulation-Based IMU Data Augmentation",
      "published": "2025-05-30T09:53:37Z",
      "updated": "2025-05-30T09:53:37Z",
      "summary": "Automated evaluation of movement quality holds significant potential for\nenhancing physiotherapeutic treatments and sports training by providing\nobjective, real-time feedback. However, the effectiveness of deep learning\nmodels in assessing movements captured by inertial measurement units (IMUs) is\noften hampered by limited data availability, class imbalance, and label\nambiguity. In this work, we present a novel data augmentation method that\ngenerates realistic IMU data using musculoskeletal simulations integrated with\nsystematic modifications of movement trajectories. Crucially, our approach\nensures biomechanical plausibility and allows for automatic, reliable labeling\nby combining inverse kinematic parameters with a knowledge-based evaluation\nstrategy. Extensive evaluations demonstrate that augmented variants closely\nresembles real-world data, significantly improving the classification accuracy\nand generalization capability of neural network models. Additionally, we\nhighlight the benefits of augmented data for patient-specific fine-tuning\nscenarios, particularly when only limited subject-specific training examples\nare available. Our findings underline the practicality and efficacy of this\naugmentation method in overcoming common challenges faced by deep learning\napplications in physiotherapeutic exercise evaluation.",
      "authors": [
        "Andreas Spilz",
        "Heiko Oppel",
        "Michael Munz"
      ],
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "links": [
        "http://arxiv.org/abs/2505.24415v1",
        "http://arxiv.org/pdf/2505.24415v1"
      ],
      "primary_search_term": "machine learning",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2505.24413v1",
      "title": "Multi-task Learning for Heterogeneous Multi-source Block-Wise Missing\n  Data",
      "published": "2025-05-30T09:52:03Z",
      "updated": "2025-05-30T09:52:03Z",
      "summary": "Multi-task learning (MTL) has emerged as an imperative machine learning tool\nto solve multiple learning tasks simultaneously and has been successfully\napplied to healthcare, marketing, and biomedical fields. However, in order to\nborrow information across different tasks effectively, it is essential to\nutilize both homogeneous and heterogeneous information. Among the extensive\nliterature on MTL, various forms of heterogeneity are presented in MTL\nproblems, such as block-wise, distribution, and posterior heterogeneity.\nExisting methods, however, struggle to tackle these forms of heterogeneity\nsimultaneously in a unified framework. In this paper, we propose a two-step\nlearning strategy for MTL which addresses the aforementioned heterogeneity.\nFirst, we impute the missing blocks using shared representations extracted from\nhomogeneous source across different tasks. Next, we disentangle the mappings\nbetween input features and responses into a shared component and a\ntask-specific component, respectively, thereby enabling information borrowing\nthrough the shared component. Our numerical experiments and real-data analysis\nfrom the ADNI database demonstrate the superior MTL performance of the proposed\nmethod compared to other competing methods.",
      "authors": [
        "Yang Sui",
        "Qi Xu",
        "Yang Bai",
        "Annie Qu"
      ],
      "categories": [
        "cs.LG",
        "stat.CO"
      ],
      "links": [
        "http://arxiv.org/abs/2505.24413v1",
        "http://arxiv.org/pdf/2505.24413v1"
      ],
      "primary_search_term": "machine learning",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2505.24396v1",
      "title": "Reactive Aerobatic Flight via Reinforcement Learning",
      "published": "2025-05-30T09:24:30Z",
      "updated": "2025-05-30T09:24:30Z",
      "summary": "Quadrotors have demonstrated remarkable versatility, yet their full aerobatic\npotential remains largely untapped due to inherent underactuation and the\ncomplexity of aggressive maneuvers. Traditional approaches, separating\ntrajectory optimization and tracking control, suffer from tracking\ninaccuracies, computational latency, and sensitivity to initial conditions,\nlimiting their effectiveness in dynamic, high-agility scenarios. Inspired by\nrecent breakthroughs in data-driven methods, we propose a reinforcement\nlearning-based framework that directly maps drone states and aerobatic\nintentions to control commands, eliminating modular separation to enable\nquadrotors to perform end-to-end policy optimization for extreme aerobatic\nmaneuvers. To ensure efficient and stable training, we introduce an automated\ncurriculum learning strategy that dynamically adjusts aerobatic task\ndifficulty. Enabled by domain randomization for robust zero-shot sim-to-real\ntransfer, our approach is validated in demanding real-world experiments,\nincluding the first demonstration of a drone autonomously performing continuous\ninverted flight while reactively navigating a moving gate, showcasing\nunprecedented agility.",
      "authors": [
        "Zhichao Han",
        "Xijie Huang",
        "Zhuxiu Xu",
        "Jiarui Zhang",
        "Yuze Wu",
        "Mingyang Wang",
        "Tianyue Wu",
        "Fei Gao"
      ],
      "categories": [
        "cs.RO"
      ],
      "links": [
        "http://arxiv.org/abs/2505.24396v1",
        "http://arxiv.org/pdf/2505.24396v1"
      ],
      "primary_search_term": "machine learning",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2505.24369v1",
      "title": "Adversarial Preference Learning for Robust LLM Alignment",
      "published": "2025-05-30T09:02:07Z",
      "updated": "2025-05-30T09:02:07Z",
      "summary": "Modern language models often rely on Reinforcement Learning from Human\nFeedback (RLHF) to encourage safe behaviors. However, they remain vulnerable to\nadversarial attacks due to three key limitations: (1) the inefficiency and high\ncost of human annotation, (2) the vast diversity of potential adversarial\nattacks, and (3) the risk of feedback bias and reward hacking. To address these\nchallenges, we introduce Adversarial Preference Learning (APL), an iterative\nadversarial training method incorporating three key innovations. First, a\ndirect harmfulness metric based on the model's intrinsic preference\nprobabilities, eliminating reliance on external assessment. Second, a\nconditional generative attacker that synthesizes input-specific adversarial\nvariations. Third, an iterative framework with automated closed-loop feedback,\nenabling continuous adaptation through vulnerability discovery and mitigation.\nExperiments on Mistral-7B-Instruct-v0.3 demonstrate that APL significantly\nenhances robustness, achieving 83.33% harmlessness win rate over the base model\n(evaluated by GPT-4o), reducing harmful outputs from 5.88% to 0.43% (measured\nby LLaMA-Guard), and lowering attack success rate by up to 65% according to\nHarmBench. Notably, APL maintains competitive utility, with an MT-Bench score\nof 6.59 (comparable to the baseline 6.78) and an LC-WinRate of 46.52% against\nthe base model.",
      "authors": [
        "Yuanfu Wang",
        "Pengyu Wang",
        "Chenyang Xi",
        "Bo Tang",
        "Junyi Zhu",
        "Wenqiang Wei",
        "Chen Chen",
        "Chao Yang",
        "Jingfeng Zhang",
        "Chaochao Lu",
        "Yijun Niu",
        "Keming Mao",
        "Zhiyu Li",
        "Feiyu Xiong",
        "Jie Hu",
        "Mingchuan Yang"
      ],
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "links": [
        "http://arxiv.org/abs/2505.24369v1",
        "http://arxiv.org/pdf/2505.24369v1"
      ],
      "primary_search_term": "machine learning",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2505.24360v2",
      "title": "Interpreting Large Text-to-Image Diffusion Models with Dictionary\n  Learning",
      "published": "2025-05-30T08:53:27Z",
      "updated": "2025-06-03T02:01:33Z",
      "summary": "Sparse autoencoders are a promising new approach for decomposing language\nmodel activations for interpretation and control. They have been applied\nsuccessfully to vision transformer image encoders and to small-scale diffusion\nmodels. Inference-Time Decomposition of Activations (ITDA) is a recently\nproposed variant of dictionary learning that takes the dictionary to be a set\nof data points from the activation distribution and reconstructs them with\ngradient pursuit. We apply Sparse Autoencoders (SAEs) and ITDA to a large\ntext-to-image diffusion model, Flux 1, and consider the interpretability of\nembeddings of both by introducing a visual automated interpretation pipeline.\nWe find that SAEs accurately reconstruct residual stream embeddings and beat\nMLP neurons on interpretability. We are able to use SAE features to steer image\ngeneration through activation addition. We find that ITDA has comparable\ninterpretability to SAEs.",
      "authors": [
        "Stepan Shabalin",
        "Ayush Panda",
        "Dmitrii Kharlapenko",
        "Abdur Raheem Ali",
        "Yixiong Hao",
        "Arthur Conmy"
      ],
      "categories": [
        "cs.LG"
      ],
      "links": [
        "http://arxiv.org/abs/2505.24360v2",
        "http://arxiv.org/pdf/2505.24360v2"
      ],
      "primary_search_term": "machine learning",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2505.24317v1",
      "title": "ROAD: Responsibility-Oriented Reward Design for Reinforcement Learning\n  in Autonomous Driving",
      "published": "2025-05-30T08:00:51Z",
      "updated": "2025-05-30T08:00:51Z",
      "summary": "Reinforcement learning (RL) in autonomous driving employs a trial-and-error\nmechanism, enhancing robustness in unpredictable environments. However,\ncrafting effective reward functions remains challenging, as conventional\napproaches rely heavily on manual design and demonstrate limited efficacy in\ncomplex scenarios. To address this issue, this study introduces a\nresponsibility-oriented reward function that explicitly incorporates traffic\nregulations into the RL framework. Specifically, we introduced a Traffic\nRegulation Knowledge Graph and leveraged Vision-Language Models alongside\nRetrieval-Augmented Generation techniques to automate reward assignment. This\nintegration guides agents to adhere strictly to traffic laws, thus minimizing\nrule violations and optimizing decision-making performance in diverse driving\nconditions. Experimental validations demonstrate that the proposed methodology\nsignificantly improves the accuracy of assigning accident responsibilities and\neffectively reduces the agent's liability in traffic incidents.",
      "authors": [
        "Yongming Chen",
        "Miner Chen",
        "Liewen Liao",
        "Mingyang Jiang",
        "Xiang Zuo",
        "Hengrui Zhang",
        "Yuchen Xi",
        "Songan Zhang"
      ],
      "categories": [
        "cs.LG"
      ],
      "links": [
        "http://arxiv.org/abs/2505.24317v1",
        "http://arxiv.org/pdf/2505.24317v1"
      ],
      "primary_search_term": "machine learning",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2505.24281v1",
      "title": "Multi-task Learning for Heterogeneous Data via Integrating Shared and\n  Task-Specific Encodings",
      "published": "2025-05-30T06:58:42Z",
      "updated": "2025-05-30T06:58:42Z",
      "summary": "Multi-task learning (MTL) has become an essential machine learning tool for\naddressing multiple learning tasks simultaneously and has been effectively\napplied across fields such as healthcare, marketing, and biomedical research.\nHowever, to enable efficient information sharing across tasks, it is crucial to\nleverage both shared and heterogeneous information. Despite extensive research\non MTL, various forms of heterogeneity, including distribution and posterior\nheterogeneity, present significant challenges. Existing methods often fail to\naddress these forms of heterogeneity within a unified framework. In this paper,\nwe propose a dual-encoder framework to construct a heterogeneous latent factor\nspace for each task, incorporating a task-shared encoder to capture common\ninformation across tasks and a task-specific encoder to preserve unique task\ncharacteristics. Additionally, we explore the intrinsic similarity structure of\nthe coefficients corresponding to learned latent factors, allowing for adaptive\nintegration across tasks to manage posterior heterogeneity. We introduce a\nunified algorithm that alternately learns the task-specific and task-shared\nencoders and coefficients. In theory, we investigate the excess risk bound for\nthe proposed MTL method using local Rademacher complexity and apply it to a new\nbut related task. Through simulation studies, we demonstrate that the proposed\nmethod outperforms existing data integration methods across various settings.\nFurthermore, the proposed method achieves superior predictive performance for\ntime to tumor doubling across five distinct cancer types in PDX data.",
      "authors": [
        "Yang Sui",
        "Qi Xu",
        "Yang Bai",
        "Annie Qu"
      ],
      "categories": [
        "stat.ML",
        "cs.LG",
        "stat.ME"
      ],
      "links": [
        "http://arxiv.org/abs/2505.24281v1",
        "http://arxiv.org/pdf/2505.24281v1"
      ],
      "primary_search_term": "machine learning",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2505.24252v1",
      "title": "A Reward-driven Automated Webshell Malicious-code Generator for\n  Red-teaming",
      "published": "2025-05-30T06:16:42Z",
      "updated": "2025-05-30T06:16:42Z",
      "summary": "Frequent cyber-attacks have elevated WebShell exploitation and defense to a\ncritical research focus within network security. However, there remains a\nsignificant shortage of publicly available, well-categorized malicious-code\ndatasets organized by obfuscation method. Existing malicious-code generation\nmethods, which primarily rely on prompt engineering, often suffer from limited\ndiversity and high redundancy in the payloads they produce. To address these\nlimitations, we propose \\textbf{RAWG}, a \\textbf{R}eward-driven\n\\textbf{A}utomated \\textbf{W}ebshell Malicious-code \\textbf{G}enerator designed\nfor red-teaming applications. Our approach begins by categorizing webshell\nsamples from common datasets into seven distinct types of obfuscation. We then\nemploy a large language model (LLM) to extract and normalize key tokens from\neach sample, creating a standardized, high-quality corpus. Using this curated\ndataset, we perform supervised fine-tuning (SFT) on an open-source large model\nto enable the generation of diverse, highly obfuscated webshell malicious\npayloads. To further enhance generation quality, we apply Proximal Policy\nOptimization (PPO), treating malicious-code samples as \"chosen\" data and benign\ncode as \"rejected\" data during reinforcement learning. Extensive experiments\ndemonstrate that RAWG significantly outperforms current state-of-the-art\nmethods in both payload diversity and escape effectiveness.",
      "authors": [
        "Yizhong Ding"
      ],
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "links": [
        "http://arxiv.org/abs/2505.24252v1",
        "http://arxiv.org/pdf/2505.24252v1"
      ],
      "primary_search_term": "machine learning",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2505.24247v1",
      "title": "50 Years of Automated Face Recognition",
      "published": "2025-05-30T06:10:48Z",
      "updated": "2025-05-30T06:10:48Z",
      "summary": "Over the past 50 years, automated face recognition has evolved from\nrudimentary, handcrafted systems into sophisticated deep learning models that\nrival and often surpass human performance. This paper chronicles the history\nand technological progression of FR, from early geometric and statistical\nmethods to modern deep neural architectures leveraging massive real and\nAI-generated datasets. We examine key innovations that have shaped the field,\nincluding developments in dataset, loss function, neural network design and\nfeature fusion. We also analyze how the scale and diversity of training data\ninfluence model generalization, drawing connections between dataset growth and\nbenchmark improvements. Recent advances have achieved remarkable milestones:\nstate-of-the-art face verification systems now report False Negative\nIdentification Rates of 0.13% against a 12.4 million gallery in NIST FRVT\nevaluations for 1:N visa-to-border matching. While recent advances have enabled\nremarkable accuracy in high- and low-quality face scenarios, numerous\nchallenges persist. While remarkable progress has been achieved, several open\nresearch problems remain. We outline critical challenges and promising\ndirections for future face recognition research, including scalability,\nmulti-modal fusion, synthetic identity generation, and explainable systems.",
      "authors": [
        "Minchul Kim",
        "Anil Jain",
        "Xiaoming Liu"
      ],
      "categories": [
        "cs.CV"
      ],
      "links": [
        "http://arxiv.org/abs/2505.24247v1",
        "http://arxiv.org/pdf/2505.24247v1"
      ],
      "primary_search_term": "machine learning",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2505.24230v1",
      "title": "ProofNet++: A Neuro-Symbolic System for Formal Proof Verification with\n  Self-Correction",
      "published": "2025-05-30T05:44:34Z",
      "updated": "2025-05-30T05:44:34Z",
      "summary": "We propose ProofNet++, a neuro-symbolic framework that enhances automated\ntheorem proving by combining large language models (LLMs) with formal proof\nverification and self-correction mechanisms. Current LLM-based systems suffer\nfrom hallucinated logical steps and unverifiable reasoning. ProofNet++\nmitigates these limitations by integrating symbolic proof tree supervision, a\nreinforcement learning loop using verifiers as reward functions, and an\niterative self-correction module. Our experiments on miniF2F, Lean's mathlib,\nand HOL Light show that ProofNet++ significantly improves proof accuracy,\ncorrectness, and formal verifiability over prior models. We provide theoretical\nanalysis of the convergence and stability of the verifier-guided RL framework\nand release our datasets and codebase for future research.",
      "authors": [
        "Murari Ambati"
      ],
      "categories": [
        "cs.AI"
      ],
      "links": [
        "http://arxiv.org/abs/2505.24230v1",
        "http://arxiv.org/pdf/2505.24230v1"
      ],
      "primary_search_term": "machine learning",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2505.24183v1",
      "title": "CodeV-R1: Reasoning-Enhanced Verilog Generation",
      "published": "2025-05-30T03:51:06Z",
      "updated": "2025-05-30T03:51:06Z",
      "summary": "Large language models (LLMs) trained via reinforcement learning with\nverifiable reward (RLVR) have achieved breakthroughs on tasks with explicit,\nautomatable verification, such as software programming and mathematical\nproblems. Extending RLVR to electronic design automation (EDA), especially\nautomatically generating hardware description languages (HDLs) like Verilog\nfrom natural-language (NL) specifications, however, poses three key challenges:\nthe lack of automated and accurate verification environments, the scarcity of\nhigh-quality NL-code pairs, and the prohibitive computation cost of RLVR. To\nthis end, we introduce CodeV-R1, an RLVR framework for training Verilog\ngeneration LLMs. First, we develop a rule-based testbench generator that\nperforms robust equivalence checking against golden references. Second, we\npropose a round-trip data synthesis method that pairs open-source Verilog\nsnippets with LLM-generated NL descriptions, verifies code-NL-code consistency\nvia the generated testbench, and filters out inequivalent examples to yield a\nhigh-quality dataset. Third, we employ a two-stage \"distill-then-RL\" training\npipeline: distillation for the cold start of reasoning abilities, followed by\nadaptive DAPO, our novel RLVR algorithm that can reduce training cost by\nadaptively adjusting sampling rate. The resulting model, CodeV-R1-7B, achieves\n68.6% and 72.9% pass@1 on VerilogEval v2 and RTLLM v1.1, respectively,\nsurpassing prior state-of-the-art by 12~20%, while matching or even exceeding\nthe performance of 671B DeepSeek-R1. We will release our model, training\npipeline, and dataset to facilitate research in EDA and LLM communities.",
      "authors": [
        "Yaoyu Zhu",
        "Di Huang",
        "Hanqi Lyu",
        "Xiaoyun Zhang",
        "Chongxiao Li",
        "Wenxuan Shi",
        "Yutong Wu",
        "Jianan Mu",
        "Jinghua Wang",
        "Yang Zhao",
        "Pengwei Jin",
        "Shuyao Cheng",
        "Shengwen Liang",
        "Xishan Zhang",
        "Rui Zhang",
        "Zidong Du",
        "Qi Guo",
        "Xing Hu",
        "Yunji Chen"
      ],
      "categories": [
        "cs.LG",
        "cs.AR",
        "cs.PL"
      ],
      "links": [
        "http://arxiv.org/abs/2505.24183v1",
        "http://arxiv.org/pdf/2505.24183v1"
      ],
      "primary_search_term": "machine learning",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2505.24166v1",
      "title": "Deep learning-derived arterial input function",
      "published": "2025-05-30T03:16:04Z",
      "updated": "2025-05-30T03:16:04Z",
      "summary": "Dynamic positron emission tomography (PET) imaging combined with radiotracer\nkinetic modeling is a powerful technique for visualizing biological processes\nin the brain, offering valuable insights into brain functions and neurological\ndisorders such as Alzheimer's and Parkinson's diseases. Accurate kinetic\nmodeling relies heavily on the use of a metabolite-corrected arterial input\nfunction (AIF), which typically requires invasive and labor-intensive arterial\nblood sampling. While alternative non-invasive approaches have been proposed,\nthey often compromise accuracy or still necessitate at least one invasive blood\nsampling. In this study, we present the deep learning-derived arterial input\nfunction (DLIF), a deep learning framework capable of estimating a\nmetabolite-corrected AIF directly from dynamic PET image sequences without any\nblood sampling. We validated DLIF using existing dynamic PET patient data. We\ncompared DLIF and resulting parametric maps against ground truth measurements.\nOur evaluation shows that DLIF achieves accurate and robust AIF estimation. By\nleveraging deep learning's ability to capture complex temporal dynamics and\nincorporating prior knowledge of typical AIF shapes through basis functions,\nDLIF provides a rapid, accurate, and entirely non-invasive alternative to\ntraditional AIF measurement methods.",
      "authors": [
        "Junyu Chen",
        "Zirui Jiang",
        "Jennifer M. Coughlin",
        "Martin G. Pomper",
        "Yong Du"
      ],
      "categories": [
        "eess.IV"
      ],
      "links": [
        "http://arxiv.org/abs/2505.24166v1",
        "http://arxiv.org/pdf/2505.24166v1"
      ],
      "primary_search_term": "machine learning",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2505.24138v1",
      "title": "AMSbench: A Comprehensive Benchmark for Evaluating MLLM Capabilities in\n  AMS Circuits",
      "published": "2025-05-30T02:17:45Z",
      "updated": "2025-05-30T02:17:45Z",
      "summary": "Analog/Mixed-Signal (AMS) circuits play a critical role in the integrated\ncircuit (IC) industry. However, automating Analog/Mixed-Signal (AMS) circuit\ndesign has remained a longstanding challenge due to its difficulty and\ncomplexity. Recent advances in Multi-modal Large Language Models (MLLMs) offer\npromising potential for supporting AMS circuit analysis and design. However,\ncurrent research typically evaluates MLLMs on isolated tasks within the domain,\nlacking a comprehensive benchmark that systematically assesses model\ncapabilities across diverse AMS-related challenges. To address this gap, we\nintroduce AMSbench, a benchmark suite designed to evaluate MLLM performance\nacross critical tasks including circuit schematic perception, circuit analysis,\nand circuit design. AMSbench comprises approximately 8000 test questions\nspanning multiple difficulty levels and assesses eight prominent models,\nencompassing both open-source and proprietary solutions such as Qwen 2.5-VL and\nGemini 2.5 Pro. Our evaluation highlights significant limitations in current\nMLLMs, particularly in complex multi-modal reasoning and sophisticated circuit\ndesign tasks. These results underscore the necessity of advancing MLLMs'\nunderstanding and effective application of circuit-specific knowledge, thereby\nnarrowing the existing performance gap relative to human expertise and moving\ntoward fully automated AMS circuit design workflows. Our data is released at\nhttps://huggingface.co/datasets/wwhhyy/AMSBench",
      "authors": [
        "Yichen Shi",
        "Ze Zhang",
        "Hongyang Wang",
        "Zhuofu Tao",
        "Zhongyi Li",
        "Bingyu Chen",
        "Yaxin Wang",
        "Zhiping Yu",
        "Ting-Jung Lin",
        "Lei He"
      ],
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "links": [
        "http://arxiv.org/abs/2505.24138v1",
        "http://arxiv.org/pdf/2505.24138v1"
      ],
      "primary_search_term": "machine learning",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2505.24126v1",
      "title": "How Students (Really) Use ChatGPT: Uncovering Experiences Among\n  Undergraduate Students",
      "published": "2025-05-30T01:51:08Z",
      "updated": "2025-05-30T01:51:08Z",
      "summary": "This study investigates how undergraduate students engage with ChatGPT in\nself directed learning contexts. Analyzing naturalistic interaction logs, we\nidentify five dominant use categories of ChatGPT information seeking, content\ngeneration, language refinement, meta cognitive engagement, and conversational\nrepair. Behavioral modeling reveals that structured, goal driven tasks like\ncoding, multiple choice solving, and job application writing are strong\npredictors of continued use. Drawing on Self-Directed Learning (SDL) and the\nUses and Gratifications Theory (UGT), we show how students actively manage\nChatGPTs affordances and limitations through prompt adaptation, follow-ups, and\nemotional regulation. Rather than disengaging after breakdowns, students often\npersist through clarification and repair, treating the assistant as both tool\nand learning partner. We also offer design and policy recommendations to\nsupport transparent, responsive, and pedagogically grounded integration of\ngenerative AI in higher education.",
      "authors": [
        "Tawfiq Ammari",
        "Meilun Chen",
        "S M Mehedi Zaman",
        "Kiran Garimella"
      ],
      "categories": [
        "cs.HC"
      ],
      "links": [
        "http://arxiv.org/abs/2505.24126v1",
        "http://arxiv.org/pdf/2505.24126v1"
      ],
      "primary_search_term": "machine learning",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2505.24085v1",
      "title": "DeepBoost-AF: A Novel Unsupervised Feature Learning and Gradient\n  Boosting Fusion for Robust Atrial Fibrillation Detection in Raw ECG Signals",
      "published": "2025-05-30T00:08:56Z",
      "updated": "2025-05-30T00:08:56Z",
      "summary": "Atrial fibrillation (AF) is a prevalent cardiac arrhythmia associated with\nelevated health risks, where timely detection is pivotal for mitigating\nstroke-related morbidity. This study introduces an innovative hybrid\nmethodology integrating unsupervised deep learning and gradient boosting models\nto improve AF detection. A 19-layer deep convolutional autoencoder (DCAE) is\ncoupled with three boosting classifiers-AdaBoost, XGBoost, and LightGBM\n(LGBM)-to harness their complementary advantages while addressing individual\nlimitations. The proposed framework uniquely combines DCAE with gradient\nboosting, enabling end-to-end AF identification devoid of manual feature\nextraction. The DCAE-LGBM model attains an F1-score of 95.20%, sensitivity of\n99.99%, and inference latency of four seconds, outperforming existing methods\nand aligning with clinical deployment requirements. The DCAE integration\nsignificantly enhances boosting models, positioning this hybrid system as a\nreliable tool for automated AF detection in clinical settings.",
      "authors": [
        "Alireza Jafari",
        "Fereshteh Yousefirizi",
        "Vahid Seydi"
      ],
      "categories": [
        "cs.LG"
      ],
      "links": [
        "http://arxiv.org/abs/2505.24085v1",
        "http://arxiv.org/pdf/2505.24085v1"
      ],
      "primary_search_term": "machine learning",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2505.24069v1",
      "title": "DSR-Bench: Evaluating the Structural Reasoning Abilities of LLMs via\n  Data Structures",
      "published": "2025-05-29T23:24:53Z",
      "updated": "2025-05-29T23:24:53Z",
      "summary": "Large language models (LLMs) are increasingly deployed for real-world tasks\nthat fundamentally involve data manipulation. A core requirement across these\ntasks is the ability to perform structural reasoning--that is, to understand\nand reason about data relationships. For example, customer requests require a\ntemporal ordering, which can be represented by data structures such as queues.\nHowever, existing benchmarks primarily focus on high-level, application-driven\nevaluations without isolating this fundamental capability. To address this gap,\nwe introduce DSR-Bench, a novel benchmark evaluating LLMs' structural reasoning\ncapabilities through data structures, which provide interpretable\nrepresentations of data relationships. DSR-Bench includes 20 data structures,\n35 operations, and 4,140 problem instances, organized hierarchically for\nfine-grained analysis of reasoning limitations. Our evaluation pipeline is\nfully automated and deterministic, eliminating subjective human or model-based\njudgments. Its synthetic nature also ensures scalability and minimizes data\ncontamination risks. We benchmark nine state-of-the-art LLMs. Our analysis\nshows that instruction-tuned models struggle with basic multi-attribute and\nmulti-hop reasoning. Furthermore, while reasoning-oriented models perform\nbetter, they remain fragile on complex and hybrid structures, with the best\nmodel achieving an average score of only 47% on the challenge subset.\nCrucially, models often perform poorly on multi-dimensional data and natural\nlanguage task descriptions, highlighting a critical gap for real-world\ndeployment.",
      "authors": [
        "Yu He",
        "Yingxi Li",
        "Colin White",
        "Ellen Vitercik"
      ],
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "links": [
        "http://arxiv.org/abs/2505.24069v1",
        "http://arxiv.org/pdf/2505.24069v1"
      ],
      "primary_search_term": "machine learning",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2505.24068v1",
      "title": "DiffCoTune: Differentiable Co-Tuning for Cross-domain Robot Control",
      "published": "2025-05-29T23:20:07Z",
      "updated": "2025-05-29T23:20:07Z",
      "summary": "The deployment of robot controllers is hindered by modeling discrepancies due\nto necessary simplifications for computational tractability or inaccuracies in\ndata-generating simulators. Such discrepancies typically require ad-hoc tuning\nto meet the desired performance, thereby ensuring successful transfer to a\ntarget domain. We propose a framework for automated, gradient-based tuning to\nenhance performance in the deployment domain by leveraging differentiable\nsimulators. Our method collects rollouts in an iterative manner to co-tune the\nsimulator and controller parameters, enabling systematic transfer within a few\ntrials in the deployment domain. Specifically, we formulate multi-step\nobjectives for tuning and employ alternating optimization to effectively adapt\nthe controller to the deployment domain. The scalability of our framework is\ndemonstrated by co-tuning model-based and learning-based controllers of\narbitrary complexity for tasks ranging from low-dimensional cart-pole\nstabilization to high-dimensional quadruped and biped tracking, showing\nperformance improvements across different deployment domains.",
      "authors": [
        "Lokesh Krishna",
        "Sheng Cheng",
        "Junheng Li",
        "Naira Hovakimyan",
        "Quan Nguyen"
      ],
      "categories": [
        "cs.RO"
      ],
      "links": [
        "http://arxiv.org/abs/2505.24068v1",
        "http://arxiv.org/pdf/2505.24068v1"
      ],
      "primary_search_term": "machine learning",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2505.24062v1",
      "title": "Exploring Domain Wall Pinning in Ferroelectrics via Automated High\n  Throughput AFM",
      "published": "2025-05-29T23:11:40Z",
      "updated": "2025-05-29T23:11:40Z",
      "summary": "Domain-wall dynamics in ferroelectric materials are strongly\nposition-dependent since each polar interface is locked into a unique local\nmicrostructure. This necessitates spatially resolved studies of the\nwall-pinning using scanning-probe microscopy techniques. The pinning centers\nand preexisting domain walls are usually sparse within image plane, precluding\nthe use of dense hyperspectral imaging modes and requiring time-consuming human\nexperimentation. Here, a large area epitaxial PbTiO$_3$ film on cubic KTaO$_3$\nwere investigated to quantify the electric field driven dynamics of the\npolar-strain domain structures using ML-controlled automated Piezoresponse\nForce Microscopy. Analysis of 1500 switching events reveals that domain wall\ndisplacement depends not only on field parameters but also on the local\nferroelectric-ferroelastic configuration. For example, twin boundaries in\npolydomains regions like a$_1^-$/$c^+$ $\\parallel$ a$_2^-$/$c^-$ stay pinned up\nto a certain level of bias magnitude and change only marginally as the bias\nincreases from 20V to 30V, whereas single variant boundaries like a$_2^+$/$c^+$\n$\\parallel$ a$_2^-$/$c^-$ stack are already activated at 20V. These statistics\non the possible ferroelectric and ferroelastic wall orientations, together with\nthe automated, high-throughput AFM workflow, can be distilled into a predictive\nmap that links domain configurations to pulse parameters. This\nmicrostructure-specific rule set forms the foundation for designing\nferroelectric memories.",
      "authors": [
        "Kamyar Barakati",
        "Yu Liu",
        "Hiroshi Funakubo",
        "Sergei V. Kalinin"
      ],
      "categories": [
        "cond-mat.mtrl-sci",
        "cs.CV",
        "cs.LG",
        "physics.app-ph"
      ],
      "links": [
        "http://arxiv.org/abs/2505.24062v1",
        "http://arxiv.org/pdf/2505.24062v1"
      ],
      "primary_search_term": "machine learning",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    },
    {
      "id": "http://arxiv.org/abs/2506.00076v1",
      "title": "Optimizing Storytelling, Improving Audience Retention, and Reducing\n  Waste in the Entertainment Industry",
      "published": "2025-05-29T23:01:54Z",
      "updated": "2025-05-29T23:01:54Z",
      "summary": "Television networks face high financial risk when making programming\ndecisions, often relying on limited historical data to forecast episodic\nviewership. This study introduces a machine learning framework that integrates\nnatural language processing (NLP) features from over 25000 television episodes\nwith traditional viewership data to enhance predictive accuracy. By extracting\nemotional tone, cognitive complexity, and narrative structure from episode\ndialogue, we evaluate forecasting performance using SARIMAX, rolling XGBoost,\nand feature selection models. While prior viewership remains a strong baseline\npredictor, NLP features contribute meaningful improvements for some series. We\nalso introduce a similarity scoring method based on Euclidean distance between\naggregate dialogue vectors to compare shows by content. Tested across diverse\ngenres, including Better Call Saul and Abbott Elementary, our framework reveals\ngenre-specific performance and offers interpretable metrics for writers,\nexecutives, and marketers seeking data-driven insight into audience behavior.",
      "authors": [
        "Andrew Cornfeld",
        "Ashley Miller",
        "Mercedes Mora-Figueroa",
        "Kurt Samuels",
        "Anthony Palomba"
      ],
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "links": [
        "http://arxiv.org/abs/2506.00076v1",
        "http://arxiv.org/pdf/2506.00076v1"
      ],
      "primary_search_term": "machine learning",
      "secondary_search_terms": [
        "labor market",
        "employment",
        "jobs",
        "workforce",
        "automation"
      ]
    }
  ]
}